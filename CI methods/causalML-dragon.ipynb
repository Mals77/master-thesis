{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2677f95e-9002-4778-b600-ade456820209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 14:08:24.514584: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-04 14:08:25.778290: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-04 14:08:25.778319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-04 14:08:25.780277: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-04 14:08:26.000825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 14:08:42.651691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "\n",
    "from causalml.inference.meta import LRSRegressor\n",
    "from causalml.inference.meta import XGBTRegressor, MLPTRegressor\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor\n",
    "from causalml.inference.tf import DragonNet\n",
    "from causalml.match import NearestNeighborMatch, MatchOptimizer, create_table_one\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.metrics import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "from causalml.inference.nn import CEVAE\n",
    "\n",
    "import os, sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Paired')\n",
    "plt.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5e9e7-d931-421b-a9e8-1738a4bb6615",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BPIC 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161a78d9-b745-4eac-9ff6-10270dd30782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['case:concept:name', 'NumberOfOffers', 'Action', 'org:resource',\n",
      "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
      "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
      "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
      "       'Selected', 'CreditScore', 'OfferedAmount', 'treatedCase',\n",
      "       'caseSuccesful', 'treatmentSuccess', 'offerNumber', 'offerSuccess',\n",
      "       'treatmentOffer', 'timeApplication', 'weekdayApplication'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>NumberOfOffers</th>\n",
       "      <th>Action</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>EventOrigin</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>case:LoanGoal</th>\n",
       "      <th>case:ApplicationType</th>\n",
       "      <th>...</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>OfferedAmount</th>\n",
       "      <th>treatedCase</th>\n",
       "      <th>caseSuccesful</th>\n",
       "      <th>treatmentSuccess</th>\n",
       "      <th>offerNumber</th>\n",
       "      <th>offerSuccess</th>\n",
       "      <th>treatmentOffer</th>\n",
       "      <th>timeApplication</th>\n",
       "      <th>weekdayApplication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>651433.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>651434.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>651435.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>651437.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.613</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>651438.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.620</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case:concept:name  NumberOfOffers  Action  org:resource  concept:name  \\\n",
       "0                0.0             1.0     0.0           0.0           4.0   \n",
       "1                0.0             1.0     4.0           0.0           8.0   \n",
       "2                0.0             1.0     0.0           0.0          22.0   \n",
       "3                0.0             1.0     1.0           0.0          22.0   \n",
       "4                0.0             1.0     0.0           0.0          21.0   \n",
       "\n",
       "   EventOrigin  lifecycle:transition  time:timestamp  case:LoanGoal  \\\n",
       "0          0.0                   1.0        651433.0           10.0   \n",
       "1          0.0                   1.0        651434.0           10.0   \n",
       "2          2.0                   3.0        651435.0           10.0   \n",
       "3          2.0                   6.0        651437.0           10.0   \n",
       "4          2.0                   3.0        651438.0           10.0   \n",
       "\n",
       "   case:ApplicationType  ...  CreditScore  OfferedAmount  treatedCase  \\\n",
       "0                   1.0  ...          0.0         5000.0          0.0   \n",
       "1                   1.0  ...          0.0         5000.0          0.0   \n",
       "2                   1.0  ...          0.0         5000.0          0.0   \n",
       "3                   1.0  ...          0.0         5000.0          0.0   \n",
       "4                   1.0  ...          0.0         5000.0          0.0   \n",
       "\n",
       "   caseSuccesful  treatmentSuccess  offerNumber  offerSuccess  treatmentOffer  \\\n",
       "0            0.0               0.0          1.0           0.0             0.0   \n",
       "1            0.0               0.0          1.0           0.0             0.0   \n",
       "2            0.0               0.0          1.0           0.0             0.0   \n",
       "3            0.0               0.0          1.0           0.0             0.0   \n",
       "4            0.0               0.0          1.0           0.0             0.0   \n",
       "\n",
       "   timeApplication  weekdayApplication  \n",
       "0            0.000                 2.0  \n",
       "1            0.061                 2.0  \n",
       "2            0.290                 2.0  \n",
       "3           66.613                 2.0  \n",
       "4           66.620                 2.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bpi2017_final.csv\")\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90aba3a4-965d-496f-babb-ef916ff93d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['NumberOfOffers', 'Action', 'org:resource',\n",
    "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
    "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
    "       'CreditScore', 'OfferedAmount', 'offerNumber','timeApplication', 'weekdayApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa096af-76bc-49f7-8412-5d65ea3feae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_outcome = df['offerSuccess'].values\n",
    "x_feature = df[feature_names].values\n",
    "t_treatment = np.array([np.array([value]) for value in df['treatmentOffer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b44e1c-a742-49ac-8543-2c4d615bbea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dragon = DragonNet(neurons_per_layer=200, targeted_reg=True)\n",
    "dragon_ite = dragon.fit_predict(x_feature, t_treatment, y_outcome, return_components=False)\n",
    "dragon_ate = dragon_ite.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b12598-f609-40b9-8893-6dab1df10777",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dragon_ite)\n",
    "print(dragon_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae4d26b-0438-4239-979e-97e02af18b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 0.12840658\n",
      "First Quartile: 0.1284065842628479\n",
      "Median: 0.12840658\n",
      "Third Quartile: 0.1284065842628479\n",
      "Maximum: 0.12840658\n",
      "Interquartile Range: 0.0\n",
      "Upper Bound (Outliers): 0.1284065842628479\n",
      "Lower Bound (Outliers): 0.1284065842628479\n",
      "Outliers: []\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "\n",
    "# Calculate statistics\n",
    "data = np.reshape(dragon_ite, -1)\n",
    "minimum = np.min(data)\n",
    "first_quartile = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "third_quartile = np.percentile(data, 75)\n",
    "maximum = np.max(data)\n",
    "\n",
    "# Interquartile range (IQR)\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Define upper and lower bounds for outliers\n",
    "upper_bound = third_quartile + 1.5 * iqr\n",
    "lower_bound = first_quartile - 1.5 * iqr\n",
    "\n",
    "# Detect outliers\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Minimum:\", minimum)\n",
    "print(\"First Quartile:\", first_quartile)\n",
    "print(\"Median:\", median)\n",
    "print(\"Third Quartile:\", third_quartile)\n",
    "print(\"Maximum:\", maximum)\n",
    "print(\"Interquartile Range:\", iqr)\n",
    "print(\"Upper Bound (Outliers):\", upper_bound)\n",
    "print(\"Lower Bound (Outliers):\", lower_bound)\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "ite_dragon = [minimum, first_quartile, median, third_quartile, maximum, iqr, upper_bound, lower_bound]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe749ca3-5c12-4a5c-b433-70b16c694f2b",
   "metadata": {},
   "source": [
    "### CEVAE Model\n",
    "\n",
    "This module implements the Causal Effect Variational Autoencoder [1]\n",
    "\n",
    "[1] C. Louizos, U. Shalit, J. Mooij, D. Sontag, R. Zemel, M. Welling (2017).\r\n",
    "Causal Effect Inference with Deep Latent-Variable Models.\r\n",
    "http://papers.nips.cc/paper/7223-causal-effect-inference-with-deep-latent-variable-models.pdf\r\n",
    "https://github.com/AMLab-Amsterdam/CEVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c2c74d-e827-452d-9082-1fbd95cff7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=11101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1698d98-3ebb-41f6-a52c-0c1737bb2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[feature_names].values\n",
    "X_test = df_test[feature_names].values\n",
    "treatment_train = df_train['treatmentOffer'].values\n",
    "treatment_test = df_test['treatmentOffer'].values\n",
    "y_train = df_train['offerSuccess'].values\n",
    "y_test = df_test['offerSuccess'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f19ae3-646e-45b8-a322-88f151020120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cevae model settings\n",
    "outcome_dist = \"normal\"\n",
    "latent_dim = 20\n",
    "hidden_dim = 200\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.01\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603cf6e6-93ae-4697-8233-33d3260bd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "cevae = CEVAE(outcome_dist=outcome_dist,\n",
    "              latent_dim=latent_dim,\n",
    "              hidden_dim=hidden_dim,\n",
    "              num_epochs=num_epochs,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              learning_rate_decay=learning_rate_decay,\n",
    "              num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c51f5e8-705f-4fdc-89ca-41d608cf464a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Training with 959 minibatches per epoch\n",
      "DEBUG \t step     0 loss = 39.432\n",
      "DEBUG \t step   100 loss = 21.5587\n",
      "DEBUG \t step   200 loss = 17.7834\n",
      "DEBUG \t step   300 loss = 16.1056\n",
      "DEBUG \t step   400 loss = 14.7949\n",
      "DEBUG \t step   500 loss = 13.9928\n",
      "DEBUG \t step   600 loss = 12.7464\n",
      "DEBUG \t step   700 loss = 11.3728\n",
      "DEBUG \t step   800 loss = 11.5583\n",
      "DEBUG \t step   900 loss = 11.1287\n",
      "DEBUG \t step  1000 loss = 10.064\n",
      "DEBUG \t step  1100 loss = 10.4556\n",
      "DEBUG \t step  1200 loss = 9.80264\n",
      "DEBUG \t step  1300 loss = 9.04074\n",
      "DEBUG \t step  1400 loss = 8.62775\n",
      "DEBUG \t step  1500 loss = 8.42838\n",
      "DEBUG \t step  1600 loss = 7.67158\n",
      "DEBUG \t step  1700 loss = 8.49774\n",
      "DEBUG \t step  1800 loss = 8.13176\n",
      "DEBUG \t step  1900 loss = 7.54507\n",
      "DEBUG \t step  2000 loss = 7.20558\n",
      "DEBUG \t step  2100 loss = 7.25838\n",
      "DEBUG \t step  2200 loss = 7.25943\n",
      "DEBUG \t step  2300 loss = 7.16487\n",
      "DEBUG \t step  2400 loss = 6.76904\n",
      "DEBUG \t step  2500 loss = 7.35741\n",
      "DEBUG \t step  2600 loss = 7.30728\n",
      "DEBUG \t step  2700 loss = 6.72472\n",
      "DEBUG \t step  2800 loss = 5.91676\n",
      "DEBUG \t step  2900 loss = 7.01281\n",
      "DEBUG \t step  3000 loss = 6.46313\n",
      "DEBUG \t step  3100 loss = 6.01823\n",
      "DEBUG \t step  3200 loss = 6.77085\n",
      "DEBUG \t step  3300 loss = 6.09749\n",
      "DEBUG \t step  3400 loss = 6.0199\n",
      "DEBUG \t step  3500 loss = 6.10678\n",
      "DEBUG \t step  3600 loss = 6.03084\n",
      "DEBUG \t step  3700 loss = 5.77299\n",
      "DEBUG \t step  3800 loss = 6.07771\n",
      "DEBUG \t step  3900 loss = 5.44751\n",
      "DEBUG \t step  4000 loss = 6.00574\n",
      "DEBUG \t step  4100 loss = 5.40907\n",
      "DEBUG \t step  4200 loss = 5.17617\n",
      "DEBUG \t step  4300 loss = 5.88157\n",
      "DEBUG \t step  4400 loss = 5.46786\n",
      "DEBUG \t step  4500 loss = 5.53999\n",
      "DEBUG \t step  4600 loss = 5.44812\n",
      "DEBUG \t step  4700 loss = 5.25068\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "losses = cevae.fit(X=torch.tensor(X_train, dtype=torch.float),\n",
    "                   treatment=torch.tensor(treatment_train, dtype=torch.float),\n",
    "                   y=torch.tensor(y_train, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be0af1e-41d0-4926-9bec-a221d5bff8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Evaluating 240 minibatches\n",
      "DEBUG \t batch ate = -0.0518396\n",
      "DEBUG \t batch ate = -0.0578103\n",
      "DEBUG \t batch ate = 0.0179996\n",
      "DEBUG \t batch ate = -0.0672696\n",
      "DEBUG \t batch ate = -0.0212976\n",
      "DEBUG \t batch ate = -0.0394065\n",
      "DEBUG \t batch ate = -0.071669\n",
      "DEBUG \t batch ate = -0.0345337\n",
      "DEBUG \t batch ate = -0.0644459\n",
      "DEBUG \t batch ate = -0.0492067\n",
      "DEBUG \t batch ate = -0.0511732\n",
      "DEBUG \t batch ate = -0.0738739\n",
      "DEBUG \t batch ate = -0.0653151\n",
      "DEBUG \t batch ate = -0.0608201\n",
      "DEBUG \t batch ate = -0.0718545\n",
      "DEBUG \t batch ate = -0.0776044\n",
      "DEBUG \t batch ate = -0.0560759\n",
      "DEBUG \t batch ate = -0.089948\n",
      "DEBUG \t batch ate = -0.0721967\n",
      "DEBUG \t batch ate = -0.0310648\n",
      "DEBUG \t batch ate = -0.0875605\n",
      "DEBUG \t batch ate = -0.056983\n",
      "DEBUG \t batch ate = -0.0885981\n",
      "DEBUG \t batch ate = -0.0835431\n",
      "DEBUG \t batch ate = -0.0559851\n",
      "DEBUG \t batch ate = -0.096993\n",
      "DEBUG \t batch ate = -0.04468\n",
      "DEBUG \t batch ate = -0.0763461\n",
      "DEBUG \t batch ate = -0.0586184\n",
      "DEBUG \t batch ate = -0.0412979\n",
      "DEBUG \t batch ate = -0.0367664\n",
      "DEBUG \t batch ate = -0.0927234\n",
      "DEBUG \t batch ate = -0.108862\n",
      "DEBUG \t batch ate = -0.0730253\n",
      "DEBUG \t batch ate = -0.0390559\n",
      "DEBUG \t batch ate = -0.0739159\n",
      "DEBUG \t batch ate = -0.0598399\n",
      "DEBUG \t batch ate = -0.051695\n",
      "DEBUG \t batch ate = -0.0707358\n",
      "DEBUG \t batch ate = -0.0775618\n",
      "DEBUG \t batch ate = -0.0488791\n",
      "DEBUG \t batch ate = -0.0600749\n",
      "DEBUG \t batch ate = -0.0845111\n",
      "DEBUG \t batch ate = -0.100801\n",
      "DEBUG \t batch ate = -0.0790013\n",
      "DEBUG \t batch ate = -0.016994\n",
      "DEBUG \t batch ate = -0.0754732\n",
      "DEBUG \t batch ate = -0.0868959\n",
      "DEBUG \t batch ate = -0.0673093\n",
      "DEBUG \t batch ate = -0.0949192\n",
      "DEBUG \t batch ate = -0.0824036\n",
      "DEBUG \t batch ate = -0.00960939\n",
      "DEBUG \t batch ate = -0.063209\n",
      "DEBUG \t batch ate = -0.0953682\n",
      "DEBUG \t batch ate = -0.0456414\n",
      "DEBUG \t batch ate = -0.031986\n",
      "DEBUG \t batch ate = -0.0799159\n",
      "DEBUG \t batch ate = -0.0779614\n",
      "DEBUG \t batch ate = -0.0588869\n",
      "DEBUG \t batch ate = -0.0799067\n",
      "DEBUG \t batch ate = -0.0389658\n",
      "DEBUG \t batch ate = -0.0560717\n",
      "DEBUG \t batch ate = -0.0605139\n",
      "DEBUG \t batch ate = -0.0511218\n",
      "DEBUG \t batch ate = -0.0641014\n",
      "DEBUG \t batch ate = -0.0939969\n",
      "DEBUG \t batch ate = -0.103768\n",
      "DEBUG \t batch ate = -0.0983576\n",
      "DEBUG \t batch ate = -0.0532777\n",
      "DEBUG \t batch ate = -0.0568877\n",
      "DEBUG \t batch ate = -0.0776604\n",
      "DEBUG \t batch ate = -0.0498199\n",
      "DEBUG \t batch ate = -0.118822\n",
      "DEBUG \t batch ate = -0.0482421\n",
      "DEBUG \t batch ate = -0.0743496\n",
      "DEBUG \t batch ate = -0.0867228\n",
      "DEBUG \t batch ate = -0.0442112\n",
      "DEBUG \t batch ate = -0.0591625\n",
      "DEBUG \t batch ate = -0.067195\n",
      "DEBUG \t batch ate = -0.0478237\n",
      "DEBUG \t batch ate = -0.0707295\n",
      "DEBUG \t batch ate = -0.0953564\n",
      "DEBUG \t batch ate = -0.0814185\n",
      "DEBUG \t batch ate = -0.0932294\n",
      "DEBUG \t batch ate = -0.0276468\n",
      "DEBUG \t batch ate = -0.0656049\n",
      "DEBUG \t batch ate = 0.00514644\n",
      "DEBUG \t batch ate = -0.0558441\n",
      "DEBUG \t batch ate = -0.114718\n",
      "DEBUG \t batch ate = -0.0804223\n",
      "DEBUG \t batch ate = -0.0506658\n",
      "DEBUG \t batch ate = -0.0336759\n",
      "DEBUG \t batch ate = -0.0941283\n",
      "DEBUG \t batch ate = -0.0909179\n",
      "DEBUG \t batch ate = -0.0233474\n",
      "DEBUG \t batch ate = -0.0839163\n",
      "DEBUG \t batch ate = -0.0616572\n",
      "DEBUG \t batch ate = -0.0693285\n",
      "DEBUG \t batch ate = -0.0697896\n",
      "DEBUG \t batch ate = -0.0572622\n",
      "DEBUG \t batch ate = -0.051247\n",
      "DEBUG \t batch ate = -0.0891693\n",
      "DEBUG \t batch ate = -0.0652369\n",
      "DEBUG \t batch ate = -0.0671666\n",
      "DEBUG \t batch ate = -0.0428197\n",
      "DEBUG \t batch ate = -0.0448927\n",
      "DEBUG \t batch ate = -0.0807265\n",
      "DEBUG \t batch ate = -0.0717007\n",
      "DEBUG \t batch ate = -0.0675121\n",
      "DEBUG \t batch ate = -0.0240663\n",
      "DEBUG \t batch ate = -0.072596\n",
      "DEBUG \t batch ate = -0.0435182\n",
      "DEBUG \t batch ate = -0.0505328\n",
      "DEBUG \t batch ate = -0.0499899\n",
      "DEBUG \t batch ate = -0.0654331\n",
      "DEBUG \t batch ate = -0.0816748\n",
      "DEBUG \t batch ate = -0.0791226\n",
      "DEBUG \t batch ate = -0.0618122\n",
      "DEBUG \t batch ate = -0.067927\n",
      "DEBUG \t batch ate = -0.0418753\n",
      "DEBUG \t batch ate = -0.0924758\n",
      "DEBUG \t batch ate = -0.0743561\n",
      "DEBUG \t batch ate = -0.0744041\n",
      "DEBUG \t batch ate = -0.0369726\n",
      "DEBUG \t batch ate = -0.0752771\n",
      "DEBUG \t batch ate = -0.046929\n",
      "DEBUG \t batch ate = -0.0716558\n",
      "DEBUG \t batch ate = -0.0593658\n",
      "DEBUG \t batch ate = -0.0666576\n",
      "DEBUG \t batch ate = -0.0291832\n",
      "DEBUG \t batch ate = -0.104986\n",
      "DEBUG \t batch ate = -0.0668255\n",
      "DEBUG \t batch ate = -0.103578\n",
      "DEBUG \t batch ate = -0.0292368\n",
      "DEBUG \t batch ate = -0.0674471\n",
      "DEBUG \t batch ate = -0.0670198\n",
      "DEBUG \t batch ate = -0.0632421\n",
      "DEBUG \t batch ate = -0.0497514\n",
      "DEBUG \t batch ate = -0.0583849\n",
      "DEBUG \t batch ate = -0.100242\n",
      "DEBUG \t batch ate = -0.0735576\n",
      "DEBUG \t batch ate = -0.0626039\n",
      "DEBUG \t batch ate = -0.025532\n",
      "DEBUG \t batch ate = -0.0941244\n",
      "DEBUG \t batch ate = -0.0585512\n",
      "DEBUG \t batch ate = -0.128337\n",
      "DEBUG \t batch ate = -0.04384\n",
      "DEBUG \t batch ate = -0.0300757\n",
      "DEBUG \t batch ate = -0.094013\n",
      "DEBUG \t batch ate = -0.0415032\n",
      "DEBUG \t batch ate = -0.0149274\n",
      "DEBUG \t batch ate = -0.0541562\n",
      "DEBUG \t batch ate = -0.0611741\n",
      "DEBUG \t batch ate = -0.0299421\n",
      "DEBUG \t batch ate = -0.0544316\n",
      "DEBUG \t batch ate = -0.0580529\n",
      "DEBUG \t batch ate = -0.0602158\n",
      "DEBUG \t batch ate = -0.0936916\n",
      "DEBUG \t batch ate = -0.0319495\n",
      "DEBUG \t batch ate = -0.0410073\n",
      "DEBUG \t batch ate = -0.0328568\n",
      "DEBUG \t batch ate = -0.0376928\n",
      "DEBUG \t batch ate = -0.06352\n",
      "DEBUG \t batch ate = -0.0686171\n",
      "DEBUG \t batch ate = -0.0827995\n",
      "DEBUG \t batch ate = -0.121243\n",
      "DEBUG \t batch ate = -0.059349\n",
      "DEBUG \t batch ate = -0.0395915\n",
      "DEBUG \t batch ate = -0.0977358\n",
      "DEBUG \t batch ate = -0.0331063\n",
      "DEBUG \t batch ate = -0.0937992\n",
      "DEBUG \t batch ate = -0.0682878\n",
      "DEBUG \t batch ate = -0.0453255\n",
      "DEBUG \t batch ate = -0.038154\n",
      "DEBUG \t batch ate = -0.0979606\n",
      "DEBUG \t batch ate = -0.140811\n",
      "DEBUG \t batch ate = -0.0583728\n",
      "DEBUG \t batch ate = -0.0446266\n",
      "DEBUG \t batch ate = -0.0734503\n",
      "DEBUG \t batch ate = -0.0814751\n",
      "DEBUG \t batch ate = -0.0265666\n",
      "DEBUG \t batch ate = -0.0983509\n",
      "DEBUG \t batch ate = -0.0315252\n",
      "DEBUG \t batch ate = -0.0617645\n",
      "DEBUG \t batch ate = -0.0662485\n",
      "DEBUG \t batch ate = -0.0527539\n",
      "DEBUG \t batch ate = -0.0596034\n",
      "DEBUG \t batch ate = -0.0519054\n",
      "DEBUG \t batch ate = -0.0900351\n",
      "DEBUG \t batch ate = -0.0615231\n",
      "DEBUG \t batch ate = -0.0822812\n",
      "DEBUG \t batch ate = -0.0472789\n",
      "DEBUG \t batch ate = -0.019942\n",
      "DEBUG \t batch ate = -0.0878153\n",
      "DEBUG \t batch ate = -0.0455233\n",
      "DEBUG \t batch ate = -0.0709021\n",
      "DEBUG \t batch ate = -0.137001\n",
      "DEBUG \t batch ate = -0.0577906\n",
      "DEBUG \t batch ate = -0.0750455\n",
      "DEBUG \t batch ate = -0.0449473\n",
      "DEBUG \t batch ate = -0.110148\n",
      "DEBUG \t batch ate = -0.108896\n",
      "DEBUG \t batch ate = -0.0594219\n",
      "DEBUG \t batch ate = -0.0845948\n",
      "DEBUG \t batch ate = -0.0511399\n",
      "DEBUG \t batch ate = -0.0301736\n",
      "DEBUG \t batch ate = -0.0846234\n",
      "DEBUG \t batch ate = -0.12821\n",
      "DEBUG \t batch ate = -0.122765\n",
      "DEBUG \t batch ate = -0.0742341\n",
      "DEBUG \t batch ate = -0.0799123\n",
      "DEBUG \t batch ate = -0.0446661\n",
      "DEBUG \t batch ate = -0.0315228\n",
      "DEBUG \t batch ate = -0.0432041\n",
      "DEBUG \t batch ate = -0.0371497\n",
      "DEBUG \t batch ate = -0.0466465\n",
      "DEBUG \t batch ate = -0.0616004\n",
      "DEBUG \t batch ate = -0.0536708\n",
      "DEBUG \t batch ate = -0.0720947\n",
      "DEBUG \t batch ate = -0.0719956\n",
      "DEBUG \t batch ate = -0.0457895\n",
      "DEBUG \t batch ate = -0.084357\n",
      "DEBUG \t batch ate = -0.00208379\n",
      "DEBUG \t batch ate = -0.0469171\n",
      "DEBUG \t batch ate = -0.0985466\n",
      "DEBUG \t batch ate = -0.0722363\n",
      "DEBUG \t batch ate = -0.0700491\n",
      "DEBUG \t batch ate = -0.0743302\n",
      "DEBUG \t batch ate = -0.0396094\n",
      "DEBUG \t batch ate = -0.0946563\n",
      "DEBUG \t batch ate = -0.058871\n",
      "DEBUG \t batch ate = -0.0722969\n",
      "DEBUG \t batch ate = -0.0846419\n",
      "DEBUG \t batch ate = -0.0424237\n",
      "DEBUG \t batch ate = -0.054277\n",
      "DEBUG \t batch ate = -0.0830123\n",
      "DEBUG \t batch ate = -0.108943\n",
      "DEBUG \t batch ate = -0.096532\n",
      "DEBUG \t batch ate = -0.12012\n",
      "DEBUG \t batch ate = -0.0686008\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "ate_val = cevae.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2eb0c-a9eb-45fe-87e9-79cf1a6743cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Input results in result df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724fc993-9d6b-4e7a-8943-d01d733a471d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for Dragonnet\n",
    "%store -r df_results\n",
    "lib = \"CausalML\"\n",
    "method = \"Dragonnet\"\n",
    "\n",
    "if method in df_results['method'].values:\n",
    "    df_results.loc[df_results['method'] == method, 'ATE'] = dragon_ate\n",
    "    df_results.loc[df_results['method'] == method, 'ITE'] = ite_dragon\n",
    "\n",
    "else:\n",
    "    df_results = df_results._append({'method': method, 'ATE': dragon_ate, 'ITE': ite_dragon, 'Library': lib}, ignore_index=True)\n",
    "\n",
    "%store df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1e115d-9d58-4f04-b758-1ae7ffa057be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_results' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# for CEVAE\n",
    "%store -r df_results\n",
    "ate = ate_val.mean()\n",
    "lib = \"CausalML\"\n",
    "method = \"CEVAE\"\n",
    "\n",
    "if method in df_results['method'].values:\n",
    "    df_results.loc[df_results['method'] == method, 'ATE'] = ate\n",
    "\n",
    "else:\n",
    "    df_results = df_results._append({'method': method, 'ATE': ate, 'Library': lib}, ignore_index=True)\n",
    "\n",
    "%store df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47342b6-7800-4a8c-99c7-ca6808499bee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b701799-0aa4-4025-a57d-b103bc8f7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synth = pd.read_csv(\"synthetic_dataset.csv\")\n",
    "df_synth.head()\n",
    "synthetic_features = ['NumberOfOffers', 'concept:name',\n",
    "       'lifecycle:transition', 'time:timestamp', 'elementId', 'resourceId',\n",
    "       'weekdayApplication', 'timeApplication']\n",
    "\n",
    "t_treatment=np.array([np.array([value]) for value in df_synth['treatment']])\n",
    "x_feature = df_synth[synthetic_features].values\n",
    "y_outcome=df_synth['treatmentSuccess'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627617e8-4114-466d-ae2b-9f103a3dffac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2795/2795 [==============================] - 35s 10ms/step - loss: 1363.5438 - regression_loss: 657.3325 - binary_classification_loss: 44.0058 - treatment_accuracy: 0.6488 - track_epsilon: 0.0119 - val_loss: 65.0797 - val_regression_loss: 8.0731 - val_binary_classification_loss: 49.5177 - val_treatment_accuracy: 0.5405 - val_track_epsilon: 0.0069 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "2795/2795 [==============================] - 24s 8ms/step - loss: 2042.7755 - regression_loss: 1000.8886 - binary_classification_loss: 38.6559 - treatment_accuracy: 0.7872 - track_epsilon: 0.0030 - val_loss: 16.7200 - val_regression_loss: 1.2573 - val_binary_classification_loss: 12.0500 - val_treatment_accuracy: 0.9596 - val_track_epsilon: 0.0031 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "2795/2795 [==============================] - 22s 8ms/step - loss: 17.7302 - regression_loss: 1.2549 - binary_classification_loss: 12.9890 - treatment_accuracy: 0.9569 - track_epsilon: 0.0018 - val_loss: 11.4919 - val_regression_loss: 0.2895 - val_binary_classification_loss: 8.5466 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 0.0014 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "2795/2795 [==============================] - 17s 6ms/step - loss: 14.7635 - regression_loss: 1.4092 - binary_classification_loss: 9.7626 - treatment_accuracy: 0.9687 - track_epsilon: 9.9568e-04 - val_loss: 146.6949 - val_regression_loss: 33.1458 - val_binary_classification_loss: 74.1438 - val_treatment_accuracy: 0.4160 - val_track_epsilon: 0.0033 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "2795/2795 [==============================] - 16s 6ms/step - loss: 24.2040 - regression_loss: 0.4636 - binary_classification_loss: 21.1550 - treatment_accuracy: 0.9291 - track_epsilon: 4.7264e-04 - val_loss: 10.5201 - val_regression_loss: 0.0574 - val_binary_classification_loss: 8.3829 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 1.2057e-04 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "2795/2795 [==============================] - 18s 6ms/step - loss: 66.2719 - regression_loss: 17.2518 - binary_classification_loss: 29.7820 - treatment_accuracy: 0.8614 - track_epsilon: 0.0036 - val_loss: 4.0692 - val_regression_loss: 0.0272 - val_binary_classification_loss: 2.0225 - val_treatment_accuracy: 0.9804 - val_track_epsilon: 2.6849e-04 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "2795/2795 [==============================] - 27s 10ms/step - loss: 17.3002 - regression_loss: 0.1616 - binary_classification_loss: 15.0523 - treatment_accuracy: 0.9586 - track_epsilon: 3.0490e-04 - val_loss: 3.6781 - val_regression_loss: 0.0069 - val_binary_classification_loss: 1.8731 - val_treatment_accuracy: 0.9843 - val_track_epsilon: 1.4432e-04 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "2795/2795 [==============================] - 26s 9ms/step - loss: 5.2428 - regression_loss: 0.0735 - binary_classification_loss: 3.4706 - treatment_accuracy: 0.9823 - track_epsilon: 0.0010 - val_loss: 3.2326 - val_regression_loss: 0.0047 - val_binary_classification_loss: 1.7784 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 3.5432e-04 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "2795/2795 [==============================] - 20s 7ms/step - loss: 2.1432 - regression_loss: 0.0030 - binary_classification_loss: 0.9126 - treatment_accuracy: 0.9929 - track_epsilon: 2.1923e-04 - val_loss: 2.3326 - val_regression_loss: 3.1497e-04 - val_binary_classification_loss: 1.3404 - val_treatment_accuracy: 0.9808 - val_track_epsilon: 6.8371e-06 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "2795/2795 [==============================] - 24s 9ms/step - loss: 16.6240 - regression_loss: 0.4602 - binary_classification_loss: 14.7967 - treatment_accuracy: 0.9389 - track_epsilon: 4.2672e-04 - val_loss: 2.3146 - val_regression_loss: 0.0037 - val_binary_classification_loss: 1.5167 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 1.0606e-04 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "2795/2795 [==============================] - 31s 11ms/step - loss: 1.4772 - regression_loss: 7.4762e-04 - binary_classification_loss: 0.8432 - treatment_accuracy: 0.9934 - track_epsilon: 6.1488e-05 - val_loss: 2.0644 - val_regression_loss: 0.0113 - val_binary_classification_loss: 1.5745 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 7.5950e-05 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "2795/2795 [==============================] - 26s 9ms/step - loss: 1.1469 - regression_loss: 0.0017 - binary_classification_loss: 0.8264 - treatment_accuracy: 0.9935 - track_epsilon: 8.8575e-05 - val_loss: 1.8625 - val_regression_loss: 3.7507e-04 - val_binary_classification_loss: 1.6695 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 8.0063e-05 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "2795/2795 [==============================] - 26s 9ms/step - loss: 21.2376 - regression_loss: 0.0453 - binary_classification_loss: 20.9714 - treatment_accuracy: 0.8887 - track_epsilon: 5.1541e-04 - val_loss: 2.8401 - val_regression_loss: 5.1991e-04 - val_binary_classification_loss: 2.7138 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 3.4280e-04 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "2795/2795 [==============================] - 27s 9ms/step - loss: 1.0995 - regression_loss: 4.8562e-04 - binary_classification_loss: 1.0134 - treatment_accuracy: 0.9936 - track_epsilon: 1.7513e-04 - val_loss: 1.7666 - val_regression_loss: 6.6480e-04 - val_binary_classification_loss: 1.7121 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 1.2617e-04 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "2795/2795 [==============================] - 28s 10ms/step - loss: 0.8608 - regression_loss: 4.1805e-04 - binary_classification_loss: 0.8299 - treatment_accuracy: 0.9934 - track_epsilon: 1.1455e-04 - val_loss: 1.5762 - val_regression_loss: 1.5177e-04 - val_binary_classification_loss: 1.5608 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 8.8233e-05 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "2795/2795 [==============================] - 29s 10ms/step - loss: 0.8389 - regression_loss: 3.5894e-04 - binary_classification_loss: 0.8292 - treatment_accuracy: 0.9935 - track_epsilon: 1.0271e-04 - val_loss: 1.6078 - val_regression_loss: 1.6733e-04 - val_binary_classification_loss: 1.6009 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 1.8051e-04 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "2795/2795 [==============================] - 22s 8ms/step - loss: 0.8350 - regression_loss: 2.2615e-04 - binary_classification_loss: 0.8295 - treatment_accuracy: 0.9935 - track_epsilon: 9.2489e-05 - val_loss: 1.5642 - val_regression_loss: 6.4354e-06 - val_binary_classification_loss: 1.5589 - val_treatment_accuracy: 0.9818 - val_track_epsilon: 1.2205e-04 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "2795/2795 [==============================] - 27s 10ms/step - loss: 136.9826 - regression_loss: 0.0038 - binary_classification_loss: 136.9505 - treatment_accuracy: 0.6894 - track_epsilon: 2.9086e-05 - val_loss: 145.2963 - val_regression_loss: 1.0914e-04 - val_binary_classification_loss: 145.2177 - val_treatment_accuracy: 0.6717 - val_track_epsilon: 4.1690e-06 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "2795/2795 [==============================] - 27s 10ms/step - loss: 158.2722 - regression_loss: 2.8937e-04 - binary_classification_loss: 158.2570 - treatment_accuracy: 0.6422 - track_epsilon: 1.8224e-05 - val_loss: 145.3103 - val_regression_loss: 1.1298e-04 - val_binary_classification_loss: 145.2341 - val_treatment_accuracy: 0.6717 - val_track_epsilon: 1.4611e-05 - lr: 0.0010\n",
      "Epoch 1/100\n",
      "   9/2795 [..............................] - ETA: 36s - loss: 522981824.0000 - regression_loss: 10491740.0000 - binary_classification_loss: 148.4771 - treatment_accuracy: 0.6545 - track_epsilon: 17.3554Batch 9: Invalid loss, terminating training\n",
      "2795/2795 [==============================] - 12s 3ms/step - loss: nan - regression_loss: nan - binary_classification_loss: 148.8348 - treatment_accuracy: 0.6547 - track_epsilon: 131.2619 - val_loss: nan - val_regression_loss: nan - val_binary_classification_loss: nan - val_treatment_accuracy: 0.6717 - val_track_epsilon: 269376002268529634050048.0000 - lr: 1.0000e-05\n",
      "6987/6987 [==============================] - 23s 3ms/step\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " ...\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]] nan\n"
     ]
    }
   ],
   "source": [
    "synth_dragon = DragonNet(neurons_per_layer=100, targeted_reg=True)\n",
    "synth_dragon_ite = synth_dragon.fit_predict(x_feature, t_treatment, y_outcome, return_components=False)\n",
    "synth_dragon_ate = synth_dragon_ite.mean()\n",
    "print(synth_dragon_ite, synth_dragon_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06dd6878-4827-464c-9a40-a5d368d0a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: nan\n",
      "First Quartile: nan\n",
      "Median: nan\n",
      "Third Quartile: nan\n",
      "Maximum: nan\n",
      "Interquartile Range: nan\n",
      "Upper Bound (Outliers): nan\n",
      "Lower Bound (Outliers): nan\n",
      "Outliers: []\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "\n",
    "# Calculate statistics\n",
    "data = np.reshape(synth_dragon_ite, -1)\n",
    "minimum = np.min(data)\n",
    "first_quartile = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "third_quartile = np.percentile(data, 75)\n",
    "maximum = np.max(data)\n",
    "\n",
    "# Interquartile range (IQR)\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Define upper and lower bounds for outliers\n",
    "upper_bound = third_quartile + 1.5 * iqr\n",
    "lower_bound = first_quartile - 1.5 * iqr\n",
    "\n",
    "# Detect outliers\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Minimum:\", minimum)\n",
    "print(\"First Quartile:\", first_quartile)\n",
    "print(\"Median:\", median)\n",
    "print(\"Third Quartile:\", third_quartile)\n",
    "print(\"Maximum:\", maximum)\n",
    "print(\"Interquartile Range:\", iqr)\n",
    "print(\"Upper Bound (Outliers):\", upper_bound)\n",
    "print(\"Lower Bound (Outliers):\", lower_bound)\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "ite_synth_dragon = [minimum, first_quartile, median, third_quartile, maximum, iqr, upper_bound, lower_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9bbbb1-ed32-4972-a56e-9d909ea31a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_synth_train, df_synth_test = train_test_split(df_synth, test_size=0.2, random_state=11101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827bb601-1e03-4235-8568-cd98de539892",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_synth_train[synthetic_features].values\n",
    "X_test = df_synth_test[synthetic_features].values\n",
    "treatment_train = df_synth_train['treatment'].values\n",
    "treatment_test = df_synth_test['treatment'].values\n",
    "y_train = df_synth_train['treatmentSuccess'].values\n",
    "y_test = df_synth_test['treatmentSuccess'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4666c931-1762-406d-8d34-7dc2f721e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cevae model settings\n",
    "outcome_dist = \"normal\"\n",
    "latent_dim = 20\n",
    "hidden_dim = 200\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.01\n",
    "num_layers = 2\n",
    "\n",
    "cevae = CEVAE(outcome_dist=outcome_dist,\n",
    "              latent_dim=latent_dim,\n",
    "              hidden_dim=hidden_dim,\n",
    "              num_epochs=num_epochs,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              learning_rate_decay=learning_rate_decay,\n",
    "              num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced203d6-4cdb-4f05-9b1b-47c076263e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Training with 179 minibatches per epoch\n",
      "DEBUG \t step     0 loss = 22.5471\n",
      "DEBUG \t step   100 loss = 5.76006\n",
      "DEBUG \t step   200 loss = 3.36875\n",
      "DEBUG \t step   300 loss = 2.09083\n",
      "DEBUG \t step   400 loss = 1.69662\n",
      "DEBUG \t step   500 loss = 0.765509\n",
      "DEBUG \t step   600 loss = 0.490193\n",
      "DEBUG \t step   700 loss = 0.127225\n",
      "DEBUG \t step   800 loss = 0.150899\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "synth_losses = cevae.fit(X=torch.tensor(X_train, dtype=torch.float),\n",
    "                   treatment=torch.tensor(treatment_train, dtype=torch.float),\n",
    "                   y=torch.tensor(y_train, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1088a78-df70-4973-9349-c13ac7247dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Evaluating 45 minibatches\n",
      "DEBUG \t batch ate = 1.96903\n",
      "DEBUG \t batch ate = 1.96812\n",
      "DEBUG \t batch ate = 1.96889\n",
      "DEBUG \t batch ate = 1.96825\n",
      "DEBUG \t batch ate = 1.96991\n",
      "DEBUG \t batch ate = 1.96704\n",
      "DEBUG \t batch ate = 1.96895\n",
      "DEBUG \t batch ate = 1.96887\n",
      "DEBUG \t batch ate = 1.96924\n",
      "DEBUG \t batch ate = 1.96949\n",
      "DEBUG \t batch ate = 1.96844\n",
      "DEBUG \t batch ate = 1.96864\n",
      "DEBUG \t batch ate = 1.96885\n",
      "DEBUG \t batch ate = 1.96882\n",
      "DEBUG \t batch ate = 1.96859\n",
      "DEBUG \t batch ate = 1.96905\n",
      "DEBUG \t batch ate = 1.9686\n",
      "DEBUG \t batch ate = 1.97003\n",
      "DEBUG \t batch ate = 1.96898\n",
      "DEBUG \t batch ate = 1.96913\n",
      "DEBUG \t batch ate = 1.96972\n",
      "DEBUG \t batch ate = 1.96717\n",
      "DEBUG \t batch ate = 1.96924\n",
      "DEBUG \t batch ate = 1.96976\n",
      "DEBUG \t batch ate = 1.96751\n",
      "DEBUG \t batch ate = 1.96974\n",
      "DEBUG \t batch ate = 1.96928\n",
      "DEBUG \t batch ate = 1.96844\n",
      "DEBUG \t batch ate = 1.96896\n",
      "DEBUG \t batch ate = 1.96951\n",
      "DEBUG \t batch ate = 1.96915\n",
      "DEBUG \t batch ate = 1.96845\n",
      "DEBUG \t batch ate = 1.96846\n",
      "DEBUG \t batch ate = 1.96725\n",
      "DEBUG \t batch ate = 1.96909\n",
      "DEBUG \t batch ate = 1.96896\n",
      "DEBUG \t batch ate = 1.96873\n",
      "DEBUG \t batch ate = 1.96822\n",
      "DEBUG \t batch ate = 1.96952\n",
      "DEBUG \t batch ate = 1.96845\n",
      "DEBUG \t batch ate = 1.96901\n",
      "DEBUG \t batch ate = 1.96851\n",
      "DEBUG \t batch ate = 1.9685\n",
      "DEBUG \t batch ate = 1.96825\n",
      "DEBUG \t batch ate = 1.96798\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "synth_ite = cevae.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157787e1-246b-47d0-ae01-8b94e8801d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_ate = synth_ite.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2a569a-e793-4464-be00-5f3ee172d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8834785, 1.950274258852005, 1.9614615, 1.9918553829193115, 2.3098311, 0.04158112406730652, 2.0542270690202713, 1.8879025727510452] [0.0014018206, 0.031313766, 0.037440896, 0.031222465, 0.018720448]\n"
     ]
    }
   ],
   "source": [
    "import evaluation_metrics\n",
    "true_ate = 2\n",
    "boxplot = evaluation_metrics.boxplot_ite(synth_ite)\n",
    "metrics = evaluation_metrics.evaluation_metrics(true_ate, synth_ite)\n",
    "print(boxplot, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5abb76d0-f8d5-4535-835e-54c9b9a03728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           method  \\\n",
      "0                                    S-Learner LR   \n",
      "1                                   XGBTRegressor   \n",
      "2                              BaseTRegressor XGB   \n",
      "3                               BaseTRegressor LR   \n",
      "4                              BaseXRegressor XGB   \n",
      "5                               BaseXRegressor LR   \n",
      "6   BaseXRegressor XGB (without propensity score)   \n",
      "7    BaseXRegressor LR (without propensity score)   \n",
      "8                              BaseRRegressor XGB   \n",
      "9                               BaseRRegressor LR   \n",
      "10        BaseRRegressor XGB (with random weight)   \n",
      "11  BaseRRegressor XGB (without propensity score)   \n",
      "12                           Neural Network (MLP)   \n",
      "13                                         BCAUSS   \n",
      "14                                          CEVAE   \n",
      "\n",
      "                                                  ITE       ATE  \\\n",
      "0   [1.999999999999998, 1.9999999999999987, 1.9999...  2.000000   \n",
      "1            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "2            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "3            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "4            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "5            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "6            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "7            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "8   [3.6223648294253508e-06, 3.6223648294253508e-0...  0.000004   \n",
      "9   [-2.9272906491754958, 0.7035801392352368, 1.20...  0.963643   \n",
      "10  [3.6222759263182525e-06, 3.6222759263182525e-0...  0.000004   \n",
      "11  [3.6812089092563838e-06, 3.6812089092563838e-0...  0.000004   \n",
      "12                                                     2.000000   \n",
      "13  [0.0, 4.016592899560254e-23, 5.3845826e-16, 2....  0.003403   \n",
      "14  [1.8834785, 1.950274258852005, 1.9614615, 1.99...  1.968778   \n",
      "\n",
      "                                              metrics  \n",
      "0   [1.698796199394374e-30, 1.2862177740158606e-15...  \n",
      "1                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "2                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "3                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "4                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "5                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "6                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "7                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "8   [3.9999855105538034, 1.9999963776351706, 1.999...  \n",
      "9   [1.415439487411538, 1.036357273412645, 1.18972...  \n",
      "10  [3.9999855109094153, 1.9999963777240737, 1.999...  \n",
      "11  [3.9999852751779157, 1.9999963187910907, 1.999...  \n",
      "12                                                     \n",
      "13  [3.9870214, 1.9965966, 1.9967527, 1.9965966, 0...  \n",
      "14  [0.0014018206, 0.031313766, 0.037440896, 0.031...  \n",
      "Stored 'df_synthetic_results_metric' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store -r df_synthetic_results_metric\n",
    "\n",
    "method = \"CEVAE\"\n",
    "ate = synth_ate\n",
    "ite = boxplot\n",
    "metric = metrics\n",
    "\n",
    "df_synthetic_results_metric = df_synthetic_results_metric._append({'method': method, 'ATE': ate, 'ITE': ite, 'metrics': metric}, ignore_index=True)\n",
    "\n",
    "print(df_synthetic_results_metric)\n",
    "%store df_synthetic_results_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38ca3d-3b69-4c7d-9265-c8171d70e5bb",
   "metadata": {},
   "source": [
    "## Refutation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7642578d-a052-451f-b9be-3881a0f1b815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeboDataset2.csv\n",
      "Epoch 1/30\n",
      "14979/14979 [==============================] - 40s 3ms/step - loss: 2940743.5000 - regression_loss: 1470298.7500 - binary_classification_loss: 222.7343 - treatment_accuracy: 0.4964 - track_epsilon: 0.0036 - val_loss: 274.2520 - val_regression_loss: 23.6782 - val_binary_classification_loss: 222.9408 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 6.2120e-05 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 35492.7812 - regression_loss: 17642.8223 - binary_classification_loss: 194.8339 - treatment_accuracy: 0.4971 - track_epsilon: 0.0013 - val_loss: 79.3366 - val_regression_loss: 15.3410 - val_binary_classification_loss: 44.6888 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0022 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 567.1451 - regression_loss: 259.7182 - binary_classification_loss: 44.7305 - treatment_accuracy: 0.5002 - track_epsilon: 0.0104 - val_loss: 83.4572 - val_regression_loss: 18.0763 - val_binary_classification_loss: 44.5273 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0464 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 80.6541 - regression_loss: 16.5355 - binary_classification_loss: 44.5314 - treatment_accuracy: 0.4993 - track_epsilon: 0.0122 - val_loss: 78.6893 - val_regression_loss: 16.0505 - val_binary_classification_loss: 45.4910 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 1.9662e-04 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 77.2001 - regression_loss: 16.1649 - binary_classification_loss: 44.5328 - treatment_accuracy: 0.5004 - track_epsilon: 7.6274e-04 - val_loss: 77.4897 - val_regression_loss: 16.3793 - val_binary_classification_loss: 44.6275 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 4.4335e-04 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.9256 - regression_loss: 16.1429 - binary_classification_loss: 44.5346 - treatment_accuracy: 0.5002 - track_epsilon: 8.1756e-04 - val_loss: 76.5794 - val_regression_loss: 16.0545 - val_binary_classification_loss: 44.3891 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 4.7305e-04 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.8987 - regression_loss: 16.1351 - binary_classification_loss: 44.5378 - treatment_accuracy: 0.5007 - track_epsilon: 8.5237e-04 - val_loss: 76.6008 - val_regression_loss: 16.0172 - val_binary_classification_loss: 44.4823 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0016 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.8439 - regression_loss: 16.1217 - binary_classification_loss: 44.5256 - treatment_accuracy: 0.5011 - track_epsilon: 7.9576e-04 - val_loss: 77.1650 - val_regression_loss: 16.3448 - val_binary_classification_loss: 44.3731 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 6.8472e-04 - lr: 0.0010\n",
      "Epoch 1/100\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 76.7641 - regression_loss: 16.1104 - binary_classification_loss: 44.4314 - treatment_accuracy: 0.5000 - track_epsilon: 9.6340e-04 - val_loss: 76.6874 - val_regression_loss: 16.0420 - val_binary_classification_loss: 44.4953 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.7287 - regression_loss: 16.0993 - binary_classification_loss: 44.4253 - treatment_accuracy: 0.5004 - track_epsilon: 0.0010 - val_loss: 76.5565 - val_regression_loss: 16.0475 - val_binary_classification_loss: 44.3622 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 8.7832e-04 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.7096 - regression_loss: 16.0941 - binary_classification_loss: 44.4231 - treatment_accuracy: 0.5002 - track_epsilon: 0.0011 - val_loss: 76.5879 - val_regression_loss: 16.0717 - val_binary_classification_loss: 44.3570 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 1.6316e-04 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.6987 - regression_loss: 16.0920 - binary_classification_loss: 44.4230 - treatment_accuracy: 0.5013 - track_epsilon: 0.0011 - val_loss: 76.5071 - val_regression_loss: 16.0147 - val_binary_classification_loss: 44.3919 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.6955 - regression_loss: 16.0912 - binary_classification_loss: 44.4265 - treatment_accuracy: 0.5003 - track_epsilon: 0.0011 - val_loss: 77.0051 - val_regression_loss: 16.1413 - val_binary_classification_loss: 44.6472 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 3.2938e-04 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.6887 - regression_loss: 16.0914 - binary_classification_loss: 44.4245 - treatment_accuracy: 0.5013 - track_epsilon: 0.0011 - val_loss: 76.8669 - val_regression_loss: 16.1110 - val_binary_classification_loss: 44.5614 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0020 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.6718 - regression_loss: 16.0887 - binary_classification_loss: 44.4189 - treatment_accuracy: 0.5016 - track_epsilon: 0.0011 - val_loss: 76.5111 - val_regression_loss: 16.0367 - val_binary_classification_loss: 44.3572 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0022 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.6616 - regression_loss: 16.0841 - binary_classification_loss: 44.4234 - treatment_accuracy: 0.5008 - track_epsilon: 0.0011 - val_loss: 76.6664 - val_regression_loss: 16.0308 - val_binary_classification_loss: 44.5344 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.6539 - regression_loss: 16.0808 - binary_classification_loss: 44.4275 - treatment_accuracy: 0.5003 - track_epsilon: 0.0011 - val_loss: 76.7628 - val_regression_loss: 16.1240 - val_binary_classification_loss: 44.4594 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.6567 - regression_loss: 16.0840 - binary_classification_loss: 44.4278 - treatment_accuracy: 0.5000 - track_epsilon: 0.0012 - val_loss: 76.9185 - val_regression_loss: 16.1444 - val_binary_classification_loss: 44.5515 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0024 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.6338 - regression_loss: 16.0741 - binary_classification_loss: 44.4294 - treatment_accuracy: 0.4998 - track_epsilon: 0.0012 - val_loss: 76.8260 - val_regression_loss: 16.0206 - val_binary_classification_loss: 44.7380 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 2.5737e-04 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.6201 - regression_loss: 16.0711 - binary_classification_loss: 44.4254 - treatment_accuracy: 0.5007 - track_epsilon: 0.0013 - val_loss: 76.4708 - val_regression_loss: 16.0074 - val_binary_classification_loss: 44.4167 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.6086 - regression_loss: 16.0681 - binary_classification_loss: 44.4236 - treatment_accuracy: 0.5003 - track_epsilon: 0.0013 - val_loss: 76.4025 - val_regression_loss: 15.9927 - val_binary_classification_loss: 44.3782 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 3.8720e-04 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.5948 - regression_loss: 16.0623 - binary_classification_loss: 44.4249 - treatment_accuracy: 0.5012 - track_epsilon: 0.0014 - val_loss: 76.6786 - val_regression_loss: 16.1204 - val_binary_classification_loss: 44.3935 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0024 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5831 - regression_loss: 16.0584 - binary_classification_loss: 44.4239 - treatment_accuracy: 0.5009 - track_epsilon: 0.0014 - val_loss: 76.5320 - val_regression_loss: 16.0034 - val_binary_classification_loss: 44.4803 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0024 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5645 - regression_loss: 16.0495 - binary_classification_loss: 44.4258 - treatment_accuracy: 0.5012 - track_epsilon: 0.0015 - val_loss: 76.4058 - val_regression_loss: 16.0003 - val_binary_classification_loss: 44.3677 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0022 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "14979/14979 [==============================] - 37s 3ms/step - loss: 76.5550 - regression_loss: 16.0456 - binary_classification_loss: 44.4263 - treatment_accuracy: 0.5002 - track_epsilon: 0.0016 - val_loss: 76.3735 - val_regression_loss: 15.9918 - val_binary_classification_loss: 44.3559 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0018 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5430 - regression_loss: 16.0390 - binary_classification_loss: 44.4291 - treatment_accuracy: 0.4998 - track_epsilon: 0.0017 - val_loss: 76.3861 - val_regression_loss: 15.9944 - val_binary_classification_loss: 44.3710 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 4.1172e-04 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.5285 - regression_loss: 16.0350 - binary_classification_loss: 44.4239 - treatment_accuracy: 0.5019 - track_epsilon: 0.0018 - val_loss: 76.3797 - val_regression_loss: 15.9939 - val_binary_classification_loss: 44.3656 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 9.2775e-04 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.5223 - regression_loss: 16.0306 - binary_classification_loss: 44.4285 - treatment_accuracy: 0.5006 - track_epsilon: 0.0018 - val_loss: 76.4962 - val_regression_loss: 16.0371 - val_binary_classification_loss: 44.3690 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0047 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.5001 - regression_loss: 16.0240 - binary_classification_loss: 44.4196 - treatment_accuracy: 0.5008 - track_epsilon: 0.0020 - val_loss: 76.4294 - val_regression_loss: 16.0181 - val_binary_classification_loss: 44.3646 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0024 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4950 - regression_loss: 16.0184 - binary_classification_loss: 44.4263 - treatment_accuracy: 0.5004 - track_epsilon: 0.0023 - val_loss: 76.3675 - val_regression_loss: 15.9919 - val_binary_classification_loss: 44.3587 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 8.8198e-04 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4871 - regression_loss: 16.0146 - binary_classification_loss: 44.4264 - treatment_accuracy: 0.5004 - track_epsilon: 0.0024 - val_loss: 76.5765 - val_regression_loss: 15.9894 - val_binary_classification_loss: 44.5731 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4748 - regression_loss: 16.0088 - binary_classification_loss: 44.4266 - treatment_accuracy: 0.5004 - track_epsilon: 0.0026 - val_loss: 76.9080 - val_regression_loss: 15.9892 - val_binary_classification_loss: 44.9007 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0028 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4678 - regression_loss: 16.0038 - binary_classification_loss: 44.4297 - treatment_accuracy: 0.5004 - track_epsilon: 0.0027 - val_loss: 76.4064 - val_regression_loss: 16.0045 - val_binary_classification_loss: 44.3566 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0037 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4630 - regression_loss: 16.0032 - binary_classification_loss: 44.4260 - treatment_accuracy: 0.5002 - track_epsilon: 0.0030 - val_loss: 76.3667 - val_regression_loss: 15.9930 - val_binary_classification_loss: 44.3565 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0020 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4571 - regression_loss: 16.0018 - binary_classification_loss: 44.4237 - treatment_accuracy: 0.5009 - track_epsilon: 0.0030 - val_loss: 76.4660 - val_regression_loss: 15.9874 - val_binary_classification_loss: 44.4675 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4566 - regression_loss: 16.0013 - binary_classification_loss: 44.4243 - treatment_accuracy: 0.5012 - track_epsilon: 0.0030 - val_loss: 76.3943 - val_regression_loss: 15.9974 - val_binary_classification_loss: 44.3564 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0044 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4550 - regression_loss: 15.9999 - binary_classification_loss: 44.4266 - treatment_accuracy: 0.5000 - track_epsilon: 0.0030 - val_loss: 76.5256 - val_regression_loss: 15.9933 - val_binary_classification_loss: 44.5110 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0021 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4524 - regression_loss: 15.9999 - binary_classification_loss: 44.4242 - treatment_accuracy: 0.5006 - track_epsilon: 0.0029 - val_loss: 76.4993 - val_regression_loss: 15.9880 - val_binary_classification_loss: 44.5027 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4494 - regression_loss: 15.9989 - binary_classification_loss: 44.4237 - treatment_accuracy: 0.5012 - track_epsilon: 0.0031 - val_loss: 76.3821 - val_regression_loss: 15.9843 - val_binary_classification_loss: 44.3915 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "14979/14979 [==============================] - 33s 2ms/step - loss: 76.4505 - regression_loss: 15.9990 - binary_classification_loss: 44.4250 - treatment_accuracy: 0.4999 - track_epsilon: 0.0032 - val_loss: 76.4564 - val_regression_loss: 15.9994 - val_binary_classification_loss: 44.4383 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0024 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4544 - regression_loss: 15.9994 - binary_classification_loss: 44.4284 - treatment_accuracy: 0.5006 - track_epsilon: 0.0030 - val_loss: 76.3812 - val_regression_loss: 15.9850 - val_binary_classification_loss: 44.3837 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0051 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "14979/14979 [==============================] - 34s 2ms/step - loss: 76.4458 - regression_loss: 15.9979 - binary_classification_loss: 44.4234 - treatment_accuracy: 0.5004 - track_epsilon: 0.0033 - val_loss: 76.4841 - val_regression_loss: 15.9921 - val_binary_classification_loss: 44.4680 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0036 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4508 - regression_loss: 15.9980 - binary_classification_loss: 44.4280 - treatment_accuracy: 0.5003 - track_epsilon: 0.0033 - val_loss: 76.5133 - val_regression_loss: 15.9920 - val_binary_classification_loss: 44.4919 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0047 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4488 - regression_loss: 15.9970 - binary_classification_loss: 44.4296 - treatment_accuracy: 0.5003 - track_epsilon: 0.0033 - val_loss: 76.4322 - val_regression_loss: 15.9987 - val_binary_classification_loss: 44.3983 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0037 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4419 - regression_loss: 15.9970 - binary_classification_loss: 44.4230 - treatment_accuracy: 0.5011 - track_epsilon: 0.0033 - val_loss: 76.4846 - val_regression_loss: 15.9831 - val_binary_classification_loss: 44.5012 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4452 - regression_loss: 15.9970 - binary_classification_loss: 44.4260 - treatment_accuracy: 0.5003 - track_epsilon: 0.0032 - val_loss: 76.3904 - val_regression_loss: 15.9916 - val_binary_classification_loss: 44.3884 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0020 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4436 - regression_loss: 15.9961 - binary_classification_loss: 44.4269 - treatment_accuracy: 0.5000 - track_epsilon: 0.0032 - val_loss: 77.0103 - val_regression_loss: 15.9845 - val_binary_classification_loss: 45.0220 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0016 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4437 - regression_loss: 15.9968 - binary_classification_loss: 44.4256 - treatment_accuracy: 0.5007 - track_epsilon: 0.0031 - val_loss: 76.3650 - val_regression_loss: 15.9892 - val_binary_classification_loss: 44.3689 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4411 - regression_loss: 15.9959 - binary_classification_loss: 44.4255 - treatment_accuracy: 0.4998 - track_epsilon: 0.0033 - val_loss: 76.4659 - val_regression_loss: 15.9940 - val_binary_classification_loss: 44.4541 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0022 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4369 - regression_loss: 15.9953 - binary_classification_loss: 44.4236 - treatment_accuracy: 0.5012 - track_epsilon: 0.0033 - val_loss: 76.4642 - val_regression_loss: 15.9878 - val_binary_classification_loss: 44.4383 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0080 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4391 - regression_loss: 15.9953 - binary_classification_loss: 44.4256 - treatment_accuracy: 0.5002 - track_epsilon: 0.0034 - val_loss: 76.4170 - val_regression_loss: 15.9891 - val_binary_classification_loss: 44.4187 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0014 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.4369 - regression_loss: 15.9944 - binary_classification_loss: 44.4246 - treatment_accuracy: 0.5011 - track_epsilon: 0.0037 - val_loss: 76.4175 - val_regression_loss: 15.9845 - val_binary_classification_loss: 44.4256 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4405 - regression_loss: 15.9948 - binary_classification_loss: 44.4288 - treatment_accuracy: 0.4999 - track_epsilon: 0.0034 - val_loss: 76.3633 - val_regression_loss: 15.9900 - val_binary_classification_loss: 44.3666 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 8.3584e-04 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4361 - regression_loss: 15.9944 - binary_classification_loss: 44.4249 - treatment_accuracy: 0.5001 - track_epsilon: 0.0036 - val_loss: 76.3399 - val_regression_loss: 15.9839 - val_binary_classification_loss: 44.3564 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 5.1844e-04 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4356 - regression_loss: 15.9945 - binary_classification_loss: 44.4240 - treatment_accuracy: 0.5007 - track_epsilon: 0.0036 - val_loss: 76.3487 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3609 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0049 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4375 - regression_loss: 15.9941 - binary_classification_loss: 44.4270 - treatment_accuracy: 0.5010 - track_epsilon: 0.0035 - val_loss: 76.3839 - val_regression_loss: 15.9837 - val_binary_classification_loss: 44.3950 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0042 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4381 - regression_loss: 15.9942 - binary_classification_loss: 44.4276 - treatment_accuracy: 0.5005 - track_epsilon: 0.0034 - val_loss: 76.8495 - val_regression_loss: 15.9850 - val_binary_classification_loss: 44.8594 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0026 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4359 - regression_loss: 15.9940 - binary_classification_loss: 44.4264 - treatment_accuracy: 0.5009 - track_epsilon: 0.0034 - val_loss: 76.5153 - val_regression_loss: 15.9863 - val_binary_classification_loss: 44.5278 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4357 - regression_loss: 15.9935 - binary_classification_loss: 44.4270 - treatment_accuracy: 0.4993 - track_epsilon: 0.0035 - val_loss: 76.4606 - val_regression_loss: 15.9888 - val_binary_classification_loss: 44.4611 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0030 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.4345 - regression_loss: 15.9933 - binary_classification_loss: 44.4268 - treatment_accuracy: 0.4997 - track_epsilon: 0.0035 - val_loss: 76.3609 - val_regression_loss: 15.9839 - val_binary_classification_loss: 44.3762 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4295 - regression_loss: 15.9932 - binary_classification_loss: 44.4222 - treatment_accuracy: 0.5009 - track_epsilon: 0.0036 - val_loss: 76.5106 - val_regression_loss: 15.9833 - val_binary_classification_loss: 44.5290 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0018 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4332 - regression_loss: 15.9932 - binary_classification_loss: 44.4265 - treatment_accuracy: 0.5007 - track_epsilon: 0.0035 - val_loss: 76.4294 - val_regression_loss: 15.9830 - val_binary_classification_loss: 44.4371 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0058 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.4315 - regression_loss: 15.9936 - binary_classification_loss: 44.4231 - treatment_accuracy: 0.5005 - track_epsilon: 0.0035 - val_loss: 76.4138 - val_regression_loss: 16.0083 - val_binary_classification_loss: 44.3571 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0069 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4322 - regression_loss: 15.9933 - binary_classification_loss: 44.4252 - treatment_accuracy: 0.5004 - track_epsilon: 0.0035 - val_loss: 76.3427 - val_regression_loss: 15.9829 - val_binary_classification_loss: 44.3564 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0043 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "14979/14979 [==============================] - 34s 2ms/step - loss: 76.4342 - regression_loss: 15.9935 - binary_classification_loss: 44.4266 - treatment_accuracy: 0.5004 - track_epsilon: 0.0035 - val_loss: 76.3431 - val_regression_loss: 15.9828 - val_binary_classification_loss: 44.3638 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0024 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "14979/14979 [==============================] - ETA: 0s - loss: 76.4356 - regression_loss: 15.9932 - binary_classification_loss: 44.4293 - treatment_accuracy: 0.5004 - track_epsilon: 0.0037\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4356 - regression_loss: 15.9932 - binary_classification_loss: 44.4293 - treatment_accuracy: 0.5004 - track_epsilon: 0.0037 - val_loss: 76.3909 - val_regression_loss: 15.9857 - val_binary_classification_loss: 44.3808 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0070 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3958 - regression_loss: 15.9917 - binary_classification_loss: 44.3960 - treatment_accuracy: 0.5004 - track_epsilon: 0.0024 - val_loss: 76.4036 - val_regression_loss: 15.9834 - val_binary_classification_loss: 44.4191 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0039 - lr: 5.0000e-06\n",
      "Epoch 60/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3949 - regression_loss: 15.9919 - binary_classification_loss: 44.3948 - treatment_accuracy: 0.5009 - track_epsilon: 0.0024 - val_loss: 76.3964 - val_regression_loss: 15.9882 - val_binary_classification_loss: 44.3906 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0049 - lr: 5.0000e-06\n",
      "Epoch 61/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3957 - regression_loss: 15.9916 - binary_classification_loss: 44.3965 - treatment_accuracy: 0.4999 - track_epsilon: 0.0026 - val_loss: 76.3770 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3888 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0055 - lr: 5.0000e-06\n",
      "Epoch 62/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3976 - regression_loss: 15.9919 - binary_classification_loss: 44.3980 - treatment_accuracy: 0.5008 - track_epsilon: 0.0025 - val_loss: 76.4135 - val_regression_loss: 15.9847 - val_binary_classification_loss: 44.4125 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0073 - lr: 5.0000e-06\n",
      "Epoch 63/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3936 - regression_loss: 15.9918 - binary_classification_loss: 44.3942 - treatment_accuracy: 0.5017 - track_epsilon: 0.0024 - val_loss: 76.3513 - val_regression_loss: 15.9849 - val_binary_classification_loss: 44.3671 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 7.0634e-04 - lr: 5.0000e-06\n",
      "Epoch 64/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3967 - regression_loss: 15.9916 - binary_classification_loss: 44.3971 - treatment_accuracy: 0.5000 - track_epsilon: 0.0026 - val_loss: 76.3619 - val_regression_loss: 15.9830 - val_binary_classification_loss: 44.3771 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0034 - lr: 5.0000e-06\n",
      "Epoch 65/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3950 - regression_loss: 15.9916 - binary_classification_loss: 44.3959 - treatment_accuracy: 0.5009 - track_epsilon: 0.0026 - val_loss: 76.3490 - val_regression_loss: 15.9882 - val_binary_classification_loss: 44.3591 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 1.1227e-04 - lr: 5.0000e-06\n",
      "Epoch 66/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3936 - regression_loss: 15.9914 - binary_classification_loss: 44.3957 - treatment_accuracy: 0.5006 - track_epsilon: 0.0027 - val_loss: 76.4017 - val_regression_loss: 15.9908 - val_binary_classification_loss: 44.3741 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0071 - lr: 5.0000e-06\n",
      "Epoch 67/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3968 - regression_loss: 15.9914 - binary_classification_loss: 44.3982 - treatment_accuracy: 0.4998 - track_epsilon: 0.0025 - val_loss: 76.4313 - val_regression_loss: 15.9926 - val_binary_classification_loss: 44.4283 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0020 - lr: 5.0000e-06\n",
      "Epoch 68/100\n",
      "14959/14979 [============================>.] - ETA: 0s - loss: 76.3946 - regression_loss: 15.9917 - binary_classification_loss: 44.3956 - treatment_accuracy: 0.5008 - track_epsilon: 0.0027\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3944 - regression_loss: 15.9917 - binary_classification_loss: 44.3955 - treatment_accuracy: 0.5008 - track_epsilon: 0.0027 - val_loss: 76.4141 - val_regression_loss: 15.9860 - val_binary_classification_loss: 44.4241 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0029 - lr: 5.0000e-06\n",
      "Epoch 69/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3752 - regression_loss: 15.9908 - binary_classification_loss: 44.3798 - treatment_accuracy: 0.5012 - track_epsilon: 0.0018 - val_loss: 76.3506 - val_regression_loss: 15.9850 - val_binary_classification_loss: 44.3654 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0022 - lr: 2.5000e-06\n",
      "Epoch 70/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3734 - regression_loss: 15.9907 - binary_classification_loss: 44.3790 - treatment_accuracy: 0.5004 - track_epsilon: 0.0017 - val_loss: 76.3516 - val_regression_loss: 15.9850 - val_binary_classification_loss: 44.3589 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0046 - lr: 2.5000e-06\n",
      "Epoch 71/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3751 - regression_loss: 15.9910 - binary_classification_loss: 44.3797 - treatment_accuracy: 0.5006 - track_epsilon: 0.0019 - val_loss: 76.3427 - val_regression_loss: 15.9861 - val_binary_classification_loss: 44.3579 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 6.0177e-04 - lr: 2.5000e-06\n",
      "Epoch 72/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3746 - regression_loss: 15.9908 - binary_classification_loss: 44.3797 - treatment_accuracy: 0.5008 - track_epsilon: 0.0019 - val_loss: 76.3441 - val_regression_loss: 15.9852 - val_binary_classification_loss: 44.3619 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0011 - lr: 2.5000e-06\n",
      "Epoch 73/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3727 - regression_loss: 15.9907 - binary_classification_loss: 44.3782 - treatment_accuracy: 0.5018 - track_epsilon: 0.0017 - val_loss: 76.3718 - val_regression_loss: 15.9839 - val_binary_classification_loss: 44.3849 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0032 - lr: 2.5000e-06\n",
      "Epoch 74/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3752 - regression_loss: 15.9910 - binary_classification_loss: 44.3798 - treatment_accuracy: 0.5008 - track_epsilon: 0.0017 - val_loss: 76.3348 - val_regression_loss: 15.9832 - val_binary_classification_loss: 44.3560 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0020 - lr: 2.5000e-06\n",
      "Epoch 75/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3738 - regression_loss: 15.9907 - binary_classification_loss: 44.3794 - treatment_accuracy: 0.5008 - track_epsilon: 0.0018 - val_loss: 76.3763 - val_regression_loss: 15.9850 - val_binary_classification_loss: 44.3931 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0012 - lr: 2.5000e-06\n",
      "Epoch 76/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3730 - regression_loss: 15.9910 - binary_classification_loss: 44.3778 - treatment_accuracy: 0.5013 - track_epsilon: 0.0017 - val_loss: 76.3498 - val_regression_loss: 15.9828 - val_binary_classification_loss: 44.3710 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0029 - lr: 2.5000e-06\n",
      "Epoch 77/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3739 - regression_loss: 15.9910 - binary_classification_loss: 44.3786 - treatment_accuracy: 0.5006 - track_epsilon: 0.0016 - val_loss: 76.3388 - val_regression_loss: 15.9826 - val_binary_classification_loss: 44.3603 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0016 - lr: 2.5000e-06\n",
      "Epoch 78/100\n",
      "14962/14979 [============================>.] - ETA: 0s - loss: 76.3727 - regression_loss: 15.9911 - binary_classification_loss: 44.3780 - treatment_accuracy: 0.5009 - track_epsilon: 0.0018\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.3728 - regression_loss: 15.9910 - binary_classification_loss: 44.3781 - treatment_accuracy: 0.5009 - track_epsilon: 0.0018 - val_loss: 76.3392 - val_regression_loss: 15.9843 - val_binary_classification_loss: 44.3567 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0014 - lr: 2.5000e-06\n",
      "Epoch 79/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3615 - regression_loss: 15.9906 - binary_classification_loss: 44.3682 - treatment_accuracy: 0.5019 - track_epsilon: 0.0011 - val_loss: 76.3397 - val_regression_loss: 15.9829 - val_binary_classification_loss: 44.3612 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 3.5541e-04 - lr: 1.2500e-06\n",
      "Epoch 80/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3643 - regression_loss: 15.9905 - binary_classification_loss: 44.3714 - treatment_accuracy: 0.5005 - track_epsilon: 0.0012 - val_loss: 76.3420 - val_regression_loss: 15.9835 - val_binary_classification_loss: 44.3604 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0014 - lr: 1.2500e-06\n",
      "Epoch 81/100\n",
      "14979/14979 [==============================] - 37s 3ms/step - loss: 76.3638 - regression_loss: 15.9905 - binary_classification_loss: 44.3709 - treatment_accuracy: 0.5009 - track_epsilon: 0.0012 - val_loss: 76.3380 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3563 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0037 - lr: 1.2500e-06\n",
      "Epoch 82/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3633 - regression_loss: 15.9908 - binary_classification_loss: 44.3701 - treatment_accuracy: 0.5018 - track_epsilon: 0.0011 - val_loss: 76.3363 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3580 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0021 - lr: 1.2500e-06\n",
      "Epoch 83/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3623 - regression_loss: 15.9904 - binary_classification_loss: 44.3700 - treatment_accuracy: 0.5015 - track_epsilon: 0.0013 - val_loss: 76.3403 - val_regression_loss: 15.9828 - val_binary_classification_loss: 44.3605 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 0.0017 - lr: 1.2500e-06\n",
      "Epoch 84/100\n",
      "14969/14979 [============================>.] - ETA: 0s - loss: 76.3631 - regression_loss: 15.9906 - binary_classification_loss: 44.3702 - treatment_accuracy: 0.5007 - track_epsilon: 0.0012\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3630 - regression_loss: 15.9906 - binary_classification_loss: 44.3701 - treatment_accuracy: 0.5007 - track_epsilon: 0.0012 - val_loss: 76.3379 - val_regression_loss: 15.9834 - val_binary_classification_loss: 44.3581 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0017 - lr: 1.2500e-06\n",
      "Epoch 85/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3567 - regression_loss: 15.9904 - binary_classification_loss: 44.3647 - treatment_accuracy: 0.5017 - track_epsilon: 7.8747e-04 - val_loss: 76.3355 - val_regression_loss: 15.9834 - val_binary_classification_loss: 44.3559 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 1.0636e-04 - lr: 6.2500e-07\n",
      "Epoch 86/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3571 - regression_loss: 15.9904 - binary_classification_loss: 44.3653 - treatment_accuracy: 0.5018 - track_epsilon: 8.1345e-04 - val_loss: 76.3349 - val_regression_loss: 15.9833 - val_binary_classification_loss: 44.3559 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 7.6769e-04 - lr: 6.2500e-07\n",
      "Epoch 87/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3576 - regression_loss: 15.9904 - binary_classification_loss: 44.3660 - treatment_accuracy: 0.5008 - track_epsilon: 9.0647e-04 - val_loss: 76.3390 - val_regression_loss: 15.9845 - val_binary_classification_loss: 44.3574 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 8.7919e-05 - lr: 6.2500e-07\n",
      "Epoch 88/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3569 - regression_loss: 15.9903 - binary_classification_loss: 44.3652 - treatment_accuracy: 0.5019 - track_epsilon: 7.9412e-04 - val_loss: 76.3407 - val_regression_loss: 15.9859 - val_binary_classification_loss: 44.3569 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 8.3899e-04 - lr: 6.2500e-07\n",
      "Epoch 89/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3562 - regression_loss: 15.9902 - binary_classification_loss: 44.3650 - treatment_accuracy: 0.5016 - track_epsilon: 8.0955e-04 - val_loss: 76.3350 - val_regression_loss: 15.9828 - val_binary_classification_loss: 44.3568 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0016 - lr: 6.2500e-07\n",
      "Epoch 90/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3559 - regression_loss: 15.9903 - binary_classification_loss: 44.3647 - treatment_accuracy: 0.5019 - track_epsilon: 8.6516e-04 - val_loss: 76.3353 - val_regression_loss: 15.9832 - val_binary_classification_loss: 44.3572 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0012 - lr: 6.2500e-07\n",
      "Epoch 91/100\n",
      "14979/14979 [==============================] - 34s 2ms/step - loss: 76.3573 - regression_loss: 15.9903 - binary_classification_loss: 44.3659 - treatment_accuracy: 0.5018 - track_epsilon: 9.6178e-04 - val_loss: 76.3391 - val_regression_loss: 15.9840 - val_binary_classification_loss: 44.3581 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0012 - lr: 6.2500e-07\n",
      "Epoch 92/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3564 - regression_loss: 15.9903 - binary_classification_loss: 44.3652 - treatment_accuracy: 0.5014 - track_epsilon: 9.0684e-04 - val_loss: 76.3414 - val_regression_loss: 15.9850 - val_binary_classification_loss: 44.3596 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 5.8744e-04 - lr: 6.2500e-07\n",
      "Epoch 93/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3567 - regression_loss: 15.9904 - binary_classification_loss: 44.3649 - treatment_accuracy: 0.5016 - track_epsilon: 8.1541e-04 - val_loss: 76.3385 - val_regression_loss: 15.9843 - val_binary_classification_loss: 44.3570 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0012 - lr: 6.2500e-07\n",
      "Epoch 94/100\n",
      "14979/14979 [==============================] - 34s 2ms/step - loss: 76.3563 - regression_loss: 15.9901 - binary_classification_loss: 44.3650 - treatment_accuracy: 0.5023 - track_epsilon: 8.7727e-04 - val_loss: 76.3370 - val_regression_loss: 15.9832 - val_binary_classification_loss: 44.3591 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 7.6890e-04 - lr: 6.2500e-07\n",
      "Epoch 95/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3556 - regression_loss: 15.9902 - binary_classification_loss: 44.3646 - treatment_accuracy: 0.5012 - track_epsilon: 8.4424e-04 - val_loss: 76.3341 - val_regression_loss: 15.9827 - val_binary_classification_loss: 44.3569 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0014 - lr: 6.2500e-07\n",
      "Epoch 96/100\n",
      "14979/14979 [==============================] - 34s 2ms/step - loss: 76.3566 - regression_loss: 15.9904 - binary_classification_loss: 44.3651 - treatment_accuracy: 0.5021 - track_epsilon: 8.8818e-04 - val_loss: 76.3363 - val_regression_loss: 15.9835 - val_binary_classification_loss: 44.3564 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0016 - lr: 6.2500e-07\n",
      "Epoch 97/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3556 - regression_loss: 15.9903 - binary_classification_loss: 44.3646 - treatment_accuracy: 0.5024 - track_epsilon: 8.8148e-04 - val_loss: 76.3375 - val_regression_loss: 15.9829 - val_binary_classification_loss: 44.3600 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 5.7875e-04 - lr: 6.2500e-07\n",
      "Epoch 98/100\n",
      "14979/14979 [==============================] - 34s 2ms/step - loss: 76.3561 - regression_loss: 15.9904 - binary_classification_loss: 44.3648 - treatment_accuracy: 0.5025 - track_epsilon: 7.6753e-04 - val_loss: 76.3383 - val_regression_loss: 15.9832 - val_binary_classification_loss: 44.3604 - val_treatment_accuracy: 0.4959 - val_track_epsilon: 5.7242e-04 - lr: 6.2500e-07\n",
      "Epoch 99/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3560 - regression_loss: 15.9903 - binary_classification_loss: 44.3650 - treatment_accuracy: 0.5024 - track_epsilon: 7.8324e-04 - val_loss: 76.3341 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3558 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0019 - lr: 6.2500e-07\n",
      "Epoch 100/100\n",
      "14970/14979 [============================>.] - ETA: 0s - loss: 76.3568 - regression_loss: 15.9905 - binary_classification_loss: 44.3657 - treatment_accuracy: 0.5012 - track_epsilon: 7.5821e-04\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.3567 - regression_loss: 15.9905 - binary_classification_loss: 44.3656 - treatment_accuracy: 0.5012 - track_epsilon: 7.5854e-04 - val_loss: 76.3733 - val_regression_loss: 15.9837 - val_binary_classification_loss: 44.3931 - val_treatment_accuracy: 0.5041 - val_track_epsilon: 0.0014 - lr: 6.2500e-07\n",
      "37448/37448 [==============================] - 33s 865us/step\n",
      "-0.0053493977\n",
      "placeboDataset1.csv\n",
      "Epoch 1/30\n",
      "14979/14979 [==============================] - 41s 3ms/step - loss: 4529208.0000 - regression_loss: 2264521.5000 - binary_classification_loss: 218.4356 - treatment_accuracy: 0.5061 - track_epsilon: 0.0060 - val_loss: 562.8616 - val_regression_loss: 170.6840 - val_binary_classification_loss: 217.2814 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.4170e-05 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 1952.0750 - regression_loss: 876.7127 - binary_classification_loss: 193.2241 - treatment_accuracy: 0.5021 - track_epsilon: 0.0011 - val_loss: 81.2094 - val_regression_loss: 16.2707 - val_binary_classification_loss: 44.3770 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0044 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 80.4616 - regression_loss: 16.3338 - binary_classification_loss: 44.7005 - treatment_accuracy: 0.5004 - track_epsilon: 0.0012 - val_loss: 77.6953 - val_regression_loss: 16.0432 - val_binary_classification_loss: 44.4806 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 3.9220e-04 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 77.2967 - regression_loss: 16.1953 - binary_classification_loss: 44.5483 - treatment_accuracy: 0.5005 - track_epsilon: 7.8801e-04 - val_loss: 76.5335 - val_regression_loss: 16.0394 - val_binary_classification_loss: 44.3492 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0017 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 76.9393 - regression_loss: 16.1418 - binary_classification_loss: 44.5652 - treatment_accuracy: 0.5003 - track_epsilon: 8.6432e-04 - val_loss: 77.0075 - val_regression_loss: 16.2572 - val_binary_classification_loss: 44.3708 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0012 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 76.9126 - regression_loss: 16.1366 - binary_classification_loss: 44.5568 - treatment_accuracy: 0.5000 - track_epsilon: 8.3628e-04 - val_loss: 76.8216 - val_regression_loss: 16.1835 - val_binary_classification_loss: 44.3765 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 4.5371e-04 - lr: 0.0010\n",
      "Epoch 1/100\n",
      "14979/14979 [==============================] - 38s 2ms/step - loss: 76.7447 - regression_loss: 16.1094 - binary_classification_loss: 44.4446 - treatment_accuracy: 0.5011 - track_epsilon: 9.7806e-04 - val_loss: 76.9773 - val_regression_loss: 16.2395 - val_binary_classification_loss: 44.4129 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0014 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.7202 - regression_loss: 16.1045 - binary_classification_loss: 44.4353 - treatment_accuracy: 0.5010 - track_epsilon: 9.7792e-04 - val_loss: 77.1026 - val_regression_loss: 16.3269 - val_binary_classification_loss: 44.3837 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.5662e-04 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "14979/14979 [==============================] - 37s 3ms/step - loss: 76.7098 - regression_loss: 16.1021 - binary_classification_loss: 44.4345 - treatment_accuracy: 0.5009 - track_epsilon: 9.9116e-04 - val_loss: 76.9324 - val_regression_loss: 16.2441 - val_binary_classification_loss: 44.3503 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0022 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "14979/14979 [==============================] - 37s 3ms/step - loss: 76.6990 - regression_loss: 16.1031 - binary_classification_loss: 44.4266 - treatment_accuracy: 0.5007 - track_epsilon: 9.7848e-04 - val_loss: 76.7483 - val_regression_loss: 16.1341 - val_binary_classification_loss: 44.4183 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.6926 - regression_loss: 16.1028 - binary_classification_loss: 44.4261 - treatment_accuracy: 0.5008 - track_epsilon: 0.0010 - val_loss: 76.5357 - val_regression_loss: 16.0564 - val_binary_classification_loss: 44.3734 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 6.2270e-06 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.6692 - regression_loss: 16.0961 - binary_classification_loss: 44.4209 - treatment_accuracy: 0.5019 - track_epsilon: 0.0011 - val_loss: 76.4682 - val_regression_loss: 16.0368 - val_binary_classification_loss: 44.3490 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.8275e-04 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.6594 - regression_loss: 16.0932 - binary_classification_loss: 44.4221 - treatment_accuracy: 0.5012 - track_epsilon: 0.0011 - val_loss: 76.4118 - val_regression_loss: 16.0110 - val_binary_classification_loss: 44.3498 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.0541e-04 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.6480 - regression_loss: 16.0889 - binary_classification_loss: 44.4236 - treatment_accuracy: 0.5016 - track_epsilon: 0.0011 - val_loss: 76.4368 - val_regression_loss: 16.0239 - val_binary_classification_loss: 44.3489 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.6268 - regression_loss: 16.0814 - binary_classification_loss: 44.4222 - treatment_accuracy: 0.5014 - track_epsilon: 0.0012 - val_loss: 76.7650 - val_regression_loss: 16.1158 - val_binary_classification_loss: 44.5030 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 1.4415e-04 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.6095 - regression_loss: 16.0749 - binary_classification_loss: 44.4218 - treatment_accuracy: 0.5006 - track_epsilon: 0.0012 - val_loss: 76.5046 - val_regression_loss: 16.0177 - val_binary_classification_loss: 44.4359 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 9.8610e-04 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5954 - regression_loss: 16.0680 - binary_classification_loss: 44.4249 - treatment_accuracy: 0.5013 - track_epsilon: 0.0013 - val_loss: 76.7001 - val_regression_loss: 16.0429 - val_binary_classification_loss: 44.5908 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 4.6277e-04 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5788 - regression_loss: 16.0628 - binary_classification_loss: 44.4219 - treatment_accuracy: 0.5004 - track_epsilon: 0.0014 - val_loss: 76.7594 - val_regression_loss: 16.0756 - val_binary_classification_loss: 44.5649 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0030 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5640 - regression_loss: 16.0550 - binary_classification_loss: 44.4246 - treatment_accuracy: 0.5011 - track_epsilon: 0.0015 - val_loss: 76.5502 - val_regression_loss: 16.0609 - val_binary_classification_loss: 44.3835 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0033 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5481 - regression_loss: 16.0500 - binary_classification_loss: 44.4203 - treatment_accuracy: 0.5017 - track_epsilon: 0.0015 - val_loss: 76.4896 - val_regression_loss: 16.0446 - val_binary_classification_loss: 44.3617 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0025 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.5412 - regression_loss: 16.0464 - binary_classification_loss: 44.4219 - treatment_accuracy: 0.5022 - track_epsilon: 0.0016 - val_loss: 76.5193 - val_regression_loss: 16.0755 - val_binary_classification_loss: 44.3497 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.4693e-04 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5328 - regression_loss: 16.0419 - binary_classification_loss: 44.4233 - treatment_accuracy: 0.5020 - track_epsilon: 0.0017 - val_loss: 76.4815 - val_regression_loss: 16.0490 - val_binary_classification_loss: 44.3502 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0032 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.5126 - regression_loss: 16.0336 - binary_classification_loss: 44.4203 - treatment_accuracy: 0.5020 - track_epsilon: 0.0018 - val_loss: 76.4777 - val_regression_loss: 15.9853 - val_binary_classification_loss: 44.4873 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0019 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.5057 - regression_loss: 16.0286 - binary_classification_loss: 44.4231 - treatment_accuracy: 0.5013 - track_epsilon: 0.0019 - val_loss: 76.4107 - val_regression_loss: 16.0171 - val_binary_classification_loss: 44.3606 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 1.9103e-04 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4981 - regression_loss: 16.0250 - binary_classification_loss: 44.4229 - treatment_accuracy: 0.5003 - track_epsilon: 0.0020 - val_loss: 76.5735 - val_regression_loss: 16.0163 - val_binary_classification_loss: 44.5043 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0031 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4926 - regression_loss: 16.0214 - binary_classification_loss: 44.4245 - treatment_accuracy: 0.5015 - track_epsilon: 0.0021 - val_loss: 76.4097 - val_regression_loss: 16.0051 - val_binary_classification_loss: 44.3772 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0018 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4833 - regression_loss: 16.0177 - binary_classification_loss: 44.4215 - treatment_accuracy: 0.5015 - track_epsilon: 0.0022 - val_loss: 76.4238 - val_regression_loss: 15.9932 - val_binary_classification_loss: 44.4172 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0019 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "14979/14979 [==============================] - 37s 3ms/step - loss: 76.4774 - regression_loss: 16.0134 - binary_classification_loss: 44.4243 - treatment_accuracy: 0.5010 - track_epsilon: 0.0023 - val_loss: 76.3593 - val_regression_loss: 15.9841 - val_binary_classification_loss: 44.3736 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0027 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4721 - regression_loss: 16.0111 - binary_classification_loss: 44.4222 - treatment_accuracy: 0.5015 - track_epsilon: 0.0026 - val_loss: 76.4584 - val_regression_loss: 15.9992 - val_binary_classification_loss: 44.4380 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0016 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4612 - regression_loss: 16.0080 - binary_classification_loss: 44.4182 - treatment_accuracy: 0.5019 - track_epsilon: 0.0025 - val_loss: 76.3936 - val_regression_loss: 15.9980 - val_binary_classification_loss: 44.3614 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4656 - regression_loss: 16.0064 - binary_classification_loss: 44.4253 - treatment_accuracy: 0.5012 - track_epsilon: 0.0025 - val_loss: 77.2425 - val_regression_loss: 16.0230 - val_binary_classification_loss: 45.1432 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0051 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4576 - regression_loss: 16.0048 - binary_classification_loss: 44.4201 - treatment_accuracy: 0.5008 - track_epsilon: 0.0028 - val_loss: 76.3593 - val_regression_loss: 15.9915 - val_binary_classification_loss: 44.3504 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0020 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4547 - regression_loss: 16.0027 - binary_classification_loss: 44.4211 - treatment_accuracy: 0.5013 - track_epsilon: 0.0029 - val_loss: 76.4218 - val_regression_loss: 16.0100 - val_binary_classification_loss: 44.3818 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 4.8447e-04 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4532 - regression_loss: 16.0017 - binary_classification_loss: 44.4215 - treatment_accuracy: 0.5021 - track_epsilon: 0.0030 - val_loss: 76.3746 - val_regression_loss: 15.9832 - val_binary_classification_loss: 44.3842 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4441 - regression_loss: 16.0008 - binary_classification_loss: 44.4141 - treatment_accuracy: 0.5025 - track_epsilon: 0.0030 - val_loss: 76.5373 - val_regression_loss: 16.0007 - val_binary_classification_loss: 44.4761 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0066 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4476 - regression_loss: 15.9986 - binary_classification_loss: 44.4222 - treatment_accuracy: 0.5008 - track_epsilon: 0.0030 - val_loss: 76.4112 - val_regression_loss: 15.9844 - val_binary_classification_loss: 44.4082 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0047 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4459 - regression_loss: 15.9978 - binary_classification_loss: 44.4220 - treatment_accuracy: 0.5005 - track_epsilon: 0.0031 - val_loss: 76.3735 - val_regression_loss: 15.9960 - val_binary_classification_loss: 44.3517 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0033 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4476 - regression_loss: 15.9985 - binary_classification_loss: 44.4221 - treatment_accuracy: 0.5013 - track_epsilon: 0.0030 - val_loss: 76.3331 - val_regression_loss: 15.9816 - val_binary_classification_loss: 44.3491 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0020 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4512 - regression_loss: 15.9986 - binary_classification_loss: 44.4254 - treatment_accuracy: 0.5008 - track_epsilon: 0.0032 - val_loss: 76.4358 - val_regression_loss: 15.9939 - val_binary_classification_loss: 44.3769 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0087 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "14957/14979 [============================>.] - ETA: 0s - loss: 76.4457 - regression_loss: 15.9973 - binary_classification_loss: 44.4229 - treatment_accuracy: 0.5014 - track_epsilon: 0.0031\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "14979/14979 [==============================] - 34s 2ms/step - loss: 76.4457 - regression_loss: 15.9974 - binary_classification_loss: 44.4229 - treatment_accuracy: 0.5014 - track_epsilon: 0.0031 - val_loss: 76.4169 - val_regression_loss: 15.9895 - val_binary_classification_loss: 44.4189 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0024 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4017 - regression_loss: 15.9931 - binary_classification_loss: 44.3923 - treatment_accuracy: 0.5015 - track_epsilon: 0.0023 - val_loss: 76.4288 - val_regression_loss: 15.9837 - val_binary_classification_loss: 44.4412 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 2.0039e-04 - lr: 5.0000e-06\n",
      "Epoch 36/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4013 - regression_loss: 15.9932 - binary_classification_loss: 44.3926 - treatment_accuracy: 0.5018 - track_epsilon: 0.0022 - val_loss: 76.3353 - val_regression_loss: 15.9814 - val_binary_classification_loss: 44.3525 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.0290e-04 - lr: 5.0000e-06\n",
      "Epoch 37/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3996 - regression_loss: 15.9931 - binary_classification_loss: 44.3909 - treatment_accuracy: 0.5020 - track_epsilon: 0.0022 - val_loss: 76.3394 - val_regression_loss: 15.9837 - val_binary_classification_loss: 44.3498 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0018 - lr: 5.0000e-06\n",
      "Epoch 38/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4030 - regression_loss: 15.9933 - binary_classification_loss: 44.3939 - treatment_accuracy: 0.5012 - track_epsilon: 0.0021 - val_loss: 76.3556 - val_regression_loss: 15.9885 - val_binary_classification_loss: 44.3605 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0016 - lr: 5.0000e-06\n",
      "Epoch 39/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4023 - regression_loss: 15.9935 - binary_classification_loss: 44.3929 - treatment_accuracy: 0.5014 - track_epsilon: 0.0021 - val_loss: 76.3435 - val_regression_loss: 15.9820 - val_binary_classification_loss: 44.3603 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 9.9330e-04 - lr: 5.0000e-06\n",
      "Epoch 40/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.4005 - regression_loss: 15.9932 - binary_classification_loss: 44.3921 - treatment_accuracy: 0.5013 - track_epsilon: 0.0022 - val_loss: 76.3941 - val_regression_loss: 15.9929 - val_binary_classification_loss: 44.3899 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 0.0013 - lr: 5.0000e-06\n",
      "Epoch 41/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.4007 - regression_loss: 15.9933 - binary_classification_loss: 44.3923 - treatment_accuracy: 0.5016 - track_epsilon: 0.0023 - val_loss: 76.3381 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3517 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 42/100\n",
      "14959/14979 [============================>.] - ETA: 0s - loss: 76.4008 - regression_loss: 15.9933 - binary_classification_loss: 44.3924 - treatment_accuracy: 0.5016 - track_epsilon: 0.0023\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.4007 - regression_loss: 15.9933 - binary_classification_loss: 44.3923 - treatment_accuracy: 0.5016 - track_epsilon: 0.0022 - val_loss: 76.4123 - val_regression_loss: 15.9850 - val_binary_classification_loss: 44.4221 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0018 - lr: 5.0000e-06\n",
      "Epoch 43/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3774 - regression_loss: 15.9916 - binary_classification_loss: 44.3748 - treatment_accuracy: 0.5028 - track_epsilon: 0.0016 - val_loss: 76.3503 - val_regression_loss: 15.9842 - val_binary_classification_loss: 44.3523 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0039 - lr: 2.5000e-06\n",
      "Epoch 44/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3778 - regression_loss: 15.9921 - binary_classification_loss: 44.3740 - treatment_accuracy: 0.5027 - track_epsilon: 0.0016 - val_loss: 76.3710 - val_regression_loss: 15.9839 - val_binary_classification_loss: 44.3848 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 7.9019e-04 - lr: 2.5000e-06\n",
      "Epoch 45/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3790 - regression_loss: 15.9917 - binary_classification_loss: 44.3763 - treatment_accuracy: 0.5023 - track_epsilon: 0.0015 - val_loss: 76.3345 - val_regression_loss: 15.9835 - val_binary_classification_loss: 44.3494 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 3.4846e-04 - lr: 2.5000e-06\n",
      "Epoch 46/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3791 - regression_loss: 15.9918 - binary_classification_loss: 44.3767 - treatment_accuracy: 0.5016 - track_epsilon: 0.0014 - val_loss: 76.3493 - val_regression_loss: 15.9844 - val_binary_classification_loss: 44.3619 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 7.9753e-04 - lr: 2.5000e-06\n",
      "Epoch 47/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3782 - regression_loss: 15.9918 - binary_classification_loss: 44.3758 - treatment_accuracy: 0.5020 - track_epsilon: 0.0016 - val_loss: 76.3391 - val_regression_loss: 15.9852 - val_binary_classification_loss: 44.3493 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0014 - lr: 2.5000e-06\n",
      "Epoch 48/100\n",
      "14956/14979 [============================>.] - ETA: 0s - loss: 76.3794 - regression_loss: 15.9920 - binary_classification_loss: 44.3765 - treatment_accuracy: 0.5018 - track_epsilon: 0.0015\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3792 - regression_loss: 15.9920 - binary_classification_loss: 44.3763 - treatment_accuracy: 0.5019 - track_epsilon: 0.0015 - val_loss: 76.3972 - val_regression_loss: 15.9854 - val_binary_classification_loss: 44.4055 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0012 - lr: 2.5000e-06\n",
      "Epoch 49/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3646 - regression_loss: 15.9908 - binary_classification_loss: 44.3657 - treatment_accuracy: 0.5038 - track_epsilon: 0.0012 - val_loss: 76.3332 - val_regression_loss: 15.9829 - val_binary_classification_loss: 44.3491 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 6.5534e-04 - lr: 1.2500e-06\n",
      "Epoch 50/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.3653 - regression_loss: 15.9909 - binary_classification_loss: 44.3663 - treatment_accuracy: 0.5033 - track_epsilon: 0.0010 - val_loss: 76.3362 - val_regression_loss: 15.9828 - val_binary_classification_loss: 44.3535 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 6.9776e-04 - lr: 1.2500e-06\n",
      "Epoch 51/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 76.3646 - regression_loss: 15.9906 - binary_classification_loss: 44.3666 - treatment_accuracy: 0.5033 - track_epsilon: 0.0012 - val_loss: 76.3395 - val_regression_loss: 15.9832 - val_binary_classification_loss: 44.3494 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0031 - lr: 1.2500e-06\n",
      "Epoch 52/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3659 - regression_loss: 15.9909 - binary_classification_loss: 44.3668 - treatment_accuracy: 0.5033 - track_epsilon: 0.0011 - val_loss: 76.3318 - val_regression_loss: 15.9818 - val_binary_classification_loss: 44.3509 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.4780e-06 - lr: 1.2500e-06\n",
      "Epoch 53/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3656 - regression_loss: 15.9908 - binary_classification_loss: 44.3673 - treatment_accuracy: 0.5034 - track_epsilon: 0.0011 - val_loss: 76.3585 - val_regression_loss: 15.9828 - val_binary_classification_loss: 44.3752 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 8.2851e-04 - lr: 1.2500e-06\n",
      "Epoch 54/100\n",
      "14972/14979 [============================>.] - ETA: 0s - loss: 76.3655 - regression_loss: 15.9910 - binary_classification_loss: 44.3666 - treatment_accuracy: 0.5036 - track_epsilon: 9.7682e-04\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3654 - regression_loss: 15.9910 - binary_classification_loss: 44.3666 - treatment_accuracy: 0.5036 - track_epsilon: 9.7637e-04 - val_loss: 76.3480 - val_regression_loss: 15.9820 - val_binary_classification_loss: 44.3669 - val_treatment_accuracy: 0.4913 - val_track_epsilon: 5.6643e-06 - lr: 1.2500e-06\n",
      "Epoch 55/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3587 - regression_loss: 15.9905 - binary_classification_loss: 44.3613 - treatment_accuracy: 0.5040 - track_epsilon: 7.7126e-04 - val_loss: 76.3390 - val_regression_loss: 15.9827 - val_binary_classification_loss: 44.3561 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0012 - lr: 6.2500e-07\n",
      "Epoch 56/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3582 - regression_loss: 15.9905 - binary_classification_loss: 44.3612 - treatment_accuracy: 0.5044 - track_epsilon: 7.9129e-04 - val_loss: 76.3324 - val_regression_loss: 15.9820 - val_binary_classification_loss: 44.3511 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.7882e-04 - lr: 6.2500e-07\n",
      "Epoch 57/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3589 - regression_loss: 15.9902 - binary_classification_loss: 44.3624 - treatment_accuracy: 0.5037 - track_epsilon: 8.6534e-04 - val_loss: 76.3371 - val_regression_loss: 15.9840 - val_binary_classification_loss: 44.3520 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 3.5640e-04 - lr: 6.2500e-07\n",
      "Epoch 58/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3593 - regression_loss: 15.9904 - binary_classification_loss: 44.3621 - treatment_accuracy: 0.5039 - track_epsilon: 7.1500e-04 - val_loss: 76.3335 - val_regression_loss: 15.9830 - val_binary_classification_loss: 44.3492 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 4.7755e-04 - lr: 6.2500e-07\n",
      "Epoch 59/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3590 - regression_loss: 15.9905 - binary_classification_loss: 44.3621 - treatment_accuracy: 0.5038 - track_epsilon: 7.8342e-04 - val_loss: 76.3398 - val_regression_loss: 15.9847 - val_binary_classification_loss: 44.3484 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 0.0021 - lr: 6.2500e-07\n",
      "Epoch 60/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3592 - regression_loss: 15.9906 - binary_classification_loss: 44.3619 - treatment_accuracy: 0.5042 - track_epsilon: 6.4093e-04 - val_loss: 76.3328 - val_regression_loss: 15.9823 - val_binary_classification_loss: 44.3513 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 5.7396e-04 - lr: 6.2500e-07\n",
      "Epoch 61/100\n",
      "14957/14979 [============================>.] - ETA: 0s - loss: 76.3590 - regression_loss: 15.9905 - binary_classification_loss: 44.3624 - treatment_accuracy: 0.5041 - track_epsilon: 7.7520e-04\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3590 - regression_loss: 15.9905 - binary_classification_loss: 44.3623 - treatment_accuracy: 0.5041 - track_epsilon: 7.7521e-04 - val_loss: 76.3336 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3511 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 5.1963e-04 - lr: 6.2500e-07\n",
      "Epoch 62/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3557 - regression_loss: 15.9903 - binary_classification_loss: 44.3597 - treatment_accuracy: 0.5047 - track_epsilon: 4.4572e-04 - val_loss: 76.3309 - val_regression_loss: 15.9819 - val_binary_classification_loss: 44.3492 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.6073e-04 - lr: 3.1250e-07\n",
      "Epoch 63/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3551 - regression_loss: 15.9902 - binary_classification_loss: 44.3592 - treatment_accuracy: 0.5049 - track_epsilon: 4.8674e-04 - val_loss: 76.3343 - val_regression_loss: 15.9829 - val_binary_classification_loss: 44.3517 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 5.7504e-04 - lr: 3.1250e-07\n",
      "Epoch 64/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3558 - regression_loss: 15.9901 - binary_classification_loss: 44.3599 - treatment_accuracy: 0.5052 - track_epsilon: 4.9801e-04 - val_loss: 76.3321 - val_regression_loss: 15.9830 - val_binary_classification_loss: 44.3491 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 4.7538e-04 - lr: 3.1250e-07\n",
      "Epoch 65/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3555 - regression_loss: 15.9903 - binary_classification_loss: 44.3595 - treatment_accuracy: 0.5053 - track_epsilon: 4.6538e-04 - val_loss: 76.3346 - val_regression_loss: 15.9842 - val_binary_classification_loss: 44.3489 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.7472e-04 - lr: 3.1250e-07\n",
      "Epoch 66/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3557 - regression_loss: 15.9903 - binary_classification_loss: 44.3598 - treatment_accuracy: 0.5052 - track_epsilon: 5.5914e-04 - val_loss: 76.3359 - val_regression_loss: 15.9822 - val_binary_classification_loss: 44.3545 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 3.5285e-04 - lr: 3.1250e-07\n",
      "Epoch 67/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3554 - regression_loss: 15.9902 - binary_classification_loss: 44.3597 - treatment_accuracy: 0.5047 - track_epsilon: 4.4428e-04 - val_loss: 76.3314 - val_regression_loss: 15.9827 - val_binary_classification_loss: 44.3485 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 9.0273e-05 - lr: 3.1250e-07\n",
      "Epoch 68/100\n",
      "14973/14979 [============================>.] - ETA: 0s - loss: 76.3557 - regression_loss: 15.9903 - binary_classification_loss: 44.3590 - treatment_accuracy: 0.5053 - track_epsilon: 4.4081e-04\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3556 - regression_loss: 15.9903 - binary_classification_loss: 44.3589 - treatment_accuracy: 0.5053 - track_epsilon: 4.4072e-04 - val_loss: 76.3334 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3519 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.5059e-04 - lr: 3.1250e-07\n",
      "Epoch 69/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3535 - regression_loss: 15.9901 - binary_classification_loss: 44.3583 - treatment_accuracy: 0.5056 - track_epsilon: 2.6093e-04 - val_loss: 76.3344 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3519 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.0791e-04 - lr: 1.5625e-07\n",
      "Epoch 70/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3531 - regression_loss: 15.9900 - binary_classification_loss: 44.3580 - treatment_accuracy: 0.5058 - track_epsilon: 4.3294e-04 - val_loss: 76.3362 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3543 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 9.0504e-04 - lr: 1.5625e-07\n",
      "Epoch 71/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3534 - regression_loss: 15.9900 - binary_classification_loss: 44.3580 - treatment_accuracy: 0.5053 - track_epsilon: 3.8388e-04 - val_loss: 76.3317 - val_regression_loss: 15.9826 - val_binary_classification_loss: 44.3496 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.4259e-04 - lr: 1.5625e-07\n",
      "Epoch 72/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3538 - regression_loss: 15.9901 - binary_classification_loss: 44.3583 - treatment_accuracy: 0.5057 - track_epsilon: 3.3881e-04 - val_loss: 76.3310 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3496 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 9.1376e-05 - lr: 1.5625e-07\n",
      "Epoch 73/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3536 - regression_loss: 15.9901 - binary_classification_loss: 44.3581 - treatment_accuracy: 0.5056 - track_epsilon: 3.6605e-04 - val_loss: 76.3307 - val_regression_loss: 15.9826 - val_binary_classification_loss: 44.3482 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 4.0516e-04 - lr: 1.5625e-07\n",
      "Epoch 74/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3543 - regression_loss: 15.9901 - binary_classification_loss: 44.3585 - treatment_accuracy: 0.5059 - track_epsilon: 3.7953e-04 - val_loss: 76.3371 - val_regression_loss: 15.9828 - val_binary_classification_loss: 44.3543 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 5.3444e-04 - lr: 1.5625e-07\n",
      "Epoch 75/100\n",
      "14967/14979 [============================>.] - ETA: 0s - loss: 76.3541 - regression_loss: 15.9901 - binary_classification_loss: 44.3583 - treatment_accuracy: 0.5059 - track_epsilon: 3.2714e-04\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3541 - regression_loss: 15.9901 - binary_classification_loss: 44.3582 - treatment_accuracy: 0.5059 - track_epsilon: 3.2713e-04 - val_loss: 76.3319 - val_regression_loss: 15.9831 - val_binary_classification_loss: 44.3488 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 3.6493e-04 - lr: 1.5625e-07\n",
      "Epoch 76/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3529 - regression_loss: 15.9900 - binary_classification_loss: 44.3576 - treatment_accuracy: 0.5061 - track_epsilon: 1.6475e-04 - val_loss: 76.3321 - val_regression_loss: 15.9828 - val_binary_classification_loss: 44.3491 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 4.5332e-04 - lr: 7.8125e-08\n",
      "Epoch 77/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3528 - regression_loss: 15.9899 - binary_classification_loss: 44.3574 - treatment_accuracy: 0.5061 - track_epsilon: 2.0484e-04 - val_loss: 76.3298 - val_regression_loss: 15.9821 - val_binary_classification_loss: 44.3491 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.9792e-04 - lr: 7.8125e-08\n",
      "Epoch 78/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3528 - regression_loss: 15.9899 - binary_classification_loss: 44.3575 - treatment_accuracy: 0.5059 - track_epsilon: 1.8845e-04 - val_loss: 76.3303 - val_regression_loss: 15.9821 - val_binary_classification_loss: 44.3493 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 5.8722e-05 - lr: 7.8125e-08\n",
      "Epoch 79/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3528 - regression_loss: 15.9900 - binary_classification_loss: 44.3570 - treatment_accuracy: 0.5061 - track_epsilon: 1.5611e-04 - val_loss: 76.3314 - val_regression_loss: 15.9829 - val_binary_classification_loss: 44.3490 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.9854e-04 - lr: 7.8125e-08\n",
      "Epoch 80/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3522 - regression_loss: 15.9900 - binary_classification_loss: 44.3567 - treatment_accuracy: 0.5061 - track_epsilon: 3.6357e-04 - val_loss: 76.3313 - val_regression_loss: 15.9827 - val_binary_classification_loss: 44.3488 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.6041e-04 - lr: 7.8125e-08\n",
      "Epoch 81/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3530 - regression_loss: 15.9901 - binary_classification_loss: 44.3578 - treatment_accuracy: 0.5061 - track_epsilon: 1.9935e-04 - val_loss: 76.3325 - val_regression_loss: 15.9822 - val_binary_classification_loss: 44.3515 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.0153e-04 - lr: 7.8125e-08\n",
      "Epoch 82/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3530 - regression_loss: 15.9900 - binary_classification_loss: 44.3574 - treatment_accuracy: 0.5061 - track_epsilon: 1.7559e-04 - val_loss: 76.3309 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3493 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 3.3263e-04 - lr: 7.8125e-08\n",
      "Epoch 83/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3525 - regression_loss: 15.9900 - binary_classification_loss: 44.3574 - treatment_accuracy: 0.5061 - track_epsilon: 2.4807e-04 - val_loss: 76.3314 - val_regression_loss: 15.9823 - val_binary_classification_loss: 44.3498 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.5482e-04 - lr: 7.8125e-08\n",
      "Epoch 84/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3530 - regression_loss: 15.9900 - binary_classification_loss: 44.3578 - treatment_accuracy: 0.5061 - track_epsilon: 2.4735e-04 - val_loss: 76.3314 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3501 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 4.9083e-05 - lr: 7.8125e-08\n",
      "Epoch 85/100\n",
      "14969/14979 [============================>.] - ETA: 0s - loss: 76.3524 - regression_loss: 15.9900 - binary_classification_loss: 44.3575 - treatment_accuracy: 0.5061 - track_epsilon: 2.0290e-04\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-08.\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3524 - regression_loss: 15.9900 - binary_classification_loss: 44.3574 - treatment_accuracy: 0.5061 - track_epsilon: 2.0277e-04 - val_loss: 76.3319 - val_regression_loss: 15.9821 - val_binary_classification_loss: 44.3513 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.6100e-05 - lr: 7.8125e-08\n",
      "Epoch 86/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3525 - regression_loss: 15.9899 - binary_classification_loss: 44.3572 - treatment_accuracy: 0.5061 - track_epsilon: 1.2632e-04 - val_loss: 76.3328 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3509 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.6460e-04 - lr: 3.9062e-08\n",
      "Epoch 87/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3524 - regression_loss: 15.9899 - binary_classification_loss: 44.3573 - treatment_accuracy: 0.5061 - track_epsilon: 1.3358e-04 - val_loss: 76.3318 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3498 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.9856e-04 - lr: 3.9062e-08\n",
      "Epoch 88/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3524 - regression_loss: 15.9899 - binary_classification_loss: 44.3574 - treatment_accuracy: 0.5061 - track_epsilon: 1.4130e-04 - val_loss: 76.3324 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3503 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.0713e-04 - lr: 3.9062e-08\n",
      "Epoch 89/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3523 - regression_loss: 15.9899 - binary_classification_loss: 44.3569 - treatment_accuracy: 0.5061 - track_epsilon: 1.3672e-04 - val_loss: 76.3322 - val_regression_loss: 15.9822 - val_binary_classification_loss: 44.3514 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.2622e-04 - lr: 3.9062e-08\n",
      "Epoch 90/100\n",
      "14969/14979 [============================>.] - ETA: 0s - loss: 76.3527 - regression_loss: 15.9900 - binary_classification_loss: 44.3575 - treatment_accuracy: 0.5061 - track_epsilon: 1.6452e-04\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-08.\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3525 - regression_loss: 15.9900 - binary_classification_loss: 44.3574 - treatment_accuracy: 0.5061 - track_epsilon: 1.6454e-04 - val_loss: 76.3328 - val_regression_loss: 15.9827 - val_binary_classification_loss: 44.3500 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.8992e-04 - lr: 3.9062e-08\n",
      "Epoch 91/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3522 - regression_loss: 15.9899 - binary_classification_loss: 44.3565 - treatment_accuracy: 0.5061 - track_epsilon: 1.3676e-04 - val_loss: 76.3309 - val_regression_loss: 15.9826 - val_binary_classification_loss: 44.3490 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.3797e-05 - lr: 1.9531e-08\n",
      "Epoch 92/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3519 - regression_loss: 15.9899 - binary_classification_loss: 44.3567 - treatment_accuracy: 0.5061 - track_epsilon: 9.4796e-05 - val_loss: 76.3334 - val_regression_loss: 15.9826 - val_binary_classification_loss: 44.3510 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 7.3274e-05 - lr: 1.9531e-08\n",
      "Epoch 93/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3522 - regression_loss: 15.9899 - binary_classification_loss: 44.3566 - treatment_accuracy: 0.5061 - track_epsilon: 1.1204e-04 - val_loss: 76.3323 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3508 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 2.2489e-04 - lr: 1.9531e-08\n",
      "Epoch 94/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 76.3519 - regression_loss: 15.9899 - binary_classification_loss: 44.3568 - treatment_accuracy: 0.5061 - track_epsilon: 1.0314e-04 - val_loss: 76.3319 - val_regression_loss: 15.9826 - val_binary_classification_loss: 44.3496 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 4.4079e-05 - lr: 1.9531e-08\n",
      "Epoch 95/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3522 - regression_loss: 15.9899 - binary_classification_loss: 44.3578 - treatment_accuracy: 0.5061 - track_epsilon: 1.1543e-04 - val_loss: 76.3313 - val_regression_loss: 15.9824 - val_binary_classification_loss: 44.3498 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.8734e-04 - lr: 1.9531e-08\n",
      "Epoch 96/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3519 - regression_loss: 15.9899 - binary_classification_loss: 44.3571 - treatment_accuracy: 0.5061 - track_epsilon: 7.4287e-05 - val_loss: 76.3309 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3492 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 3.7471e-05 - lr: 1.9531e-08\n",
      "Epoch 97/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 76.3518 - regression_loss: 15.9899 - binary_classification_loss: 44.3569 - treatment_accuracy: 0.5061 - track_epsilon: 5.7258e-05 - val_loss: 76.3312 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3493 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.0470e-04 - lr: 1.9531e-08\n",
      "Epoch 98/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3521 - regression_loss: 15.9899 - binary_classification_loss: 44.3564 - treatment_accuracy: 0.5061 - track_epsilon: 1.2640e-04 - val_loss: 76.3308 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3495 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 8.6308e-05 - lr: 1.9531e-08\n",
      "Epoch 99/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3523 - regression_loss: 15.9899 - binary_classification_loss: 44.3571 - treatment_accuracy: 0.5061 - track_epsilon: 8.2861e-05 - val_loss: 76.3329 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3513 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 6.3538e-05 - lr: 1.9531e-08\n",
      "Epoch 100/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 76.3516 - regression_loss: 15.9899 - binary_classification_loss: 44.3573 - treatment_accuracy: 0.5061 - track_epsilon: 9.6091e-05 - val_loss: 76.3322 - val_regression_loss: 15.9825 - val_binary_classification_loss: 44.3504 - val_treatment_accuracy: 0.5087 - val_track_epsilon: 1.2141e-04 - lr: 1.9531e-08\n",
      "37448/37448 [==============================] - 30s 799us/step\n",
      "0.0038567781\n",
      "Average ATE: \n",
      "-0.000746309757232666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Define folder path\n",
    "folder_path = \"./evaluationDatasets/Placebo/\"\n",
    "\n",
    "# List to store treatment effects\n",
    "ate_values = []\n",
    "\n",
    "feature_names = ['NumberOfOffers', 'Action', 'org:resource',\n",
    "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
    "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
    "       'CreditScore', 'OfferedAmount', 'offerNumber','timeApplication', 'weekdayApplication']\n",
    "\n",
    "columns_to_drop = ['offerSuccess', 'treatmentOffer']\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    print(file_name)\n",
    "    # Read CSV file\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    refutation = pd.read_csv(file_path)\n",
    "    \n",
    "    y_outcome = refutation['offerSuccess'].values\n",
    "    x_feature = refutation[feature_names].values\n",
    "    #x_feature = refutation.drop(columns=columns_to_drop).values\n",
    "    t_treatment = np.array([np.array([value]) for value in refutation['treatmentOffer']])\n",
    "    \n",
    "    dragon = DragonNet(neurons_per_layer=200, targeted_reg=True)\n",
    "    dragon_ite = dragon.fit_predict(x_feature, t_treatment, y_outcome, return_components=False)\n",
    "    dragon_ate = dragon_ite.mean()\n",
    "    print(dragon_ate)\n",
    "    ate_values.append(dragon_ate)\n",
    "\n",
    "print(\"Average ATE: \")\n",
    "print(sum(ate_values) / len(ate_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85614adf-d5e0-43a1-a198-eb5ae4f7a05f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomReplacedDataset2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Training with 959 minibatches per epoch\n",
      "DEBUG \t step     0 loss = 37.0999\n",
      "DEBUG \t step   100 loss = 17.5016\n",
      "DEBUG \t step   200 loss = 15.2348\n",
      "DEBUG \t step   300 loss = 14.2043\n",
      "DEBUG \t step   400 loss = 12.7082\n",
      "DEBUG \t step   500 loss = 11.5089\n",
      "DEBUG \t step   600 loss = 10.0316\n",
      "DEBUG \t step   700 loss = 9.32373\n",
      "DEBUG \t step   800 loss = 8.90895\n",
      "DEBUG \t step   900 loss = 8.07123\n",
      "DEBUG \t step  1000 loss = 7.07197\n",
      "DEBUG \t step  1100 loss = 6.50148\n",
      "DEBUG \t step  1200 loss = 6.31736\n",
      "DEBUG \t step  1300 loss = 5.42301\n",
      "DEBUG \t step  1400 loss = 4.61341\n",
      "DEBUG \t step  1500 loss = 5.6245\n",
      "DEBUG \t step  1600 loss = 5.0767\n",
      "DEBUG \t step  1700 loss = 4.01805\n",
      "DEBUG \t step  1800 loss = 4.58147\n",
      "DEBUG \t step  1900 loss = 3.4589\n",
      "DEBUG \t step  2000 loss = 3.49854\n",
      "DEBUG \t step  2100 loss = 3.1304\n",
      "DEBUG \t step  2200 loss = 2.93198\n",
      "DEBUG \t step  2300 loss = 2.48879\n",
      "DEBUG \t step  2400 loss = 2.56436\n",
      "DEBUG \t step  2500 loss = 2.60999\n",
      "DEBUG \t step  2600 loss = 2.55109\n",
      "DEBUG \t step  2700 loss = 2.06989\n",
      "DEBUG \t step  2800 loss = 2.61943\n",
      "DEBUG \t step  2900 loss = 2.20944\n",
      "DEBUG \t step  3000 loss = 1.30472\n",
      "DEBUG \t step  3100 loss = 1.50126\n",
      "DEBUG \t step  3200 loss = 2.32273\n",
      "DEBUG \t step  3300 loss = 1.3059\n",
      "DEBUG \t step  3400 loss = 1.36315\n",
      "DEBUG \t step  3500 loss = 1.39877\n",
      "DEBUG \t step  3600 loss = 0.724392\n",
      "DEBUG \t step  3700 loss = 1.79418\n",
      "DEBUG \t step  3800 loss = 0.755038\n",
      "DEBUG \t step  3900 loss = 1.5017\n",
      "DEBUG \t step  4000 loss = 2.01078\n",
      "DEBUG \t step  4100 loss = 0.972347\n",
      "DEBUG \t step  4200 loss = 2.11714\n",
      "DEBUG \t step  4300 loss = 0.954735\n",
      "DEBUG \t step  4400 loss = 0.81214\n",
      "DEBUG \t step  4500 loss = 1.18121\n",
      "DEBUG \t step  4600 loss = 0.228095\n",
      "DEBUG \t step  4700 loss = 1.08674\n",
      "INFO \t Evaluating 240 minibatches\n",
      "DEBUG \t batch ate = -0.17054\n",
      "DEBUG \t batch ate = -0.131891\n",
      "DEBUG \t batch ate = -0.0706633\n",
      "DEBUG \t batch ate = -0.155965\n",
      "DEBUG \t batch ate = -0.158724\n",
      "DEBUG \t batch ate = -0.0796632\n",
      "DEBUG \t batch ate = -0.17363\n",
      "DEBUG \t batch ate = -0.19356\n",
      "DEBUG \t batch ate = -0.156931\n",
      "DEBUG \t batch ate = -0.152872\n",
      "DEBUG \t batch ate = -0.140364\n",
      "DEBUG \t batch ate = -0.14729\n",
      "DEBUG \t batch ate = -0.212062\n",
      "DEBUG \t batch ate = -0.154875\n",
      "DEBUG \t batch ate = -0.119151\n",
      "DEBUG \t batch ate = -0.177642\n",
      "DEBUG \t batch ate = -0.132323\n",
      "DEBUG \t batch ate = -0.147297\n",
      "DEBUG \t batch ate = -0.16173\n",
      "DEBUG \t batch ate = -0.118001\n",
      "DEBUG \t batch ate = -0.147951\n",
      "DEBUG \t batch ate = -0.11807\n",
      "DEBUG \t batch ate = -0.182023\n",
      "DEBUG \t batch ate = -0.209507\n",
      "DEBUG \t batch ate = -0.115556\n",
      "DEBUG \t batch ate = -0.16462\n",
      "DEBUG \t batch ate = -0.139127\n",
      "DEBUG \t batch ate = -0.152525\n",
      "DEBUG \t batch ate = -0.143988\n",
      "DEBUG \t batch ate = -0.127197\n",
      "DEBUG \t batch ate = -0.16561\n",
      "DEBUG \t batch ate = -0.156795\n",
      "DEBUG \t batch ate = -0.186879\n",
      "DEBUG \t batch ate = -0.13234\n",
      "DEBUG \t batch ate = -0.174488\n",
      "DEBUG \t batch ate = -0.171589\n",
      "DEBUG \t batch ate = -0.154723\n",
      "DEBUG \t batch ate = -0.128208\n",
      "DEBUG \t batch ate = -0.158452\n",
      "DEBUG \t batch ate = -0.157145\n",
      "DEBUG \t batch ate = -0.187042\n",
      "DEBUG \t batch ate = -0.136617\n",
      "DEBUG \t batch ate = -0.156047\n",
      "DEBUG \t batch ate = -0.152076\n",
      "DEBUG \t batch ate = -0.14206\n",
      "DEBUG \t batch ate = -0.144633\n",
      "DEBUG \t batch ate = -0.171262\n",
      "DEBUG \t batch ate = -0.123978\n",
      "DEBUG \t batch ate = -0.177516\n",
      "DEBUG \t batch ate = -0.150917\n",
      "DEBUG \t batch ate = -0.16534\n",
      "DEBUG \t batch ate = -0.133451\n",
      "DEBUG \t batch ate = -0.150944\n",
      "DEBUG \t batch ate = -0.159496\n",
      "DEBUG \t batch ate = -0.136218\n",
      "DEBUG \t batch ate = -0.135391\n",
      "DEBUG \t batch ate = -0.108543\n",
      "DEBUG \t batch ate = -0.117792\n",
      "DEBUG \t batch ate = -0.14401\n",
      "DEBUG \t batch ate = -0.134118\n",
      "DEBUG \t batch ate = -0.15183\n",
      "DEBUG \t batch ate = -0.113046\n",
      "DEBUG \t batch ate = -0.120561\n",
      "DEBUG \t batch ate = -0.136381\n",
      "DEBUG \t batch ate = -0.158245\n",
      "DEBUG \t batch ate = -0.161153\n",
      "DEBUG \t batch ate = -0.199417\n",
      "DEBUG \t batch ate = -0.165222\n",
      "DEBUG \t batch ate = -0.171073\n",
      "DEBUG \t batch ate = -0.11945\n",
      "DEBUG \t batch ate = -0.198095\n",
      "DEBUG \t batch ate = -0.156256\n",
      "DEBUG \t batch ate = -0.197912\n",
      "DEBUG \t batch ate = -0.146375\n",
      "DEBUG \t batch ate = -0.142714\n",
      "DEBUG \t batch ate = -0.171941\n",
      "DEBUG \t batch ate = -0.13682\n",
      "DEBUG \t batch ate = -0.196454\n",
      "DEBUG \t batch ate = -0.153551\n",
      "DEBUG \t batch ate = -0.176061\n",
      "DEBUG \t batch ate = -0.115826\n",
      "DEBUG \t batch ate = -0.184159\n",
      "DEBUG \t batch ate = -0.181655\n",
      "DEBUG \t batch ate = -0.172735\n",
      "DEBUG \t batch ate = -0.147109\n",
      "DEBUG \t batch ate = -0.0985262\n",
      "DEBUG \t batch ate = -0.126061\n",
      "DEBUG \t batch ate = -0.177363\n",
      "DEBUG \t batch ate = -0.149274\n",
      "DEBUG \t batch ate = -0.179502\n",
      "DEBUG \t batch ate = -0.203575\n",
      "DEBUG \t batch ate = -0.152126\n",
      "DEBUG \t batch ate = -0.163615\n",
      "DEBUG \t batch ate = -0.126541\n",
      "DEBUG \t batch ate = -0.176854\n",
      "DEBUG \t batch ate = -0.138229\n",
      "DEBUG \t batch ate = -0.186338\n",
      "DEBUG \t batch ate = -0.18935\n",
      "DEBUG \t batch ate = -0.105879\n",
      "DEBUG \t batch ate = -0.167869\n",
      "DEBUG \t batch ate = -0.175312\n",
      "DEBUG \t batch ate = -0.144067\n",
      "DEBUG \t batch ate = -0.176346\n",
      "DEBUG \t batch ate = -0.159469\n",
      "DEBUG \t batch ate = -0.149667\n",
      "DEBUG \t batch ate = -0.101196\n",
      "DEBUG \t batch ate = -0.182606\n",
      "DEBUG \t batch ate = -0.156468\n",
      "DEBUG \t batch ate = -0.143266\n",
      "DEBUG \t batch ate = -0.151319\n",
      "DEBUG \t batch ate = -0.155966\n",
      "DEBUG \t batch ate = -0.147565\n",
      "DEBUG \t batch ate = -0.114008\n",
      "DEBUG \t batch ate = -0.178785\n",
      "DEBUG \t batch ate = -0.0898998\n",
      "DEBUG \t batch ate = -0.147505\n",
      "DEBUG \t batch ate = -0.161205\n",
      "DEBUG \t batch ate = -0.148435\n",
      "DEBUG \t batch ate = -0.152033\n",
      "DEBUG \t batch ate = -0.172467\n",
      "DEBUG \t batch ate = -0.173134\n",
      "DEBUG \t batch ate = -0.165934\n",
      "DEBUG \t batch ate = -0.141772\n",
      "DEBUG \t batch ate = -0.162533\n",
      "DEBUG \t batch ate = -0.142785\n",
      "DEBUG \t batch ate = -0.14454\n",
      "DEBUG \t batch ate = -0.136572\n",
      "DEBUG \t batch ate = -0.160769\n",
      "DEBUG \t batch ate = -0.183352\n",
      "DEBUG \t batch ate = -0.122\n",
      "DEBUG \t batch ate = -0.123267\n",
      "DEBUG \t batch ate = -0.137058\n",
      "DEBUG \t batch ate = -0.191199\n",
      "DEBUG \t batch ate = -0.16241\n",
      "DEBUG \t batch ate = -0.177438\n",
      "DEBUG \t batch ate = -0.18892\n",
      "DEBUG \t batch ate = -0.141704\n",
      "DEBUG \t batch ate = -0.18987\n",
      "DEBUG \t batch ate = -0.14202\n",
      "DEBUG \t batch ate = -0.173327\n",
      "DEBUG \t batch ate = -0.150532\n",
      "DEBUG \t batch ate = -0.164006\n",
      "DEBUG \t batch ate = -0.143433\n",
      "DEBUG \t batch ate = -0.181068\n",
      "DEBUG \t batch ate = -0.189983\n",
      "DEBUG \t batch ate = -0.16293\n",
      "DEBUG \t batch ate = -0.141049\n",
      "DEBUG \t batch ate = -0.14155\n",
      "DEBUG \t batch ate = -0.169542\n",
      "DEBUG \t batch ate = -0.199574\n",
      "DEBUG \t batch ate = -0.111044\n",
      "DEBUG \t batch ate = -0.153743\n",
      "DEBUG \t batch ate = -0.134649\n",
      "DEBUG \t batch ate = -0.14884\n",
      "DEBUG \t batch ate = -0.122977\n",
      "DEBUG \t batch ate = -0.183988\n",
      "DEBUG \t batch ate = -0.16101\n",
      "DEBUG \t batch ate = -0.194405\n",
      "DEBUG \t batch ate = -0.173726\n",
      "DEBUG \t batch ate = -0.138923\n",
      "DEBUG \t batch ate = -0.142391\n",
      "DEBUG \t batch ate = -0.177974\n",
      "DEBUG \t batch ate = -0.152813\n",
      "DEBUG \t batch ate = -0.147339\n",
      "DEBUG \t batch ate = -0.179024\n",
      "DEBUG \t batch ate = -0.148796\n",
      "DEBUG \t batch ate = -0.157028\n",
      "DEBUG \t batch ate = -0.141943\n",
      "DEBUG \t batch ate = -0.185408\n",
      "DEBUG \t batch ate = -0.158596\n",
      "DEBUG \t batch ate = -0.197001\n",
      "DEBUG \t batch ate = -0.13545\n",
      "DEBUG \t batch ate = -0.158552\n",
      "DEBUG \t batch ate = -0.149893\n",
      "DEBUG \t batch ate = -0.136348\n",
      "DEBUG \t batch ate = -0.187614\n",
      "DEBUG \t batch ate = -0.13633\n",
      "DEBUG \t batch ate = -0.125641\n",
      "DEBUG \t batch ate = -0.153908\n",
      "DEBUG \t batch ate = -0.157453\n",
      "DEBUG \t batch ate = -0.111959\n",
      "DEBUG \t batch ate = -0.160006\n",
      "DEBUG \t batch ate = -0.176628\n",
      "DEBUG \t batch ate = -0.125711\n",
      "DEBUG \t batch ate = -0.185864\n",
      "DEBUG \t batch ate = -0.191529\n",
      "DEBUG \t batch ate = -0.160241\n",
      "DEBUG \t batch ate = -0.156903\n",
      "DEBUG \t batch ate = -0.189981\n",
      "DEBUG \t batch ate = -0.136499\n",
      "DEBUG \t batch ate = -0.165665\n",
      "DEBUG \t batch ate = -0.135452\n",
      "DEBUG \t batch ate = -0.140421\n",
      "DEBUG \t batch ate = -0.139735\n",
      "DEBUG \t batch ate = -0.173237\n",
      "DEBUG \t batch ate = -0.119009\n",
      "DEBUG \t batch ate = -0.154878\n",
      "DEBUG \t batch ate = -0.17138\n",
      "DEBUG \t batch ate = -0.148552\n",
      "DEBUG \t batch ate = -0.148448\n",
      "DEBUG \t batch ate = -0.139236\n",
      "DEBUG \t batch ate = -0.153673\n",
      "DEBUG \t batch ate = -0.155929\n",
      "DEBUG \t batch ate = -0.15052\n",
      "DEBUG \t batch ate = -0.132509\n",
      "DEBUG \t batch ate = -0.148795\n",
      "DEBUG \t batch ate = -0.172175\n",
      "DEBUG \t batch ate = -0.165141\n",
      "DEBUG \t batch ate = -0.157282\n",
      "DEBUG \t batch ate = -0.154019\n",
      "DEBUG \t batch ate = -0.152645\n",
      "DEBUG \t batch ate = -0.120775\n",
      "DEBUG \t batch ate = -0.156931\n",
      "DEBUG \t batch ate = -0.110628\n",
      "DEBUG \t batch ate = -0.184028\n",
      "DEBUG \t batch ate = -0.170505\n",
      "DEBUG \t batch ate = -0.107044\n",
      "DEBUG \t batch ate = -0.134221\n",
      "DEBUG \t batch ate = -0.172891\n",
      "DEBUG \t batch ate = -0.145005\n",
      "DEBUG \t batch ate = -0.135366\n",
      "DEBUG \t batch ate = -0.139171\n",
      "DEBUG \t batch ate = -0.113359\n",
      "DEBUG \t batch ate = -0.16907\n",
      "DEBUG \t batch ate = -0.167327\n",
      "DEBUG \t batch ate = -0.116854\n",
      "DEBUG \t batch ate = -0.13877\n",
      "DEBUG \t batch ate = -0.172445\n",
      "DEBUG \t batch ate = -0.116374\n",
      "DEBUG \t batch ate = -0.193007\n",
      "DEBUG \t batch ate = -0.179433\n",
      "DEBUG \t batch ate = -0.190267\n",
      "DEBUG \t batch ate = -0.140539\n",
      "DEBUG \t batch ate = -0.173962\n",
      "DEBUG \t batch ate = -0.158977\n",
      "DEBUG \t batch ate = -0.168515\n",
      "DEBUG \t batch ate = -0.176492\n",
      "DEBUG \t batch ate = -0.201335\n",
      "DEBUG \t batch ate = -0.189282\n",
      "DEBUG \t batch ate = -0.193116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.15435798\n",
      "randomReplacedDataset3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Training with 959 minibatches per epoch\n",
      "DEBUG \t step     0 loss = 36.2184\n",
      "DEBUG \t step   100 loss = 36.3472\n",
      "DEBUG \t step   200 loss = 36.7701\n",
      "DEBUG \t step   300 loss = 36.2554\n",
      "DEBUG \t step   400 loss = 37.8101\n",
      "DEBUG \t step   500 loss = 36.4088\n",
      "DEBUG \t step   600 loss = 36.1793\n",
      "DEBUG \t step   700 loss = 36.4583\n",
      "DEBUG \t step   800 loss = 36.369\n",
      "DEBUG \t step   900 loss = 38.4637\n",
      "DEBUG \t step  1000 loss = 37.1505\n",
      "DEBUG \t step  1100 loss = 36.6821\n",
      "DEBUG \t step  1200 loss = 36.9666\n",
      "DEBUG \t step  1300 loss = 36.7821\n",
      "DEBUG \t step  1400 loss = 37.3813\n",
      "DEBUG \t step  1500 loss = 36.5468\n",
      "DEBUG \t step  1600 loss = 37.4426\n",
      "DEBUG \t step  1700 loss = 37.0136\n",
      "DEBUG \t step  1800 loss = 36.7318\n",
      "DEBUG \t step  1900 loss = 36.6915\n",
      "DEBUG \t step  2000 loss = 36.3755\n",
      "DEBUG \t step  2100 loss = 36.8656\n",
      "DEBUG \t step  2200 loss = 36.4642\n",
      "DEBUG \t step  2300 loss = 36.8496\n",
      "DEBUG \t step  2400 loss = 37.5386\n",
      "DEBUG \t step  2500 loss = 36.354\n",
      "DEBUG \t step  2600 loss = 36.5195\n",
      "DEBUG \t step  2700 loss = 37.0572\n",
      "DEBUG \t step  2800 loss = 37.1729\n",
      "DEBUG \t step  2900 loss = 37.9118\n",
      "DEBUG \t step  3000 loss = 37.3606\n",
      "DEBUG \t step  3100 loss = 37.0626\n",
      "DEBUG \t step  3200 loss = 37.0244\n",
      "DEBUG \t step  3300 loss = 36.1761\n",
      "DEBUG \t step  3400 loss = 36.8433\n",
      "DEBUG \t step  3500 loss = 36.9983\n",
      "DEBUG \t step  3600 loss = 37.3778\n",
      "DEBUG \t step  3700 loss = 36.5161\n",
      "DEBUG \t step  3800 loss = 36.0568\n",
      "DEBUG \t step  3900 loss = 37.558\n",
      "DEBUG \t step  4000 loss = 36.682\n",
      "DEBUG \t step  4100 loss = 37.5376\n",
      "DEBUG \t step  4200 loss = 36.4675\n",
      "DEBUG \t step  4300 loss = 37.8298\n",
      "DEBUG \t step  4400 loss = 37.2531\n",
      "DEBUG \t step  4500 loss = 36.1585\n",
      "DEBUG \t step  4600 loss = 37.0934\n",
      "DEBUG \t step  4700 loss = 37.347\n",
      "INFO \t Evaluating 240 minibatches\n",
      "DEBUG \t batch ate = 0.0483476\n",
      "DEBUG \t batch ate = 0.0465612\n",
      "DEBUG \t batch ate = 0.0498471\n",
      "DEBUG \t batch ate = 0.0490094\n",
      "DEBUG \t batch ate = 0.0483936\n",
      "DEBUG \t batch ate = 0.0463873\n",
      "DEBUG \t batch ate = 0.0496989\n",
      "DEBUG \t batch ate = 0.0496822\n",
      "DEBUG \t batch ate = 0.0468155\n",
      "DEBUG \t batch ate = 0.0483949\n",
      "DEBUG \t batch ate = 0.0498628\n",
      "DEBUG \t batch ate = 0.0501915\n",
      "DEBUG \t batch ate = 0.0480775\n",
      "DEBUG \t batch ate = 0.0483158\n",
      "DEBUG \t batch ate = 0.0488316\n",
      "DEBUG \t batch ate = 0.0479457\n",
      "DEBUG \t batch ate = 0.0480416\n",
      "DEBUG \t batch ate = 0.0476432\n",
      "DEBUG \t batch ate = 0.0505402\n",
      "DEBUG \t batch ate = 0.0493272\n",
      "DEBUG \t batch ate = 0.0508913\n",
      "DEBUG \t batch ate = 0.050189\n",
      "DEBUG \t batch ate = 0.0480352\n",
      "DEBUG \t batch ate = 0.0497102\n",
      "DEBUG \t batch ate = 0.0500799\n",
      "DEBUG \t batch ate = 0.0504291\n",
      "DEBUG \t batch ate = 0.0496826\n",
      "DEBUG \t batch ate = 0.0484229\n",
      "DEBUG \t batch ate = 0.0501945\n",
      "DEBUG \t batch ate = 0.0512003\n",
      "DEBUG \t batch ate = 0.0487406\n",
      "DEBUG \t batch ate = 0.0499481\n",
      "DEBUG \t batch ate = 0.0499321\n",
      "DEBUG \t batch ate = 0.0516385\n",
      "DEBUG \t batch ate = 0.0499516\n",
      "DEBUG \t batch ate = 0.0500448\n",
      "DEBUG \t batch ate = 0.0477184\n",
      "DEBUG \t batch ate = 0.0503779\n",
      "DEBUG \t batch ate = 0.0509954\n",
      "DEBUG \t batch ate = 0.0530566\n",
      "DEBUG \t batch ate = 0.0463402\n",
      "DEBUG \t batch ate = 0.0503995\n",
      "DEBUG \t batch ate = 0.051487\n",
      "DEBUG \t batch ate = 0.0500252\n",
      "DEBUG \t batch ate = 0.0495834\n",
      "DEBUG \t batch ate = 0.0485211\n",
      "DEBUG \t batch ate = 0.0484281\n",
      "DEBUG \t batch ate = 0.0470741\n",
      "DEBUG \t batch ate = 0.0488775\n",
      "DEBUG \t batch ate = 0.0524349\n",
      "DEBUG \t batch ate = 0.0478885\n",
      "DEBUG \t batch ate = 0.0481429\n",
      "DEBUG \t batch ate = 0.0490103\n",
      "DEBUG \t batch ate = 0.0488011\n",
      "DEBUG \t batch ate = 0.0495013\n",
      "DEBUG \t batch ate = 0.0504789\n",
      "DEBUG \t batch ate = 0.0504394\n",
      "DEBUG \t batch ate = 0.0493379\n",
      "DEBUG \t batch ate = 0.0503173\n",
      "DEBUG \t batch ate = 0.0496958\n",
      "DEBUG \t batch ate = 0.050152\n",
      "DEBUG \t batch ate = 0.0505217\n",
      "DEBUG \t batch ate = 0.0537532\n",
      "DEBUG \t batch ate = 0.0496683\n",
      "DEBUG \t batch ate = 0.0487323\n",
      "DEBUG \t batch ate = 0.0467741\n",
      "DEBUG \t batch ate = 0.0500917\n",
      "DEBUG \t batch ate = 0.047973\n",
      "DEBUG \t batch ate = 0.0510306\n",
      "DEBUG \t batch ate = 0.0480429\n",
      "DEBUG \t batch ate = 0.0535657\n",
      "DEBUG \t batch ate = 0.0530677\n",
      "DEBUG \t batch ate = 0.0485779\n",
      "DEBUG \t batch ate = 0.0505223\n",
      "DEBUG \t batch ate = 0.0501716\n",
      "DEBUG \t batch ate = 0.0506096\n",
      "DEBUG \t batch ate = 0.0448795\n",
      "DEBUG \t batch ate = 0.0469706\n",
      "DEBUG \t batch ate = 0.0509889\n",
      "DEBUG \t batch ate = 0.0520586\n",
      "DEBUG \t batch ate = 0.0465324\n",
      "DEBUG \t batch ate = 0.0480232\n",
      "DEBUG \t batch ate = 0.0481257\n",
      "DEBUG \t batch ate = 0.0478469\n",
      "DEBUG \t batch ate = 0.0504458\n",
      "DEBUG \t batch ate = 0.0473832\n",
      "DEBUG \t batch ate = 0.0511971\n",
      "DEBUG \t batch ate = 0.0510502\n",
      "DEBUG \t batch ate = 0.0466749\n",
      "DEBUG \t batch ate = 0.0475593\n",
      "DEBUG \t batch ate = 0.0478627\n",
      "DEBUG \t batch ate = 0.0504978\n",
      "DEBUG \t batch ate = 0.0511736\n",
      "DEBUG \t batch ate = 0.0513735\n",
      "DEBUG \t batch ate = 0.049018\n",
      "DEBUG \t batch ate = 0.0451067\n",
      "DEBUG \t batch ate = 0.0493782\n",
      "DEBUG \t batch ate = 0.0464255\n",
      "DEBUG \t batch ate = 0.0496386\n",
      "DEBUG \t batch ate = 0.0472898\n",
      "DEBUG \t batch ate = 0.0506611\n",
      "DEBUG \t batch ate = 0.0493335\n",
      "DEBUG \t batch ate = 0.0499228\n",
      "DEBUG \t batch ate = 0.0484716\n",
      "DEBUG \t batch ate = 0.0495949\n",
      "DEBUG \t batch ate = 0.0451614\n",
      "DEBUG \t batch ate = 0.0484545\n",
      "DEBUG \t batch ate = 0.049036\n",
      "DEBUG \t batch ate = 0.0492306\n",
      "DEBUG \t batch ate = 0.0486679\n",
      "DEBUG \t batch ate = 0.0488603\n",
      "DEBUG \t batch ate = 0.0499692\n",
      "DEBUG \t batch ate = 0.0481368\n",
      "DEBUG \t batch ate = 0.0495954\n",
      "DEBUG \t batch ate = 0.0499353\n",
      "DEBUG \t batch ate = 0.0538117\n",
      "DEBUG \t batch ate = 0.0500189\n",
      "DEBUG \t batch ate = 0.0458818\n",
      "DEBUG \t batch ate = 0.0486919\n",
      "DEBUG \t batch ate = 0.0513095\n",
      "DEBUG \t batch ate = 0.0502357\n",
      "DEBUG \t batch ate = 0.0486231\n",
      "DEBUG \t batch ate = 0.04982\n",
      "DEBUG \t batch ate = 0.0477646\n",
      "DEBUG \t batch ate = 0.0511458\n",
      "DEBUG \t batch ate = 0.0485933\n",
      "DEBUG \t batch ate = 0.0507941\n",
      "DEBUG \t batch ate = 0.0455329\n",
      "DEBUG \t batch ate = 0.0473704\n",
      "DEBUG \t batch ate = 0.0526758\n",
      "DEBUG \t batch ate = 0.0512781\n",
      "DEBUG \t batch ate = 0.0473089\n",
      "DEBUG \t batch ate = 0.0464885\n",
      "DEBUG \t batch ate = 0.0487546\n",
      "DEBUG \t batch ate = 0.0471251\n",
      "DEBUG \t batch ate = 0.0474006\n",
      "DEBUG \t batch ate = 0.0511302\n",
      "DEBUG \t batch ate = 0.0478558\n",
      "DEBUG \t batch ate = 0.047028\n",
      "DEBUG \t batch ate = 0.0484217\n",
      "DEBUG \t batch ate = 0.050944\n",
      "DEBUG \t batch ate = 0.0485393\n",
      "DEBUG \t batch ate = 0.0523635\n",
      "DEBUG \t batch ate = 0.0494062\n",
      "DEBUG \t batch ate = 0.0471594\n",
      "DEBUG \t batch ate = 0.051339\n",
      "DEBUG \t batch ate = 0.0493718\n",
      "DEBUG \t batch ate = 0.0522022\n",
      "DEBUG \t batch ate = 0.0488718\n",
      "DEBUG \t batch ate = 0.0506332\n",
      "DEBUG \t batch ate = 0.0480733\n",
      "DEBUG \t batch ate = 0.0495368\n",
      "DEBUG \t batch ate = 0.0500557\n",
      "DEBUG \t batch ate = 0.0495367\n",
      "DEBUG \t batch ate = 0.0468949\n",
      "DEBUG \t batch ate = 0.0521939\n",
      "DEBUG \t batch ate = 0.0483154\n",
      "DEBUG \t batch ate = 0.0511148\n",
      "DEBUG \t batch ate = 0.0484443\n",
      "DEBUG \t batch ate = 0.0511052\n",
      "DEBUG \t batch ate = 0.0467582\n",
      "DEBUG \t batch ate = 0.0466194\n",
      "DEBUG \t batch ate = 0.0472735\n",
      "DEBUG \t batch ate = 0.0508733\n",
      "DEBUG \t batch ate = 0.0474868\n",
      "DEBUG \t batch ate = 0.0520659\n",
      "DEBUG \t batch ate = 0.0495727\n",
      "DEBUG \t batch ate = 0.0510847\n",
      "DEBUG \t batch ate = 0.0545306\n",
      "DEBUG \t batch ate = 0.0502264\n",
      "DEBUG \t batch ate = 0.0542732\n",
      "DEBUG \t batch ate = 0.0467655\n",
      "DEBUG \t batch ate = 0.0487352\n",
      "DEBUG \t batch ate = 0.0496695\n",
      "DEBUG \t batch ate = 0.0479452\n",
      "DEBUG \t batch ate = 0.0533247\n",
      "DEBUG \t batch ate = 0.0511809\n",
      "DEBUG \t batch ate = 0.0491684\n",
      "DEBUG \t batch ate = 0.0505452\n",
      "DEBUG \t batch ate = 0.0477596\n",
      "DEBUG \t batch ate = 0.0485487\n",
      "DEBUG \t batch ate = 0.0493696\n",
      "DEBUG \t batch ate = 0.0511045\n",
      "DEBUG \t batch ate = 0.0490446\n",
      "DEBUG \t batch ate = 0.0488455\n",
      "DEBUG \t batch ate = 0.0492066\n",
      "DEBUG \t batch ate = 0.0510853\n",
      "DEBUG \t batch ate = 0.0483473\n",
      "DEBUG \t batch ate = 0.0486598\n",
      "DEBUG \t batch ate = 0.0510905\n",
      "DEBUG \t batch ate = 0.0499527\n",
      "DEBUG \t batch ate = 0.0493271\n",
      "DEBUG \t batch ate = 0.0503393\n",
      "DEBUG \t batch ate = 0.0532674\n",
      "DEBUG \t batch ate = 0.0517587\n",
      "DEBUG \t batch ate = 0.0497761\n",
      "DEBUG \t batch ate = 0.0518126\n",
      "DEBUG \t batch ate = 0.0510568\n",
      "DEBUG \t batch ate = 0.0486079\n",
      "DEBUG \t batch ate = 0.0475497\n",
      "DEBUG \t batch ate = 0.0512102\n",
      "DEBUG \t batch ate = 0.0499886\n",
      "DEBUG \t batch ate = 0.0492517\n",
      "DEBUG \t batch ate = 0.0506113\n",
      "DEBUG \t batch ate = 0.0504642\n",
      "DEBUG \t batch ate = 0.0491891\n",
      "DEBUG \t batch ate = 0.0509577\n",
      "DEBUG \t batch ate = 0.0507962\n",
      "DEBUG \t batch ate = 0.0488909\n",
      "DEBUG \t batch ate = 0.0498523\n",
      "DEBUG \t batch ate = 0.0515976\n",
      "DEBUG \t batch ate = 0.0477422\n",
      "DEBUG \t batch ate = 0.049753\n",
      "DEBUG \t batch ate = 0.0509991\n",
      "DEBUG \t batch ate = 0.0480033\n",
      "DEBUG \t batch ate = 0.0512755\n",
      "DEBUG \t batch ate = 0.0519749\n",
      "DEBUG \t batch ate = 0.0507471\n",
      "DEBUG \t batch ate = 0.046522\n",
      "DEBUG \t batch ate = 0.0495659\n",
      "DEBUG \t batch ate = 0.0497352\n",
      "DEBUG \t batch ate = 0.047444\n",
      "DEBUG \t batch ate = 0.0496135\n",
      "DEBUG \t batch ate = 0.0491579\n",
      "DEBUG \t batch ate = 0.0512575\n",
      "DEBUG \t batch ate = 0.048565\n",
      "DEBUG \t batch ate = 0.0524919\n",
      "DEBUG \t batch ate = 0.0482045\n",
      "DEBUG \t batch ate = 0.0512391\n",
      "DEBUG \t batch ate = 0.052458\n",
      "DEBUG \t batch ate = 0.0500944\n",
      "DEBUG \t batch ate = 0.0496208\n",
      "DEBUG \t batch ate = 0.0510034\n",
      "DEBUG \t batch ate = 0.046897\n",
      "DEBUG \t batch ate = 0.0467251\n",
      "DEBUG \t batch ate = 0.049811\n",
      "DEBUG \t batch ate = 0.0462386\n",
      "DEBUG \t batch ate = 0.0497309\n",
      "DEBUG \t batch ate = 0.0466566\n",
      "DEBUG \t batch ate = 0.0509694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.049451213\n",
      "randomReplacedDataset7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Training with 959 minibatches per epoch\n",
      "DEBUG \t step     0 loss = 38.3737\n",
      "DEBUG \t step   100 loss = 37.21\n",
      "DEBUG \t step   200 loss = 38.4292\n",
      "DEBUG \t step   300 loss = 36.9215\n",
      "DEBUG \t step   400 loss = 36.8218\n",
      "DEBUG \t step   500 loss = 36.6969\n",
      "DEBUG \t step   600 loss = 37.2269\n",
      "DEBUG \t step   700 loss = 36.599\n",
      "DEBUG \t step   800 loss = 38.0263\n",
      "DEBUG \t step   900 loss = 37.2955\n",
      "DEBUG \t step  1000 loss = 37.9216\n",
      "DEBUG \t step  1100 loss = 37.3359\n",
      "DEBUG \t step  1200 loss = 37.3713\n",
      "DEBUG \t step  1300 loss = 36.6709\n",
      "DEBUG \t step  1400 loss = 38.3146\n",
      "DEBUG \t step  1500 loss = 36.7897\n",
      "DEBUG \t step  1600 loss = 37.9787\n",
      "DEBUG \t step  1700 loss = 37.0694\n",
      "DEBUG \t step  1800 loss = 37.8516\n",
      "DEBUG \t step  1900 loss = 37.5252\n",
      "DEBUG \t step  2000 loss = 37.6231\n",
      "DEBUG \t step  2100 loss = 37.3801\n",
      "DEBUG \t step  2200 loss = 38.3831\n",
      "DEBUG \t step  2300 loss = 37.1824\n",
      "DEBUG \t step  2400 loss = 37.5972\n",
      "DEBUG \t step  2500 loss = 37.466\n",
      "DEBUG \t step  2600 loss = 37.8248\n",
      "DEBUG \t step  2700 loss = 36.4994\n",
      "DEBUG \t step  2800 loss = 37.2386\n",
      "DEBUG \t step  2900 loss = 37.5954\n",
      "DEBUG \t step  3000 loss = 36.867\n",
      "DEBUG \t step  3100 loss = 36.6477\n",
      "DEBUG \t step  3200 loss = 37.0951\n",
      "DEBUG \t step  3300 loss = 37.1585\n",
      "DEBUG \t step  3400 loss = 37.0798\n",
      "DEBUG \t step  3500 loss = 36.9801\n",
      "DEBUG \t step  3600 loss = 37.0109\n",
      "DEBUG \t step  3700 loss = 37.2153\n",
      "DEBUG \t step  3800 loss = 37.1813\n",
      "DEBUG \t step  3900 loss = 37.5348\n",
      "DEBUG \t step  4000 loss = 37.9985\n",
      "DEBUG \t step  4100 loss = 37.1822\n",
      "DEBUG \t step  4200 loss = 37.1592\n",
      "DEBUG \t step  4300 loss = 37.6636\n",
      "DEBUG \t step  4400 loss = 37.1613\n",
      "DEBUG \t step  4500 loss = 37.3339\n",
      "DEBUG \t step  4600 loss = 37.3917\n",
      "DEBUG \t step  4700 loss = 37.5079\n",
      "INFO \t Evaluating 240 minibatches\n",
      "DEBUG \t batch ate = -0.057798\n",
      "DEBUG \t batch ate = -0.0571402\n",
      "DEBUG \t batch ate = -0.058363\n",
      "DEBUG \t batch ate = -0.0565425\n",
      "DEBUG \t batch ate = -0.0602174\n",
      "DEBUG \t batch ate = -0.0592259\n",
      "DEBUG \t batch ate = -0.0600595\n",
      "DEBUG \t batch ate = -0.0582031\n",
      "DEBUG \t batch ate = -0.0582607\n",
      "DEBUG \t batch ate = -0.0577281\n",
      "DEBUG \t batch ate = -0.0543118\n",
      "DEBUG \t batch ate = -0.0554049\n",
      "DEBUG \t batch ate = -0.0597083\n",
      "DEBUG \t batch ate = -0.0545874\n",
      "DEBUG \t batch ate = -0.0568105\n",
      "DEBUG \t batch ate = -0.0580826\n",
      "DEBUG \t batch ate = -0.0601154\n",
      "DEBUG \t batch ate = -0.0552411\n",
      "DEBUG \t batch ate = -0.0562084\n",
      "DEBUG \t batch ate = -0.0578998\n",
      "DEBUG \t batch ate = -0.0561509\n",
      "DEBUG \t batch ate = -0.0558249\n",
      "DEBUG \t batch ate = -0.0559894\n",
      "DEBUG \t batch ate = -0.0596893\n",
      "DEBUG \t batch ate = -0.05688\n",
      "DEBUG \t batch ate = -0.0569752\n",
      "DEBUG \t batch ate = -0.0561774\n",
      "DEBUG \t batch ate = -0.0563104\n",
      "DEBUG \t batch ate = -0.0573446\n",
      "DEBUG \t batch ate = -0.0541252\n",
      "DEBUG \t batch ate = -0.0579574\n",
      "DEBUG \t batch ate = -0.056469\n",
      "DEBUG \t batch ate = -0.057675\n",
      "DEBUG \t batch ate = -0.0563481\n",
      "DEBUG \t batch ate = -0.0568539\n",
      "DEBUG \t batch ate = -0.0537577\n",
      "DEBUG \t batch ate = -0.0574641\n",
      "DEBUG \t batch ate = -0.0585956\n",
      "DEBUG \t batch ate = -0.0595226\n",
      "DEBUG \t batch ate = -0.0566465\n",
      "DEBUG \t batch ate = -0.057062\n",
      "DEBUG \t batch ate = -0.0564655\n",
      "DEBUG \t batch ate = -0.0567176\n",
      "DEBUG \t batch ate = -0.0570675\n",
      "DEBUG \t batch ate = -0.0560829\n",
      "DEBUG \t batch ate = -0.055669\n",
      "DEBUG \t batch ate = -0.0551195\n",
      "DEBUG \t batch ate = -0.0554512\n",
      "DEBUG \t batch ate = -0.0570328\n",
      "DEBUG \t batch ate = -0.0551658\n",
      "DEBUG \t batch ate = -0.0598843\n",
      "DEBUG \t batch ate = -0.0567127\n",
      "DEBUG \t batch ate = -0.0560985\n",
      "DEBUG \t batch ate = -0.0562037\n",
      "DEBUG \t batch ate = -0.0562045\n",
      "DEBUG \t batch ate = -0.0573206\n",
      "DEBUG \t batch ate = -0.0582164\n",
      "DEBUG \t batch ate = -0.0553568\n",
      "DEBUG \t batch ate = -0.055785\n",
      "DEBUG \t batch ate = -0.0563615\n",
      "DEBUG \t batch ate = -0.0581932\n",
      "DEBUG \t batch ate = -0.0570056\n",
      "DEBUG \t batch ate = -0.0587286\n",
      "DEBUG \t batch ate = -0.0575595\n",
      "DEBUG \t batch ate = -0.0573806\n",
      "DEBUG \t batch ate = -0.0567791\n",
      "DEBUG \t batch ate = -0.0571252\n",
      "DEBUG \t batch ate = -0.0567139\n",
      "DEBUG \t batch ate = -0.0535336\n",
      "DEBUG \t batch ate = -0.0553109\n",
      "DEBUG \t batch ate = -0.0568648\n",
      "DEBUG \t batch ate = -0.0558645\n",
      "DEBUG \t batch ate = -0.0578948\n",
      "DEBUG \t batch ate = -0.0544075\n",
      "DEBUG \t batch ate = -0.0592917\n",
      "DEBUG \t batch ate = -0.0562926\n",
      "DEBUG \t batch ate = -0.0566731\n",
      "DEBUG \t batch ate = -0.0572599\n",
      "DEBUG \t batch ate = -0.0569367\n",
      "DEBUG \t batch ate = -0.0586747\n",
      "DEBUG \t batch ate = -0.0556024\n",
      "DEBUG \t batch ate = -0.0582708\n",
      "DEBUG \t batch ate = -0.0567725\n",
      "DEBUG \t batch ate = -0.0562284\n",
      "DEBUG \t batch ate = -0.0567655\n",
      "DEBUG \t batch ate = -0.0570061\n",
      "DEBUG \t batch ate = -0.055874\n",
      "DEBUG \t batch ate = -0.055837\n",
      "DEBUG \t batch ate = -0.0532711\n",
      "DEBUG \t batch ate = -0.0561189\n",
      "DEBUG \t batch ate = -0.0567967\n",
      "DEBUG \t batch ate = -0.0597673\n",
      "DEBUG \t batch ate = -0.0554405\n",
      "DEBUG \t batch ate = -0.0537994\n",
      "DEBUG \t batch ate = -0.0575154\n",
      "DEBUG \t batch ate = -0.0561274\n",
      "DEBUG \t batch ate = -0.0559082\n",
      "DEBUG \t batch ate = -0.0589315\n",
      "DEBUG \t batch ate = -0.056527\n",
      "DEBUG \t batch ate = -0.0576872\n",
      "DEBUG \t batch ate = -0.0568053\n",
      "DEBUG \t batch ate = -0.0554342\n",
      "DEBUG \t batch ate = -0.0543911\n",
      "DEBUG \t batch ate = -0.0572194\n",
      "DEBUG \t batch ate = -0.0553212\n",
      "DEBUG \t batch ate = -0.0594651\n",
      "DEBUG \t batch ate = -0.0557149\n",
      "DEBUG \t batch ate = -0.0579751\n",
      "DEBUG \t batch ate = -0.0591846\n",
      "DEBUG \t batch ate = -0.054278\n",
      "DEBUG \t batch ate = -0.0571151\n",
      "DEBUG \t batch ate = -0.0578513\n",
      "DEBUG \t batch ate = -0.058644\n",
      "DEBUG \t batch ate = -0.0590867\n",
      "DEBUG \t batch ate = -0.0562489\n",
      "DEBUG \t batch ate = -0.0566857\n",
      "DEBUG \t batch ate = -0.0590701\n",
      "DEBUG \t batch ate = -0.0574329\n",
      "DEBUG \t batch ate = -0.0597657\n",
      "DEBUG \t batch ate = -0.0586855\n",
      "DEBUG \t batch ate = -0.0571817\n",
      "DEBUG \t batch ate = -0.0556099\n",
      "DEBUG \t batch ate = -0.0578036\n",
      "DEBUG \t batch ate = -0.058237\n",
      "DEBUG \t batch ate = -0.0552009\n",
      "DEBUG \t batch ate = -0.0577461\n",
      "DEBUG \t batch ate = -0.0548012\n",
      "DEBUG \t batch ate = -0.0582879\n",
      "DEBUG \t batch ate = -0.0587431\n",
      "DEBUG \t batch ate = -0.0566794\n",
      "DEBUG \t batch ate = -0.0579168\n",
      "DEBUG \t batch ate = -0.0592655\n",
      "DEBUG \t batch ate = -0.0579188\n",
      "DEBUG \t batch ate = -0.0561039\n",
      "DEBUG \t batch ate = -0.0573259\n",
      "DEBUG \t batch ate = -0.0560079\n",
      "DEBUG \t batch ate = -0.0577166\n",
      "DEBUG \t batch ate = -0.059691\n",
      "DEBUG \t batch ate = -0.0576185\n",
      "DEBUG \t batch ate = -0.0596359\n",
      "DEBUG \t batch ate = -0.0545019\n",
      "DEBUG \t batch ate = -0.0560527\n",
      "DEBUG \t batch ate = -0.0543248\n",
      "DEBUG \t batch ate = -0.0576069\n",
      "DEBUG \t batch ate = -0.0564915\n",
      "DEBUG \t batch ate = -0.0583957\n",
      "DEBUG \t batch ate = -0.0577641\n",
      "DEBUG \t batch ate = -0.055163\n",
      "DEBUG \t batch ate = -0.0578633\n",
      "DEBUG \t batch ate = -0.055724\n",
      "DEBUG \t batch ate = -0.0569325\n",
      "DEBUG \t batch ate = -0.0537424\n",
      "DEBUG \t batch ate = -0.0571221\n",
      "DEBUG \t batch ate = -0.0592003\n",
      "DEBUG \t batch ate = -0.057625\n",
      "DEBUG \t batch ate = -0.0535715\n",
      "DEBUG \t batch ate = -0.0560663\n",
      "DEBUG \t batch ate = -0.0587598\n",
      "DEBUG \t batch ate = -0.0574035\n",
      "DEBUG \t batch ate = -0.0591505\n",
      "DEBUG \t batch ate = -0.0581308\n",
      "DEBUG \t batch ate = -0.0595885\n",
      "DEBUG \t batch ate = -0.0571879\n",
      "DEBUG \t batch ate = -0.0568908\n",
      "DEBUG \t batch ate = -0.0575116\n",
      "DEBUG \t batch ate = -0.058546\n",
      "DEBUG \t batch ate = -0.055089\n",
      "DEBUG \t batch ate = -0.056051\n",
      "DEBUG \t batch ate = -0.0544328\n",
      "DEBUG \t batch ate = -0.0558918\n",
      "DEBUG \t batch ate = -0.0568965\n",
      "DEBUG \t batch ate = -0.0571942\n",
      "DEBUG \t batch ate = -0.0582462\n",
      "DEBUG \t batch ate = -0.0567523\n",
      "DEBUG \t batch ate = -0.056039\n",
      "DEBUG \t batch ate = -0.0565067\n",
      "DEBUG \t batch ate = -0.0565643\n",
      "DEBUG \t batch ate = -0.0576073\n",
      "DEBUG \t batch ate = -0.0601019\n",
      "DEBUG \t batch ate = -0.057541\n",
      "DEBUG \t batch ate = -0.0544619\n",
      "DEBUG \t batch ate = -0.0567601\n",
      "DEBUG \t batch ate = -0.054936\n",
      "DEBUG \t batch ate = -0.0571119\n",
      "DEBUG \t batch ate = -0.0574547\n",
      "DEBUG \t batch ate = -0.0572241\n",
      "DEBUG \t batch ate = -0.0574003\n",
      "DEBUG \t batch ate = -0.0585389\n",
      "DEBUG \t batch ate = -0.0573024\n",
      "DEBUG \t batch ate = -0.056643\n",
      "DEBUG \t batch ate = -0.0563594\n",
      "DEBUG \t batch ate = -0.0564344\n",
      "DEBUG \t batch ate = -0.057029\n",
      "DEBUG \t batch ate = -0.055928\n",
      "DEBUG \t batch ate = -0.0557673\n",
      "DEBUG \t batch ate = -0.05828\n",
      "DEBUG \t batch ate = -0.0562664\n",
      "DEBUG \t batch ate = -0.055508\n",
      "DEBUG \t batch ate = -0.058313\n",
      "DEBUG \t batch ate = -0.0567464\n",
      "DEBUG \t batch ate = -0.0549017\n",
      "DEBUG \t batch ate = -0.0560295\n",
      "DEBUG \t batch ate = -0.0588721\n",
      "DEBUG \t batch ate = -0.0543164\n",
      "DEBUG \t batch ate = -0.0567193\n",
      "DEBUG \t batch ate = -0.0567422\n",
      "DEBUG \t batch ate = -0.058852\n",
      "DEBUG \t batch ate = -0.0544509\n",
      "DEBUG \t batch ate = -0.05806\n",
      "DEBUG \t batch ate = -0.0590629\n",
      "DEBUG \t batch ate = -0.0555819\n",
      "DEBUG \t batch ate = -0.0574472\n",
      "DEBUG \t batch ate = -0.0559666\n",
      "DEBUG \t batch ate = -0.0563957\n",
      "DEBUG \t batch ate = -0.0569026\n",
      "DEBUG \t batch ate = -0.0559123\n",
      "DEBUG \t batch ate = -0.059111\n",
      "DEBUG \t batch ate = -0.0608811\n",
      "DEBUG \t batch ate = -0.0569527\n",
      "DEBUG \t batch ate = -0.0555505\n",
      "DEBUG \t batch ate = -0.055966\n",
      "DEBUG \t batch ate = -0.0564904\n",
      "DEBUG \t batch ate = -0.0583666\n",
      "DEBUG \t batch ate = -0.0580532\n",
      "DEBUG \t batch ate = -0.0578722\n",
      "DEBUG \t batch ate = -0.05667\n",
      "DEBUG \t batch ate = -0.0572794\n",
      "DEBUG \t batch ate = -0.0598732\n",
      "DEBUG \t batch ate = -0.0582616\n",
      "DEBUG \t batch ate = -0.0562929\n",
      "DEBUG \t batch ate = -0.0598542\n",
      "DEBUG \t batch ate = -0.057285\n",
      "DEBUG \t batch ate = -0.0559276\n",
      "DEBUG \t batch ate = -0.055897\n",
      "DEBUG \t batch ate = -0.0585399\n",
      "DEBUG \t batch ate = -0.0566901\n",
      "DEBUG \t batch ate = -0.0560034\n",
      "DEBUG \t batch ate = -0.0575509\n",
      "DEBUG \t batch ate = -0.0565856\n",
      "DEBUG \t batch ate = -0.0561804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.056994025\n",
      "randomReplacedDataset8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Training with 959 minibatches per epoch\n",
      "DEBUG \t step     0 loss = 37.7497\n",
      "DEBUG \t step   100 loss = 37.6847\n",
      "DEBUG \t step   200 loss = 37.5399\n",
      "DEBUG \t step   300 loss = 39.2148\n",
      "DEBUG \t step   400 loss = 37.398\n",
      "DEBUG \t step   500 loss = 36.0838\n",
      "DEBUG \t step   600 loss = 37.647\n",
      "DEBUG \t step   700 loss = 37.0493\n",
      "DEBUG \t step   800 loss = 36.8636\n",
      "DEBUG \t step   900 loss = 37.187\n",
      "DEBUG \t step  1000 loss = 36.7762\n",
      "DEBUG \t step  1100 loss = 36.0937\n",
      "DEBUG \t step  1200 loss = 36.6706\n",
      "DEBUG \t step  1300 loss = 36.9844\n",
      "DEBUG \t step  1400 loss = 37.1844\n",
      "DEBUG \t step  1500 loss = 37.1308\n",
      "DEBUG \t step  1600 loss = 39.1861\n",
      "DEBUG \t step  1700 loss = 36.996\n",
      "DEBUG \t step  1800 loss = 38.7978\n",
      "DEBUG \t step  1900 loss = 37.0659\n",
      "DEBUG \t step  2000 loss = 36.9392\n",
      "DEBUG \t step  2100 loss = 36.9086\n",
      "DEBUG \t step  2200 loss = 38.4717\n",
      "DEBUG \t step  2300 loss = 37.4286\n",
      "DEBUG \t step  2400 loss = 38.759\n",
      "DEBUG \t step  2500 loss = 36.3101\n",
      "DEBUG \t step  2600 loss = 37.0405\n",
      "DEBUG \t step  2700 loss = 37.2975\n",
      "DEBUG \t step  2800 loss = 36.8952\n",
      "DEBUG \t step  2900 loss = 37.5831\n",
      "DEBUG \t step  3000 loss = 37.7618\n",
      "DEBUG \t step  3100 loss = 36.9965\n",
      "DEBUG \t step  3200 loss = 36.6846\n",
      "DEBUG \t step  3300 loss = 36.7354\n",
      "DEBUG \t step  3400 loss = 36.8074\n",
      "DEBUG \t step  3500 loss = 36.4732\n",
      "DEBUG \t step  3600 loss = 36.8649\n",
      "DEBUG \t step  3700 loss = 37.5217\n",
      "DEBUG \t step  3800 loss = 37.6588\n",
      "DEBUG \t step  3900 loss = 36.6103\n",
      "DEBUG \t step  4000 loss = 36.1526\n",
      "DEBUG \t step  4100 loss = 37.0803\n",
      "DEBUG \t step  4200 loss = 37.6734\n",
      "DEBUG \t step  4300 loss = 38.6796\n",
      "DEBUG \t step  4400 loss = 37.5608\n",
      "DEBUG \t step  4500 loss = 35.9101\n",
      "DEBUG \t step  4600 loss = 36.6146\n",
      "DEBUG \t step  4700 loss = 38.3764\n",
      "INFO \t Evaluating 240 minibatches\n",
      "DEBUG \t batch ate = 0.123138\n",
      "DEBUG \t batch ate = 0.120888\n",
      "DEBUG \t batch ate = 0.123617\n",
      "DEBUG \t batch ate = 0.119661\n",
      "DEBUG \t batch ate = 0.120835\n",
      "DEBUG \t batch ate = 0.120914\n",
      "DEBUG \t batch ate = 0.121264\n",
      "DEBUG \t batch ate = 0.119263\n",
      "DEBUG \t batch ate = 0.122402\n",
      "DEBUG \t batch ate = 0.124441\n",
      "DEBUG \t batch ate = 0.125413\n",
      "DEBUG \t batch ate = 0.123876\n",
      "DEBUG \t batch ate = 0.122047\n",
      "DEBUG \t batch ate = 0.123215\n",
      "DEBUG \t batch ate = 0.124236\n",
      "DEBUG \t batch ate = 0.118456\n",
      "DEBUG \t batch ate = 0.122797\n",
      "DEBUG \t batch ate = 0.121257\n",
      "DEBUG \t batch ate = 0.125357\n",
      "DEBUG \t batch ate = 0.123951\n",
      "DEBUG \t batch ate = 0.119572\n",
      "DEBUG \t batch ate = 0.124747\n",
      "DEBUG \t batch ate = 0.118522\n",
      "DEBUG \t batch ate = 0.119617\n",
      "DEBUG \t batch ate = 0.12225\n",
      "DEBUG \t batch ate = 0.124674\n",
      "DEBUG \t batch ate = 0.118431\n",
      "DEBUG \t batch ate = 0.119303\n",
      "DEBUG \t batch ate = 0.122172\n",
      "DEBUG \t batch ate = 0.123469\n",
      "DEBUG \t batch ate = 0.124608\n",
      "DEBUG \t batch ate = 0.121373\n",
      "DEBUG \t batch ate = 0.124815\n",
      "DEBUG \t batch ate = 0.123011\n",
      "DEBUG \t batch ate = 0.124439\n",
      "DEBUG \t batch ate = 0.119632\n",
      "DEBUG \t batch ate = 0.121073\n",
      "DEBUG \t batch ate = 0.124165\n",
      "DEBUG \t batch ate = 0.126004\n",
      "DEBUG \t batch ate = 0.123274\n",
      "DEBUG \t batch ate = 0.121228\n",
      "DEBUG \t batch ate = 0.124309\n",
      "DEBUG \t batch ate = 0.1222\n",
      "DEBUG \t batch ate = 0.122189\n",
      "DEBUG \t batch ate = 0.121871\n",
      "DEBUG \t batch ate = 0.122259\n",
      "DEBUG \t batch ate = 0.120644\n",
      "DEBUG \t batch ate = 0.120789\n",
      "DEBUG \t batch ate = 0.123308\n",
      "DEBUG \t batch ate = 0.124783\n",
      "DEBUG \t batch ate = 0.119703\n",
      "DEBUG \t batch ate = 0.122822\n",
      "DEBUG \t batch ate = 0.119583\n",
      "DEBUG \t batch ate = 0.118925\n",
      "DEBUG \t batch ate = 0.122005\n",
      "DEBUG \t batch ate = 0.125074\n",
      "DEBUG \t batch ate = 0.120419\n",
      "DEBUG \t batch ate = 0.125568\n",
      "DEBUG \t batch ate = 0.122915\n",
      "DEBUG \t batch ate = 0.123634\n",
      "DEBUG \t batch ate = 0.125311\n",
      "DEBUG \t batch ate = 0.122807\n",
      "DEBUG \t batch ate = 0.12177\n",
      "DEBUG \t batch ate = 0.123025\n",
      "DEBUG \t batch ate = 0.11943\n",
      "DEBUG \t batch ate = 0.117619\n",
      "DEBUG \t batch ate = 0.121944\n",
      "DEBUG \t batch ate = 0.122561\n",
      "DEBUG \t batch ate = 0.121516\n",
      "DEBUG \t batch ate = 0.124555\n",
      "DEBUG \t batch ate = 0.122751\n",
      "DEBUG \t batch ate = 0.1237\n",
      "DEBUG \t batch ate = 0.121844\n",
      "DEBUG \t batch ate = 0.122049\n",
      "DEBUG \t batch ate = 0.121223\n",
      "DEBUG \t batch ate = 0.120777\n",
      "DEBUG \t batch ate = 0.122504\n",
      "DEBUG \t batch ate = 0.12313\n",
      "DEBUG \t batch ate = 0.123691\n",
      "DEBUG \t batch ate = 0.119429\n",
      "DEBUG \t batch ate = 0.121238\n",
      "DEBUG \t batch ate = 0.118682\n",
      "DEBUG \t batch ate = 0.120689\n",
      "DEBUG \t batch ate = 0.123916\n",
      "DEBUG \t batch ate = 0.120451\n",
      "DEBUG \t batch ate = 0.120061\n",
      "DEBUG \t batch ate = 0.120409\n",
      "DEBUG \t batch ate = 0.122377\n",
      "DEBUG \t batch ate = 0.11894\n",
      "DEBUG \t batch ate = 0.120919\n",
      "DEBUG \t batch ate = 0.12236\n",
      "DEBUG \t batch ate = 0.122649\n",
      "DEBUG \t batch ate = 0.121965\n",
      "DEBUG \t batch ate = 0.12393\n",
      "DEBUG \t batch ate = 0.123303\n",
      "DEBUG \t batch ate = 0.122371\n",
      "DEBUG \t batch ate = 0.123007\n",
      "DEBUG \t batch ate = 0.123374\n",
      "DEBUG \t batch ate = 0.123347\n",
      "DEBUG \t batch ate = 0.118538\n",
      "DEBUG \t batch ate = 0.121423\n",
      "DEBUG \t batch ate = 0.117302\n",
      "DEBUG \t batch ate = 0.122878\n",
      "DEBUG \t batch ate = 0.121731\n",
      "DEBUG \t batch ate = 0.120793\n",
      "DEBUG \t batch ate = 0.12392\n",
      "DEBUG \t batch ate = 0.121647\n",
      "DEBUG \t batch ate = 0.122162\n",
      "DEBUG \t batch ate = 0.12022\n",
      "DEBUG \t batch ate = 0.123026\n",
      "DEBUG \t batch ate = 0.12187\n",
      "DEBUG \t batch ate = 0.123102\n",
      "DEBUG \t batch ate = 0.122614\n",
      "DEBUG \t batch ate = 0.121669\n",
      "DEBUG \t batch ate = 0.122257\n",
      "DEBUG \t batch ate = 0.125023\n",
      "DEBUG \t batch ate = 0.12096\n",
      "DEBUG \t batch ate = 0.122941\n",
      "DEBUG \t batch ate = 0.120042\n",
      "DEBUG \t batch ate = 0.122314\n",
      "DEBUG \t batch ate = 0.122542\n",
      "DEBUG \t batch ate = 0.122267\n",
      "DEBUG \t batch ate = 0.124266\n",
      "DEBUG \t batch ate = 0.127405\n",
      "DEBUG \t batch ate = 0.122556\n",
      "DEBUG \t batch ate = 0.118851\n",
      "DEBUG \t batch ate = 0.123198\n",
      "DEBUG \t batch ate = 0.119543\n",
      "DEBUG \t batch ate = 0.120041\n",
      "DEBUG \t batch ate = 0.11959\n",
      "DEBUG \t batch ate = 0.125467\n",
      "DEBUG \t batch ate = 0.122345\n",
      "DEBUG \t batch ate = 0.119651\n",
      "DEBUG \t batch ate = 0.120001\n",
      "DEBUG \t batch ate = 0.118105\n",
      "DEBUG \t batch ate = 0.124314\n",
      "DEBUG \t batch ate = 0.124216\n",
      "DEBUG \t batch ate = 0.11951\n",
      "DEBUG \t batch ate = 0.124321\n",
      "DEBUG \t batch ate = 0.124111\n",
      "DEBUG \t batch ate = 0.121301\n",
      "DEBUG \t batch ate = 0.125107\n",
      "DEBUG \t batch ate = 0.124387\n",
      "DEBUG \t batch ate = 0.118797\n",
      "DEBUG \t batch ate = 0.121987\n",
      "DEBUG \t batch ate = 0.118475\n",
      "DEBUG \t batch ate = 0.120646\n",
      "DEBUG \t batch ate = 0.123955\n",
      "DEBUG \t batch ate = 0.122643\n",
      "DEBUG \t batch ate = 0.120124\n",
      "DEBUG \t batch ate = 0.122846\n",
      "DEBUG \t batch ate = 0.11886\n",
      "DEBUG \t batch ate = 0.121397\n",
      "DEBUG \t batch ate = 0.125877\n",
      "DEBUG \t batch ate = 0.12291\n",
      "DEBUG \t batch ate = 0.119429\n",
      "DEBUG \t batch ate = 0.120464\n",
      "DEBUG \t batch ate = 0.120181\n",
      "DEBUG \t batch ate = 0.121143\n",
      "DEBUG \t batch ate = 0.122561\n",
      "DEBUG \t batch ate = 0.121145\n",
      "DEBUG \t batch ate = 0.125745\n",
      "DEBUG \t batch ate = 0.120412\n",
      "DEBUG \t batch ate = 0.123335\n",
      "DEBUG \t batch ate = 0.119092\n",
      "DEBUG \t batch ate = 0.123698\n",
      "DEBUG \t batch ate = 0.123954\n",
      "DEBUG \t batch ate = 0.121853\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Define folder path\n",
    "folder_path = \"./evaluationDatasets/Replace/\"\n",
    "\n",
    "# List to store treatment effects\n",
    "ate_values = []\n",
    "\n",
    "feature_names = ['NumberOfOffers', 'Action', 'org:resource',\n",
    "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
    "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
    "       'CreditScore', 'OfferedAmount', 'offerNumber','timeApplication', 'weekdayApplication']\n",
    "\n",
    "columns_to_drop = ['offerSuccess', 'treatmentOffer']\n",
    "\n",
    "# cevae model settings\n",
    "outcome_dist = \"normal\"\n",
    "latent_dim = 20\n",
    "hidden_dim = 200\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.01\n",
    "num_layers = 2\n",
    "\n",
    "cevae = CEVAE(outcome_dist=outcome_dist,\n",
    "              latent_dim=latent_dim,\n",
    "              hidden_dim=hidden_dim,\n",
    "              num_epochs=num_epochs,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              learning_rate_decay=learning_rate_decay,\n",
    "              num_layers=num_layers)\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    print(file_name)\n",
    "    # Read CSV file\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    refutation = pd.read_csv(file_path)\n",
    "    \n",
    "    # Split data to training and testing samples for model validation\n",
    "    train, test = train_test_split(refutation, test_size=0.2, random_state=11101)\n",
    "    \n",
    "    X_train = train[feature_names].values\n",
    "    X_test = test[feature_names].values\n",
    "    # X_train = train.drop(columns=columns_to_drop).values\n",
    "    # X_test = test.drop(columns=columns_to_drop).values\n",
    "    treatment_train = train['treatmentOffer'].values\n",
    "    y_train = train['offerSuccess'].values\n",
    "\n",
    "    # fit\n",
    "    losses = cevae.fit(X=torch.tensor(X_train, dtype=torch.float),\n",
    "                       treatment=torch.tensor(treatment_train, dtype=torch.float),\n",
    "                       y=torch.tensor(y_train, dtype=torch.float))\n",
    "    \n",
    "    # predict\n",
    "    ate_val = cevae.predict(X_test)\n",
    "    print(ate_val.mean())\n",
    "    ate_values.append(ate_val.mean())\n",
    "\n",
    "print(\"Average ATE: \")\n",
    "print(sum(ate_values) / len(ate_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-causalml",
   "language": "python",
   "name": "nn-causalml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

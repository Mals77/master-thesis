{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63886807-67ed-4ddb-92a4-b881bb88423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 14:14:30.114458: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-15 14:14:31.475537: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 14:14:31.475573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 14:14:31.477002: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 14:14:31.802587: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 14:14:58.435892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "\n",
    "from causalml.inference.meta import LRSRegressor\n",
    "from causalml.inference.meta import XGBTRegressor, MLPTRegressor\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor\n",
    "from causalml.inference.tf import DragonNet\n",
    "from causalml.match import NearestNeighborMatch, MatchOptimizer, create_table_one\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.dataset.regression import *\n",
    "from causalml.metrics import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "from causalml.inference.nn import CEVAE\n",
    "from causalml.dataset import simulate_hidden_confounder\n",
    "\n",
    "import os, sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Paired')\n",
    "plt.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161a78d9-b745-4eac-9ff6-10270dd30782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bpi2017_final.csv\")\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90aba3a4-965d-496f-babb-ef916ff93d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['NumberOfOffers', 'Action', 'org:resource',\n",
    "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
    "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
    "       'CreditScore', 'OfferedAmount', 'offerNumber','timeApplication', 'weekdayApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa096af-76bc-49f7-8412-5d65ea3feae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_outcome = df['offerSuccess'].values\n",
    "x_feature = df[feature_names].values\n",
    "#check which one works\n",
    "#t_treatment = struct_data['treatment'].values\n",
    "t_treatment = np.array([np.array([value]) for value in df['treatmentOffer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f1150b-d1c2-4ec7-8c11-841b172c8f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14979/14979 [==============================] - 41s 3ms/step - loss: 3100.4497 - regression_loss: 1485.2797 - binary_classification_loss: 120.6330 - treatment_accuracy: 0.5967 - track_epsilon: 0.0014 - val_loss: 54.0649 - val_regression_loss: 5.8795 - val_binary_classification_loss: 38.6904 - val_treatment_accuracy: 0.6766 - val_track_epsilon: 4.7957e-04 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 109.0726 - regression_loss: 14.4636 - binary_classification_loss: 77.5407 - treatment_accuracy: 0.6663 - track_epsilon: 8.3902e-04 - val_loss: 54.2332 - val_regression_loss: 6.1323 - val_binary_classification_loss: 40.3825 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 4.9070e-04 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 222.7309 - regression_loss: 81.9535 - binary_classification_loss: 57.0485 - treatment_accuracy: 0.6610 - track_epsilon: 8.7838e-04 - val_loss: 71.2701 - val_regression_loss: 14.4648 - val_binary_classification_loss: 40.6543 - val_treatment_accuracy: 0.6709 - val_track_epsilon: 0.0016 - lr: 0.0010\n",
      "Epoch 1/100\n",
      "14979/14979 [==============================] - 40s 3ms/step - loss: 73.4460 - regression_loss: 14.6425 - binary_classification_loss: 42.5179 - treatment_accuracy: 0.6422 - track_epsilon: 0.0020 - val_loss: 71.3668 - val_regression_loss: 14.5078 - val_binary_classification_loss: 40.7668 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 4.2215e-05 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 72.5140 - regression_loss: 14.6258 - binary_classification_loss: 41.7184 - treatment_accuracy: 0.6571 - track_epsilon: 0.0016 - val_loss: 72.3127 - val_regression_loss: 14.4884 - val_binary_classification_loss: 41.8492 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 3.5707e-04 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 72.1266 - regression_loss: 14.6250 - binary_classification_loss: 41.4251 - treatment_accuracy: 0.6626 - track_epsilon: 0.0015 - val_loss: 70.9512 - val_regression_loss: 14.4572 - val_binary_classification_loss: 40.6367 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 3.7858e-04 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 71.8800 - regression_loss: 14.6172 - binary_classification_loss: 41.2811 - treatment_accuracy: 0.6652 - track_epsilon: 0.0014 - val_loss: 70.9317 - val_regression_loss: 14.5052 - val_binary_classification_loss: 40.5862 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0030 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 71.6801 - regression_loss: 14.6064 - binary_classification_loss: 41.1838 - treatment_accuracy: 0.6660 - track_epsilon: 0.0015 - val_loss: 71.2931 - val_regression_loss: 14.5467 - val_binary_classification_loss: 40.9680 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 7.6532e-04 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 71.4875 - regression_loss: 14.5915 - binary_classification_loss: 41.0977 - treatment_accuracy: 0.6668 - track_epsilon: 0.0015 - val_loss: 71.8636 - val_regression_loss: 14.4918 - val_binary_classification_loss: 41.7037 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0016 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 71.3203 - regression_loss: 14.5722 - binary_classification_loss: 41.0408 - treatment_accuracy: 0.6671 - track_epsilon: 0.0015 - val_loss: 70.8449 - val_regression_loss: 14.5576 - val_binary_classification_loss: 40.5865 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0043 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 71.1858 - regression_loss: 14.5598 - binary_classification_loss: 40.9971 - treatment_accuracy: 0.6672 - track_epsilon: 0.0016 - val_loss: 70.9044 - val_regression_loss: 14.5814 - val_binary_classification_loss: 40.6676 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0047 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 71.0709 - regression_loss: 14.5525 - binary_classification_loss: 40.9598 - treatment_accuracy: 0.6673 - track_epsilon: 0.0017 - val_loss: 71.0453 - val_regression_loss: 14.5392 - val_binary_classification_loss: 40.9764 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0042 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.9700 - regression_loss: 14.5483 - binary_classification_loss: 40.9255 - treatment_accuracy: 0.6673 - track_epsilon: 0.0019 - val_loss: 70.4839 - val_regression_loss: 14.4954 - val_binary_classification_loss: 40.5738 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.8708 - regression_loss: 14.5431 - binary_classification_loss: 40.8927 - treatment_accuracy: 0.6673 - track_epsilon: 0.0020 - val_loss: 70.6469 - val_regression_loss: 14.5660 - val_binary_classification_loss: 40.6600 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 5.0602e-04 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 70.7912 - regression_loss: 14.5419 - binary_classification_loss: 40.8668 - treatment_accuracy: 0.6673 - track_epsilon: 0.0021 - val_loss: 70.4622 - val_regression_loss: 14.5098 - val_binary_classification_loss: 40.5920 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0049 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 70.7154 - regression_loss: 14.5394 - binary_classification_loss: 40.8447 - treatment_accuracy: 0.6673 - track_epsilon: 0.0021 - val_loss: 70.2761 - val_regression_loss: 14.4669 - val_binary_classification_loss: 40.5809 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 5.7303e-04 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.6452 - regression_loss: 14.5367 - binary_classification_loss: 40.8263 - treatment_accuracy: 0.6673 - track_epsilon: 0.0021 - val_loss: 70.2117 - val_regression_loss: 14.4574 - val_binary_classification_loss: 40.5823 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 5.1868e-04 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "14979/14979 [==============================] - 37s 3ms/step - loss: 70.5889 - regression_loss: 14.5361 - binary_classification_loss: 40.8139 - treatment_accuracy: 0.6673 - track_epsilon: 0.0021 - val_loss: 70.1949 - val_regression_loss: 14.4628 - val_binary_classification_loss: 40.5914 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 70.5353 - regression_loss: 14.5356 - binary_classification_loss: 40.8014 - treatment_accuracy: 0.6673 - track_epsilon: 0.0022 - val_loss: 70.2045 - val_regression_loss: 14.4762 - val_binary_classification_loss: 40.6158 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 1.5944e-04 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 70.4796 - regression_loss: 14.5325 - binary_classification_loss: 40.7914 - treatment_accuracy: 0.6673 - track_epsilon: 0.0021 - val_loss: 70.1677 - val_regression_loss: 14.4937 - val_binary_classification_loss: 40.5749 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 70.4399 - regression_loss: 14.5343 - binary_classification_loss: 40.7835 - treatment_accuracy: 0.6673 - track_epsilon: 0.0022 - val_loss: 70.2513 - val_regression_loss: 14.5099 - val_binary_classification_loss: 40.6494 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0030 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 70.3918 - regression_loss: 14.5321 - binary_classification_loss: 40.7744 - treatment_accuracy: 0.6673 - track_epsilon: 0.0022 - val_loss: 70.0583 - val_regression_loss: 14.4650 - val_binary_classification_loss: 40.5914 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0023 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 70.3588 - regression_loss: 14.5316 - binary_classification_loss: 40.7738 - treatment_accuracy: 0.6673 - track_epsilon: 0.0022 - val_loss: 70.1858 - val_regression_loss: 14.5171 - val_binary_classification_loss: 40.6324 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0033 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.3233 - regression_loss: 14.5309 - binary_classification_loss: 40.7691 - treatment_accuracy: 0.6673 - track_epsilon: 0.0023 - val_loss: 70.0830 - val_regression_loss: 14.4945 - val_binary_classification_loss: 40.5919 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0045 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.2824 - regression_loss: 14.5289 - binary_classification_loss: 40.7616 - treatment_accuracy: 0.6673 - track_epsilon: 0.0024 - val_loss: 69.9982 - val_regression_loss: 14.4727 - val_binary_classification_loss: 40.5954 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0043 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 70.2555 - regression_loss: 14.5296 - binary_classification_loss: 40.7600 - treatment_accuracy: 0.6673 - track_epsilon: 0.0023 - val_loss: 69.9196 - val_regression_loss: 14.4609 - val_binary_classification_loss: 40.5747 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0027 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 70.2255 - regression_loss: 14.5266 - binary_classification_loss: 40.7614 - treatment_accuracy: 0.6673 - track_epsilon: 0.0024 - val_loss: 69.8900 - val_regression_loss: 14.4581 - val_binary_classification_loss: 40.5741 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0038 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.1987 - regression_loss: 14.5267 - binary_classification_loss: 40.7575 - treatment_accuracy: 0.6673 - track_epsilon: 0.0025 - val_loss: 69.9214 - val_regression_loss: 14.4878 - val_binary_classification_loss: 40.5842 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0014 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.1709 - regression_loss: 14.5253 - binary_classification_loss: 40.7560 - treatment_accuracy: 0.6673 - track_epsilon: 0.0025 - val_loss: 69.8663 - val_regression_loss: 14.4729 - val_binary_classification_loss: 40.5763 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 9.9659e-04 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.1511 - regression_loss: 14.5246 - binary_classification_loss: 40.7580 - treatment_accuracy: 0.6673 - track_epsilon: 0.0025 - val_loss: 69.8194 - val_regression_loss: 14.4601 - val_binary_classification_loss: 40.5741 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 8.4776e-05 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.1251 - regression_loss: 14.5233 - binary_classification_loss: 40.7549 - treatment_accuracy: 0.6673 - track_epsilon: 0.0026 - val_loss: 69.8667 - val_regression_loss: 14.4707 - val_binary_classification_loss: 40.5838 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0075 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.1014 - regression_loss: 14.5203 - binary_classification_loss: 40.7562 - treatment_accuracy: 0.6673 - track_epsilon: 0.0026 - val_loss: 69.9486 - val_regression_loss: 14.4657 - val_binary_classification_loss: 40.7004 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0055 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 70.0790 - regression_loss: 14.5184 - binary_classification_loss: 40.7551 - treatment_accuracy: 0.6673 - track_epsilon: 0.0027 - val_loss: 69.7867 - val_regression_loss: 14.4587 - val_binary_classification_loss: 40.6008 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0019 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.0633 - regression_loss: 14.5174 - binary_classification_loss: 40.7577 - treatment_accuracy: 0.6673 - track_epsilon: 0.0027 - val_loss: 69.8780 - val_regression_loss: 14.4939 - val_binary_classification_loss: 40.6187 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0022 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 70.0448 - regression_loss: 14.5171 - binary_classification_loss: 40.7548 - treatment_accuracy: 0.6673 - track_epsilon: 0.0028 - val_loss: 69.7722 - val_regression_loss: 14.4588 - val_binary_classification_loss: 40.6137 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 3.2890e-04 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.0225 - regression_loss: 14.5154 - binary_classification_loss: 40.7511 - treatment_accuracy: 0.6673 - track_epsilon: 0.0027 - val_loss: 69.7721 - val_regression_loss: 14.4780 - val_binary_classification_loss: 40.5812 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0026 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 70.0107 - regression_loss: 14.5153 - binary_classification_loss: 40.7523 - treatment_accuracy: 0.6673 - track_epsilon: 0.0029 - val_loss: 69.8146 - val_regression_loss: 14.4963 - val_binary_classification_loss: 40.5864 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0044 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 70.0003 - regression_loss: 14.5150 - binary_classification_loss: 40.7550 - treatment_accuracy: 0.6673 - track_epsilon: 0.0028 - val_loss: 69.7171 - val_regression_loss: 14.4642 - val_binary_classification_loss: 40.5801 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.9849 - regression_loss: 14.5137 - binary_classification_loss: 40.7544 - treatment_accuracy: 0.6673 - track_epsilon: 0.0031 - val_loss: 69.7264 - val_regression_loss: 14.4666 - val_binary_classification_loss: 40.5989 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.9702 - regression_loss: 14.5131 - binary_classification_loss: 40.7523 - treatment_accuracy: 0.6673 - track_epsilon: 0.0030 - val_loss: 69.7110 - val_regression_loss: 14.4590 - val_binary_classification_loss: 40.6110 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0016 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.9624 - regression_loss: 14.5131 - binary_classification_loss: 40.7554 - treatment_accuracy: 0.6673 - track_epsilon: 0.0030 - val_loss: 69.6920 - val_regression_loss: 14.4701 - val_binary_classification_loss: 40.5758 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0023 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.9472 - regression_loss: 14.5128 - binary_classification_loss: 40.7506 - treatment_accuracy: 0.6673 - track_epsilon: 0.0030 - val_loss: 69.7348 - val_regression_loss: 14.4591 - val_binary_classification_loss: 40.6591 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.9407 - regression_loss: 14.5125 - binary_classification_loss: 40.7537 - treatment_accuracy: 0.6673 - track_epsilon: 0.0031 - val_loss: 69.6999 - val_regression_loss: 14.4691 - val_binary_classification_loss: 40.5920 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0036 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.9305 - regression_loss: 14.5110 - binary_classification_loss: 40.7563 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.6357 - val_regression_loss: 14.4597 - val_binary_classification_loss: 40.5749 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0022 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 69.9208 - regression_loss: 14.5118 - binary_classification_loss: 40.7527 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.6703 - val_regression_loss: 14.4559 - val_binary_classification_loss: 40.6233 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0025 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.9147 - regression_loss: 14.5116 - binary_classification_loss: 40.7552 - treatment_accuracy: 0.6673 - track_epsilon: 0.0031 - val_loss: 69.6244 - val_regression_loss: 14.4594 - val_binary_classification_loss: 40.5764 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0016 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "14979/14979 [==============================] - 37s 3ms/step - loss: 69.9080 - regression_loss: 14.5110 - binary_classification_loss: 40.7574 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.6383 - val_regression_loss: 14.4567 - val_binary_classification_loss: 40.5787 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0083 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8943 - regression_loss: 14.5103 - binary_classification_loss: 40.7519 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.6407 - val_regression_loss: 14.4670 - val_binary_classification_loss: 40.5746 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0055 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8910 - regression_loss: 14.5111 - binary_classification_loss: 40.7529 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.6263 - val_regression_loss: 14.4605 - val_binary_classification_loss: 40.5795 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0052 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.8857 - regression_loss: 14.5108 - binary_classification_loss: 40.7544 - treatment_accuracy: 0.6673 - track_epsilon: 0.0031 - val_loss: 69.6364 - val_regression_loss: 14.4578 - val_binary_classification_loss: 40.5937 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0080 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.8783 - regression_loss: 14.5106 - binary_classification_loss: 40.7530 - treatment_accuracy: 0.6673 - track_epsilon: 0.0031 - val_loss: 69.5825 - val_regression_loss: 14.4577 - val_binary_classification_loss: 40.5744 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0026 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.8725 - regression_loss: 14.5101 - binary_classification_loss: 40.7542 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.5956 - val_regression_loss: 14.4654 - val_binary_classification_loss: 40.5746 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 5.8588e-04 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "10395/14979 [===================>..........] - ETA: 9s - loss: 69.9032 - regression_loss: 14.5129 - binary_classification_loss: 40.7836 - treatment_accuracy: 0.6666 - track_epsilon: 0.0031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14979/14979 [==============================] - 37s 3ms/step - loss: 69.8502 - regression_loss: 14.5103 - binary_classification_loss: 40.7498 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.5936 - val_regression_loss: 14.4565 - val_binary_classification_loss: 40.6065 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0035 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.8497 - regression_loss: 14.5097 - binary_classification_loss: 40.7549 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.6849 - val_regression_loss: 14.4639 - val_binary_classification_loss: 40.6879 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.8467 - regression_loss: 14.5101 - binary_classification_loss: 40.7542 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.5809 - val_regression_loss: 14.4604 - val_binary_classification_loss: 40.5857 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0033 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.8437 - regression_loss: 14.5099 - binary_classification_loss: 40.7557 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.5644 - val_regression_loss: 14.4602 - val_binary_classification_loss: 40.5856 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0030 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8379 - regression_loss: 14.5101 - binary_classification_loss: 40.7519 - treatment_accuracy: 0.6673 - track_epsilon: 0.0031 - val_loss: 69.5760 - val_regression_loss: 14.4672 - val_binary_classification_loss: 40.5754 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0036 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8346 - regression_loss: 14.5099 - binary_classification_loss: 40.7527 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.5416 - val_regression_loss: 14.4565 - val_binary_classification_loss: 40.5744 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.8329 - regression_loss: 14.5097 - binary_classification_loss: 40.7540 - treatment_accuracy: 0.6673 - track_epsilon: 0.0037 - val_loss: 69.6209 - val_regression_loss: 14.4654 - val_binary_classification_loss: 40.6301 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0039 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.8293 - regression_loss: 14.5094 - binary_classification_loss: 40.7536 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.6576 - val_regression_loss: 14.4611 - val_binary_classification_loss: 40.6635 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0061 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8266 - regression_loss: 14.5095 - binary_classification_loss: 40.7529 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.6090 - val_regression_loss: 14.4663 - val_binary_classification_loss: 40.6214 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0026 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8243 - regression_loss: 14.5096 - binary_classification_loss: 40.7535 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.5434 - val_regression_loss: 14.4579 - val_binary_classification_loss: 40.5771 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0026 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.8215 - regression_loss: 14.5093 - binary_classification_loss: 40.7531 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.6619 - val_regression_loss: 14.4626 - val_binary_classification_loss: 40.6959 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0050 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.8193 - regression_loss: 14.5096 - binary_classification_loss: 40.7525 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.5403 - val_regression_loss: 14.4600 - val_binary_classification_loss: 40.5787 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8165 - regression_loss: 14.5092 - binary_classification_loss: 40.7527 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5619 - val_regression_loss: 14.4656 - val_binary_classification_loss: 40.5924 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0020 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8145 - regression_loss: 14.5094 - binary_classification_loss: 40.7521 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.5438 - val_regression_loss: 14.4570 - val_binary_classification_loss: 40.5927 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0032 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.8108 - regression_loss: 14.5092 - binary_classification_loss: 40.7508 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.5368 - val_regression_loss: 14.4609 - val_binary_classification_loss: 40.5756 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0027 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.8128 - regression_loss: 14.5094 - binary_classification_loss: 40.7541 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5509 - val_regression_loss: 14.4650 - val_binary_classification_loss: 40.5897 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.8085 - regression_loss: 14.5088 - binary_classification_loss: 40.7532 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5333 - val_regression_loss: 14.4620 - val_binary_classification_loss: 40.5782 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0020 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8094 - regression_loss: 14.5091 - binary_classification_loss: 40.7543 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5640 - val_regression_loss: 14.4560 - val_binary_classification_loss: 40.6178 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0034 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 69.8079 - regression_loss: 14.5093 - binary_classification_loss: 40.7536 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.5318 - val_regression_loss: 14.4625 - val_binary_classification_loss: 40.5743 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0023 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 69.8059 - regression_loss: 14.5091 - binary_classification_loss: 40.7544 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5319 - val_regression_loss: 14.4585 - val_binary_classification_loss: 40.5814 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0068 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 69.8049 - regression_loss: 14.5093 - binary_classification_loss: 40.7530 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5513 - val_regression_loss: 14.4561 - val_binary_classification_loss: 40.6125 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 4.6158e-05 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 69.8019 - regression_loss: 14.5088 - binary_classification_loss: 40.7536 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.8956 - val_regression_loss: 14.4570 - val_binary_classification_loss: 40.8922 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0142 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 69.8025 - regression_loss: 14.5094 - binary_classification_loss: 40.7529 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.5815 - val_regression_loss: 14.4575 - val_binary_classification_loss: 40.6422 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0017 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 69.7999 - regression_loss: 14.5090 - binary_classification_loss: 40.7523 - treatment_accuracy: 0.6673 - track_epsilon: 0.0036 - val_loss: 69.5317 - val_regression_loss: 14.4581 - val_binary_classification_loss: 40.5929 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0051 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.8009 - regression_loss: 14.5089 - binary_classification_loss: 40.7546 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5460 - val_regression_loss: 14.4566 - val_binary_classification_loss: 40.5918 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0077 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7999 - regression_loss: 14.5090 - binary_classification_loss: 40.7538 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5488 - val_regression_loss: 14.4563 - val_binary_classification_loss: 40.6148 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0019 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7955 - regression_loss: 14.5091 - binary_classification_loss: 40.7505 - treatment_accuracy: 0.6673 - track_epsilon: 0.0032 - val_loss: 69.5172 - val_regression_loss: 14.4578 - val_binary_classification_loss: 40.5743 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0063 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7942 - regression_loss: 14.5089 - binary_classification_loss: 40.7505 - treatment_accuracy: 0.6673 - track_epsilon: 0.0036 - val_loss: 69.6378 - val_regression_loss: 14.4564 - val_binary_classification_loss: 40.7043 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 3.3240e-04 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.8004 - regression_loss: 14.5092 - binary_classification_loss: 40.7571 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5608 - val_regression_loss: 14.4698 - val_binary_classification_loss: 40.5968 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0060 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.7980 - regression_loss: 14.5091 - binary_classification_loss: 40.7553 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.6291 - val_regression_loss: 14.4600 - val_binary_classification_loss: 40.6394 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0102 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.7948 - regression_loss: 14.5090 - binary_classification_loss: 40.7525 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5321 - val_regression_loss: 14.4567 - val_binary_classification_loss: 40.5753 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0082 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.7938 - regression_loss: 14.5087 - binary_classification_loss: 40.7528 - treatment_accuracy: 0.6673 - track_epsilon: 0.0033 - val_loss: 69.6445 - val_regression_loss: 14.4577 - val_binary_classification_loss: 40.7062 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0028 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7935 - regression_loss: 14.5092 - binary_classification_loss: 40.7524 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5764 - val_regression_loss: 14.4573 - val_binary_classification_loss: 40.6449 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7941 - regression_loss: 14.5092 - binary_classification_loss: 40.7539 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5062 - val_regression_loss: 14.4567 - val_binary_classification_loss: 40.5736 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0021 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.7917 - regression_loss: 14.5092 - binary_classification_loss: 40.7521 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5023 - val_regression_loss: 14.4562 - val_binary_classification_loss: 40.5739 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 7.8321e-04 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.7912 - regression_loss: 14.5092 - binary_classification_loss: 40.7517 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5214 - val_regression_loss: 14.4646 - val_binary_classification_loss: 40.5767 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7904 - regression_loss: 14.5086 - binary_classification_loss: 40.7526 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.6033 - val_regression_loss: 14.4565 - val_binary_classification_loss: 40.6754 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 1.9987e-04 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7914 - regression_loss: 14.5091 - binary_classification_loss: 40.7533 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5172 - val_regression_loss: 14.4625 - val_binary_classification_loss: 40.5746 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0039 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7927 - regression_loss: 14.5088 - binary_classification_loss: 40.7563 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.6032 - val_regression_loss: 14.4590 - val_binary_classification_loss: 40.6737 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7882 - regression_loss: 14.5085 - binary_classification_loss: 40.7523 - treatment_accuracy: 0.6673 - track_epsilon: 0.0036 - val_loss: 69.5210 - val_regression_loss: 14.4587 - val_binary_classification_loss: 40.5909 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0035 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.7914 - regression_loss: 14.5090 - binary_classification_loss: 40.7547 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5307 - val_regression_loss: 14.4570 - val_binary_classification_loss: 40.5959 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0045 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.7895 - regression_loss: 14.5086 - binary_classification_loss: 40.7538 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5261 - val_regression_loss: 14.4566 - val_binary_classification_loss: 40.5871 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.7884 - regression_loss: 14.5088 - binary_classification_loss: 40.7523 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5048 - val_regression_loss: 14.4563 - val_binary_classification_loss: 40.5801 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 4.7949e-04 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7904 - regression_loss: 14.5090 - binary_classification_loss: 40.7550 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5185 - val_regression_loss: 14.4570 - val_binary_classification_loss: 40.5784 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0065 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7850 - regression_loss: 14.5090 - binary_classification_loss: 40.7501 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5596 - val_regression_loss: 14.4587 - val_binary_classification_loss: 40.5771 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0128 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 69.7877 - regression_loss: 14.5089 - binary_classification_loss: 40.7522 - treatment_accuracy: 0.6673 - track_epsilon: 0.0034 - val_loss: 69.5317 - val_regression_loss: 14.4583 - val_binary_classification_loss: 40.6033 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0020 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69.7865 - regression_loss: 14.5092 - binary_classification_loss: 40.7512 - treatment_accuracy: 0.6673 - track_epsilon: 0.0036 - val_loss: 69.5050 - val_regression_loss: 14.4578 - val_binary_classification_loss: 40.5761 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 69.7898 - regression_loss: 14.5091 - binary_classification_loss: 40.7548 - treatment_accuracy: 0.6673 - track_epsilon: 0.0035 - val_loss: 69.5150 - val_regression_loss: 14.4571 - val_binary_classification_loss: 40.5749 - val_treatment_accuracy: 0.6702 - val_track_epsilon: 0.0058 - lr: 1.0000e-05\n",
      "37448/37448 [==============================] - 33s 883us/step\n"
     ]
    }
   ],
   "source": [
    "dragon = DragonNet(neurons_per_layer=200, targeted_reg=True)\n",
    "dragon_ite = dragon.fit_predict(x_feature, t_treatment, y_outcome, return_components=False)\n",
    "dragon_ate = dragon_ite.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27fa51d-911d-4f10-ba37-aa96b2c9f6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06973356]\n",
      " [0.06973356]\n",
      " [0.06973356]\n",
      " ...\n",
      " [0.06973356]\n",
      " [0.06973356]\n",
      " [0.06973356]]\n",
      "0.06973362\n"
     ]
    }
   ],
   "source": [
    "print(dragon_ite)\n",
    "print(dragon_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ced75d41-c1b6-488d-9354-3ed173b28015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#propensity model for X- & R-Learner\n",
    "p_model = ElasticNetPropensityModel()\n",
    "p = p_model.fit_predict(x_feature, t_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f847556-f95d-48a8-8a8a-e1be79bb56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5a9c3-6e9f-49aa-9453-044c95284eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe749ca3-5c12-4a5c-b433-70b16c694f2b",
   "metadata": {},
   "source": [
    "## CEVAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e80920a-5ccb-4aa7-9071-228302f53d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "from causalml.inference.nn import CEVAE\n",
    "from causalml.dataset import simulate_hidden_confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c2c74d-e827-452d-9082-1fbd95cff7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=11101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1698d98-3ebb-41f6-a52c-0c1737bb2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[feature_names].values\n",
    "X_test = df_test[feature_names].values\n",
    "treatment_train = df_train['treatmentOffer'].values\n",
    "treatment_test = df_test['treatmentOffer'].values\n",
    "y_train = df_train['offerSuccess'].values\n",
    "y_test = df_test['offerSuccess'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f19ae3-646e-45b8-a322-88f151020120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cevae model settings\n",
    "outcome_dist = \"normal\"\n",
    "latent_dim = 20\n",
    "hidden_dim = 200\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.01\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "603cf6e6-93ae-4697-8233-33d3260bd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "cevae = CEVAE(outcome_dist=outcome_dist,\n",
    "              latent_dim=latent_dim,\n",
    "              hidden_dim=hidden_dim,\n",
    "              num_epochs=num_epochs,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              learning_rate_decay=learning_rate_decay,\n",
    "              num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c51f5e8-705f-4fdc-89ca-41d608cf464a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Training with 959 minibatches per epoch\n",
      "DEBUG \t step     0 loss = 25.4679\n",
      "DEBUG \t step   100 loss = 11.3212\n",
      "DEBUG \t step   200 loss = 9.82654\n",
      "DEBUG \t step   300 loss = 7.40294\n",
      "DEBUG \t step   400 loss = 6.05181\n",
      "DEBUG \t step   500 loss = 5.37166\n",
      "DEBUG \t step   600 loss = 4.05129\n",
      "DEBUG \t step   700 loss = 3.87522\n",
      "DEBUG \t step   800 loss = 3.19316\n",
      "DEBUG \t step   900 loss = 2.67495\n",
      "DEBUG \t step  1000 loss = 2.63969\n",
      "DEBUG \t step  1100 loss = 1.88981\n",
      "DEBUG \t step  1200 loss = 1.79991\n",
      "DEBUG \t step  1300 loss = 1.47384\n",
      "DEBUG \t step  1400 loss = 1.13407\n",
      "DEBUG \t step  1500 loss = 0.529295\n",
      "DEBUG \t step  1600 loss = 0.566418\n",
      "DEBUG \t step  1700 loss = -0.200579\n",
      "DEBUG \t step  1800 loss = 0.444337\n",
      "DEBUG \t step  1900 loss = -0.408888\n",
      "DEBUG \t step  2000 loss = 0.0754771\n",
      "DEBUG \t step  2100 loss = -0.3353\n",
      "DEBUG \t step  2200 loss = -0.576501\n",
      "DEBUG \t step  2300 loss = -0.128328\n",
      "DEBUG \t step  2400 loss = -0.717243\n",
      "DEBUG \t step  2500 loss = -0.633714\n",
      "DEBUG \t step  2600 loss = -1.07424\n",
      "DEBUG \t step  2700 loss = -0.950383\n",
      "DEBUG \t step  2800 loss = -1.67317\n",
      "DEBUG \t step  2900 loss = -1.37843\n",
      "DEBUG \t step  3000 loss = -1.15186\n",
      "DEBUG \t step  3100 loss = -1.67128\n",
      "DEBUG \t step  3200 loss = -1.23558\n",
      "DEBUG \t step  3300 loss = -1.59978\n",
      "DEBUG \t step  3400 loss = -0.875901\n",
      "DEBUG \t step  3500 loss = -1.41183\n",
      "DEBUG \t step  3600 loss = -1.68298\n",
      "DEBUG \t step  3700 loss = -1.48817\n",
      "DEBUG \t step  3800 loss = -1.49319\n",
      "DEBUG \t step  3900 loss = -0.611178\n",
      "DEBUG \t step  4000 loss = -1.74965\n",
      "DEBUG \t step  4100 loss = -1.60698\n",
      "DEBUG \t step  4200 loss = -1.51351\n",
      "DEBUG \t step  4300 loss = -2.10772\n",
      "DEBUG \t step  4400 loss = -1.78333\n",
      "DEBUG \t step  4500 loss = -1.66083\n",
      "DEBUG \t step  4600 loss = -1.00604\n",
      "DEBUG \t step  4700 loss = -1.46997\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "losses = cevae.fit(X=torch.tensor(X_train, dtype=torch.float),\n",
    "                   treatment=torch.tensor(treatment_train, dtype=torch.float),\n",
    "                   y=torch.tensor(y_train, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecd63c-eb66-4e6f-be0b-de75cd897da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Evaluating 959 minibatches\n",
      "DEBUG \t batch ate = 0.0692762\n",
      "DEBUG \t batch ate = 0.0559126\n",
      "DEBUG \t batch ate = 0.0475597\n",
      "DEBUG \t batch ate = 0.0564794\n",
      "DEBUG \t batch ate = 0.0592592\n",
      "DEBUG \t batch ate = 0.0519508\n",
      "DEBUG \t batch ate = 0.0493465\n",
      "DEBUG \t batch ate = 0.0527182\n",
      "DEBUG \t batch ate = 0.0600988\n",
      "DEBUG \t batch ate = 0.060301\n",
      "DEBUG \t batch ate = 0.0613353\n",
      "DEBUG \t batch ate = 0.0526241\n",
      "DEBUG \t batch ate = 0.0608231\n",
      "DEBUG \t batch ate = 0.0452978\n",
      "DEBUG \t batch ate = 0.0573853\n",
      "DEBUG \t batch ate = 0.0526404\n",
      "DEBUG \t batch ate = 0.05376\n",
      "DEBUG \t batch ate = 0.0538801\n",
      "DEBUG \t batch ate = 0.0700628\n",
      "DEBUG \t batch ate = 0.0560638\n",
      "DEBUG \t batch ate = 0.0450788\n",
      "DEBUG \t batch ate = 0.0516163\n",
      "DEBUG \t batch ate = 0.04656\n",
      "DEBUG \t batch ate = 0.0498335\n",
      "DEBUG \t batch ate = 0.0657349\n",
      "DEBUG \t batch ate = 0.0535928\n",
      "DEBUG \t batch ate = 0.0469221\n",
      "DEBUG \t batch ate = 0.0517338\n",
      "DEBUG \t batch ate = 0.0536664\n",
      "DEBUG \t batch ate = 0.0498623\n",
      "DEBUG \t batch ate = 0.0607853\n",
      "DEBUG \t batch ate = 0.0466884\n",
      "DEBUG \t batch ate = 0.0600484\n",
      "DEBUG \t batch ate = 0.0576178\n",
      "DEBUG \t batch ate = 0.0638758\n",
      "DEBUG \t batch ate = 0.0539476\n",
      "DEBUG \t batch ate = 0.0542836\n",
      "DEBUG \t batch ate = 0.0614394\n",
      "DEBUG \t batch ate = 0.0505306\n",
      "DEBUG \t batch ate = 0.042825\n",
      "DEBUG \t batch ate = 0.048611\n",
      "DEBUG \t batch ate = 0.0505436\n",
      "DEBUG \t batch ate = 0.0441755\n",
      "DEBUG \t batch ate = 0.0461873\n",
      "DEBUG \t batch ate = 0.0593377\n",
      "DEBUG \t batch ate = 0.0591844\n",
      "DEBUG \t batch ate = 0.0535162\n",
      "DEBUG \t batch ate = 0.0497482\n",
      "DEBUG \t batch ate = 0.0433564\n",
      "DEBUG \t batch ate = 0.0484042\n",
      "DEBUG \t batch ate = 0.0463557\n",
      "DEBUG \t batch ate = 0.0384602\n",
      "DEBUG \t batch ate = 0.0588504\n",
      "DEBUG \t batch ate = 0.0514364\n",
      "DEBUG \t batch ate = 0.0390189\n",
      "DEBUG \t batch ate = 0.0373822\n",
      "DEBUG \t batch ate = 0.0538664\n",
      "DEBUG \t batch ate = 0.0459515\n",
      "DEBUG \t batch ate = 0.0460854\n",
      "DEBUG \t batch ate = 0.0499901\n",
      "DEBUG \t batch ate = 0.0463466\n",
      "DEBUG \t batch ate = 0.046197\n",
      "DEBUG \t batch ate = 0.0657448\n",
      "DEBUG \t batch ate = 0.0602362\n",
      "DEBUG \t batch ate = 0.0484085\n",
      "DEBUG \t batch ate = 0.0364729\n",
      "DEBUG \t batch ate = 0.0527199\n",
      "DEBUG \t batch ate = 0.0597396\n",
      "DEBUG \t batch ate = 0.0512841\n",
      "DEBUG \t batch ate = 0.0588947\n",
      "DEBUG \t batch ate = 0.0468007\n",
      "DEBUG \t batch ate = 0.0500815\n",
      "DEBUG \t batch ate = 0.0608407\n",
      "DEBUG \t batch ate = 0.0533501\n",
      "DEBUG \t batch ate = 0.0483554\n",
      "DEBUG \t batch ate = 0.0518892\n",
      "DEBUG \t batch ate = 0.0683892\n",
      "DEBUG \t batch ate = 0.0404569\n",
      "DEBUG \t batch ate = 0.0568988\n",
      "DEBUG \t batch ate = 0.0606784\n",
      "DEBUG \t batch ate = 0.0428636\n",
      "DEBUG \t batch ate = 0.0665261\n",
      "DEBUG \t batch ate = 0.0546469\n",
      "DEBUG \t batch ate = 0.0647901\n",
      "DEBUG \t batch ate = 0.050693\n",
      "DEBUG \t batch ate = 0.0503661\n",
      "DEBUG \t batch ate = 0.0548317\n",
      "DEBUG \t batch ate = 0.0477105\n",
      "DEBUG \t batch ate = 0.053656\n",
      "DEBUG \t batch ate = 0.058211\n",
      "DEBUG \t batch ate = 0.043293\n",
      "DEBUG \t batch ate = 0.0540851\n",
      "DEBUG \t batch ate = 0.0633759\n",
      "DEBUG \t batch ate = 0.0571008\n",
      "DEBUG \t batch ate = 0.061345\n",
      "DEBUG \t batch ate = 0.0640237\n",
      "DEBUG \t batch ate = 0.0527918\n",
      "DEBUG \t batch ate = 0.0561524\n",
      "DEBUG \t batch ate = 0.0668388\n",
      "DEBUG \t batch ate = 0.0547661\n",
      "DEBUG \t batch ate = 0.0446751\n",
      "DEBUG \t batch ate = 0.0694651\n",
      "DEBUG \t batch ate = 0.0685763\n",
      "DEBUG \t batch ate = 0.0585512\n",
      "DEBUG \t batch ate = 0.0422224\n",
      "DEBUG \t batch ate = 0.057625\n",
      "DEBUG \t batch ate = 0.0565719\n",
      "DEBUG \t batch ate = 0.0708502\n",
      "DEBUG \t batch ate = 0.0521527\n",
      "DEBUG \t batch ate = 0.0660359\n",
      "DEBUG \t batch ate = 0.0515975\n",
      "DEBUG \t batch ate = 0.0553888\n",
      "DEBUG \t batch ate = 0.0411609\n",
      "DEBUG \t batch ate = 0.0462323\n",
      "DEBUG \t batch ate = 0.0627368\n",
      "DEBUG \t batch ate = 0.0471903\n",
      "DEBUG \t batch ate = 0.0457743\n",
      "DEBUG \t batch ate = 0.0561481\n",
      "DEBUG \t batch ate = 0.0493141\n",
      "DEBUG \t batch ate = 0.0580932\n",
      "DEBUG \t batch ate = 0.0518196\n",
      "DEBUG \t batch ate = 0.0527873\n",
      "DEBUG \t batch ate = 0.0559937\n",
      "DEBUG \t batch ate = 0.0514453\n",
      "DEBUG \t batch ate = 0.0538015\n",
      "DEBUG \t batch ate = 0.0558545\n",
      "DEBUG \t batch ate = 0.0579813\n",
      "DEBUG \t batch ate = 0.0519067\n",
      "DEBUG \t batch ate = 0.0492757\n",
      "DEBUG \t batch ate = 0.0626359\n",
      "DEBUG \t batch ate = 0.0471672\n",
      "DEBUG \t batch ate = 0.0379921\n",
      "DEBUG \t batch ate = 0.0539071\n",
      "DEBUG \t batch ate = 0.0636367\n",
      "DEBUG \t batch ate = 0.0608572\n",
      "DEBUG \t batch ate = 0.0551312\n",
      "DEBUG \t batch ate = 0.0559646\n",
      "DEBUG \t batch ate = 0.0500622\n",
      "DEBUG \t batch ate = 0.0574149\n",
      "DEBUG \t batch ate = 0.0554909\n",
      "DEBUG \t batch ate = 0.0533064\n",
      "DEBUG \t batch ate = 0.0473143\n",
      "DEBUG \t batch ate = 0.0501968\n",
      "DEBUG \t batch ate = 0.0539183\n",
      "DEBUG \t batch ate = 0.0378486\n",
      "DEBUG \t batch ate = 0.0495472\n",
      "DEBUG \t batch ate = 0.0742214\n",
      "DEBUG \t batch ate = 0.0517983\n",
      "DEBUG \t batch ate = 0.0589909\n",
      "DEBUG \t batch ate = 0.0558278\n",
      "DEBUG \t batch ate = 0.0492988\n",
      "DEBUG \t batch ate = 0.0693874\n",
      "DEBUG \t batch ate = 0.0512075\n",
      "DEBUG \t batch ate = 0.0704914\n",
      "DEBUG \t batch ate = 0.0520064\n",
      "DEBUG \t batch ate = 0.0530496\n",
      "DEBUG \t batch ate = 0.0548463\n",
      "DEBUG \t batch ate = 0.0574201\n",
      "DEBUG \t batch ate = 0.0634038\n",
      "DEBUG \t batch ate = 0.0548455\n",
      "DEBUG \t batch ate = 0.0652861\n",
      "DEBUG \t batch ate = 0.0571238\n",
      "DEBUG \t batch ate = 0.0705383\n",
      "DEBUG \t batch ate = 0.0586121\n",
      "DEBUG \t batch ate = 0.0421496\n",
      "DEBUG \t batch ate = 0.0536769\n",
      "DEBUG \t batch ate = 0.0525882\n",
      "DEBUG \t batch ate = 0.0411\n",
      "DEBUG \t batch ate = 0.0544738\n",
      "DEBUG \t batch ate = 0.0581245\n",
      "DEBUG \t batch ate = 0.0471535\n",
      "DEBUG \t batch ate = 0.054539\n",
      "DEBUG \t batch ate = 0.0468102\n",
      "DEBUG \t batch ate = 0.0531785\n",
      "DEBUG \t batch ate = 0.0517251\n",
      "DEBUG \t batch ate = 0.0437217\n",
      "DEBUG \t batch ate = 0.0547493\n",
      "DEBUG \t batch ate = 0.0549912\n",
      "DEBUG \t batch ate = 0.0464519\n",
      "DEBUG \t batch ate = 0.0517586\n",
      "DEBUG \t batch ate = 0.0636555\n",
      "DEBUG \t batch ate = 0.0466093\n",
      "DEBUG \t batch ate = 0.045027\n",
      "DEBUG \t batch ate = 0.0603885\n",
      "DEBUG \t batch ate = 0.0495583\n",
      "DEBUG \t batch ate = 0.0578512\n",
      "DEBUG \t batch ate = 0.0363526\n",
      "DEBUG \t batch ate = 0.0483916\n",
      "DEBUG \t batch ate = 0.0421153\n",
      "DEBUG \t batch ate = 0.0447801\n",
      "DEBUG \t batch ate = 0.0545886\n",
      "DEBUG \t batch ate = 0.0544923\n",
      "DEBUG \t batch ate = 0.052604\n",
      "DEBUG \t batch ate = 0.0529543\n",
      "DEBUG \t batch ate = 0.048921\n",
      "DEBUG \t batch ate = 0.0523217\n",
      "DEBUG \t batch ate = 0.067881\n",
      "DEBUG \t batch ate = 0.0599045\n",
      "DEBUG \t batch ate = 0.0606494\n",
      "DEBUG \t batch ate = 0.0562823\n",
      "DEBUG \t batch ate = 0.0536914\n",
      "DEBUG \t batch ate = 0.0598211\n",
      "DEBUG \t batch ate = 0.062835\n",
      "DEBUG \t batch ate = 0.0504123\n",
      "DEBUG \t batch ate = 0.0619969\n",
      "DEBUG \t batch ate = 0.0570778\n",
      "DEBUG \t batch ate = 0.0450348\n",
      "DEBUG \t batch ate = 0.0552306\n",
      "DEBUG \t batch ate = 0.0449434\n",
      "DEBUG \t batch ate = 0.0536396\n",
      "DEBUG \t batch ate = 0.0769907\n",
      "DEBUG \t batch ate = 0.0577021\n",
      "DEBUG \t batch ate = 0.0491863\n",
      "DEBUG \t batch ate = 0.049342\n",
      "DEBUG \t batch ate = 0.0510848\n",
      "DEBUG \t batch ate = 0.0497722\n",
      "DEBUG \t batch ate = 0.0553406\n",
      "DEBUG \t batch ate = 0.0638332\n",
      "DEBUG \t batch ate = 0.0523474\n",
      "DEBUG \t batch ate = 0.0598417\n",
      "DEBUG \t batch ate = 0.0514586\n",
      "DEBUG \t batch ate = 0.0498175\n",
      "DEBUG \t batch ate = 0.0531126\n",
      "DEBUG \t batch ate = 0.0515563\n",
      "DEBUG \t batch ate = 0.0413784\n",
      "DEBUG \t batch ate = 0.0535386\n",
      "DEBUG \t batch ate = 0.0447867\n",
      "DEBUG \t batch ate = 0.0596642\n",
      "DEBUG \t batch ate = 0.0561639\n",
      "DEBUG \t batch ate = 0.0632101\n",
      "DEBUG \t batch ate = 0.05204\n",
      "DEBUG \t batch ate = 0.0521406\n",
      "DEBUG \t batch ate = 0.0546912\n",
      "DEBUG \t batch ate = 0.0526201\n",
      "DEBUG \t batch ate = 0.0494926\n",
      "DEBUG \t batch ate = 0.0554522\n",
      "DEBUG \t batch ate = 0.0413058\n",
      "DEBUG \t batch ate = 0.0536888\n",
      "DEBUG \t batch ate = 0.0657449\n",
      "DEBUG \t batch ate = 0.0728599\n",
      "DEBUG \t batch ate = 0.0532699\n",
      "DEBUG \t batch ate = 0.0370746\n",
      "DEBUG \t batch ate = 0.0636749\n",
      "DEBUG \t batch ate = 0.0569477\n",
      "DEBUG \t batch ate = 0.0650372\n",
      "DEBUG \t batch ate = 0.0545739\n",
      "DEBUG \t batch ate = 0.0477955\n",
      "DEBUG \t batch ate = 0.0494464\n",
      "DEBUG \t batch ate = 0.0587193\n",
      "DEBUG \t batch ate = 0.0417343\n",
      "DEBUG \t batch ate = 0.0655559\n",
      "DEBUG \t batch ate = 0.0611874\n",
      "DEBUG \t batch ate = 0.058344\n",
      "DEBUG \t batch ate = 0.0443422\n",
      "DEBUG \t batch ate = 0.03868\n",
      "DEBUG \t batch ate = 0.0519187\n",
      "DEBUG \t batch ate = 0.0445825\n",
      "DEBUG \t batch ate = 0.05214\n",
      "DEBUG \t batch ate = 0.0576882\n",
      "DEBUG \t batch ate = 0.0494173\n",
      "DEBUG \t batch ate = 0.0540997\n",
      "DEBUG \t batch ate = 0.0574708\n",
      "DEBUG \t batch ate = 0.0528886\n",
      "DEBUG \t batch ate = 0.0512291\n",
      "DEBUG \t batch ate = 0.0555106\n",
      "DEBUG \t batch ate = 0.0552294\n",
      "DEBUG \t batch ate = 0.0493391\n",
      "DEBUG \t batch ate = 0.0601101\n",
      "DEBUG \t batch ate = 0.0461178\n",
      "DEBUG \t batch ate = 0.0547269\n",
      "DEBUG \t batch ate = 0.0476658\n",
      "DEBUG \t batch ate = 0.0479142\n",
      "DEBUG \t batch ate = 0.0402204\n",
      "DEBUG \t batch ate = 0.0645129\n",
      "DEBUG \t batch ate = 0.057889\n",
      "DEBUG \t batch ate = 0.0534408\n",
      "DEBUG \t batch ate = 0.0535949\n",
      "DEBUG \t batch ate = 0.0581709\n",
      "DEBUG \t batch ate = 0.0495175\n",
      "DEBUG \t batch ate = 0.0553681\n",
      "DEBUG \t batch ate = 0.0563044\n",
      "DEBUG \t batch ate = 0.0527063\n",
      "DEBUG \t batch ate = 0.0589348\n",
      "DEBUG \t batch ate = 0.0444472\n",
      "DEBUG \t batch ate = 0.0469072\n",
      "DEBUG \t batch ate = 0.0504281\n",
      "DEBUG \t batch ate = 0.0625876\n",
      "DEBUG \t batch ate = 0.0527776\n",
      "DEBUG \t batch ate = 0.0573697\n",
      "DEBUG \t batch ate = 0.0658205\n",
      "DEBUG \t batch ate = 0.0514795\n",
      "DEBUG \t batch ate = 0.0623456\n",
      "DEBUG \t batch ate = 0.0468782\n",
      "DEBUG \t batch ate = 0.0592252\n",
      "DEBUG \t batch ate = 0.0510392\n",
      "DEBUG \t batch ate = 0.0548495\n",
      "DEBUG \t batch ate = 0.050361\n",
      "DEBUG \t batch ate = 0.0587662\n",
      "DEBUG \t batch ate = 0.0642734\n",
      "DEBUG \t batch ate = 0.0493923\n",
      "DEBUG \t batch ate = 0.0543344\n",
      "DEBUG \t batch ate = 0.0545\n",
      "DEBUG \t batch ate = 0.0590089\n",
      "DEBUG \t batch ate = 0.061046\n",
      "DEBUG \t batch ate = 0.0697025\n",
      "DEBUG \t batch ate = 0.0545021\n",
      "DEBUG \t batch ate = 0.0422229\n",
      "DEBUG \t batch ate = 0.0521492\n",
      "DEBUG \t batch ate = 0.0505633\n",
      "DEBUG \t batch ate = 0.041832\n",
      "DEBUG \t batch ate = 0.063688\n",
      "DEBUG \t batch ate = 0.0604054\n",
      "DEBUG \t batch ate = 0.0582685\n",
      "DEBUG \t batch ate = 0.0475527\n",
      "DEBUG \t batch ate = 0.0654292\n",
      "DEBUG \t batch ate = 0.057239\n",
      "DEBUG \t batch ate = 0.0592518\n",
      "DEBUG \t batch ate = 0.0689669\n",
      "DEBUG \t batch ate = 0.0552568\n",
      "DEBUG \t batch ate = 0.0572989\n",
      "DEBUG \t batch ate = 0.0623843\n",
      "DEBUG \t batch ate = 0.0742578\n",
      "DEBUG \t batch ate = 0.055201\n",
      "DEBUG \t batch ate = 0.0518562\n",
      "DEBUG \t batch ate = 0.0569231\n",
      "DEBUG \t batch ate = 0.0494953\n",
      "DEBUG \t batch ate = 0.0560866\n",
      "DEBUG \t batch ate = 0.0523991\n",
      "DEBUG \t batch ate = 0.0583635\n",
      "DEBUG \t batch ate = 0.0512542\n",
      "DEBUG \t batch ate = 0.0575123\n",
      "DEBUG \t batch ate = 0.0463538\n",
      "DEBUG \t batch ate = 0.0521533\n",
      "DEBUG \t batch ate = 0.0542322\n",
      "DEBUG \t batch ate = 0.0657102\n",
      "DEBUG \t batch ate = 0.0556142\n",
      "DEBUG \t batch ate = 0.0654474\n",
      "DEBUG \t batch ate = 0.048088\n",
      "DEBUG \t batch ate = 0.0629771\n",
      "DEBUG \t batch ate = 0.0574923\n",
      "DEBUG \t batch ate = 0.0473084\n",
      "DEBUG \t batch ate = 0.0564725\n",
      "DEBUG \t batch ate = 0.0377616\n",
      "DEBUG \t batch ate = 0.0378671\n",
      "DEBUG \t batch ate = 0.0536519\n",
      "DEBUG \t batch ate = 0.0629404\n",
      "DEBUG \t batch ate = 0.0542121\n",
      "DEBUG \t batch ate = 0.0665602\n",
      "DEBUG \t batch ate = 0.0590765\n",
      "DEBUG \t batch ate = 0.0703287\n",
      "DEBUG \t batch ate = 0.0583302\n",
      "DEBUG \t batch ate = 0.04684\n",
      "DEBUG \t batch ate = 0.0708582\n",
      "DEBUG \t batch ate = 0.049981\n",
      "DEBUG \t batch ate = 0.0496204\n",
      "DEBUG \t batch ate = 0.0473248\n",
      "DEBUG \t batch ate = 0.0643536\n",
      "DEBUG \t batch ate = 0.0404994\n",
      "DEBUG \t batch ate = 0.0531822\n",
      "DEBUG \t batch ate = 0.0620331\n",
      "DEBUG \t batch ate = 0.0553895\n",
      "DEBUG \t batch ate = 0.0536986\n",
      "DEBUG \t batch ate = 0.0592444\n",
      "DEBUG \t batch ate = 0.0490208\n",
      "DEBUG \t batch ate = 0.0465722\n",
      "DEBUG \t batch ate = 0.0660862\n",
      "DEBUG \t batch ate = 0.0519867\n",
      "DEBUG \t batch ate = 0.0371816\n",
      "DEBUG \t batch ate = 0.0543225\n",
      "DEBUG \t batch ate = 0.0432716\n",
      "DEBUG \t batch ate = 0.0530057\n",
      "DEBUG \t batch ate = 0.0555359\n",
      "DEBUG \t batch ate = 0.0432524\n",
      "DEBUG \t batch ate = 0.0479899\n",
      "DEBUG \t batch ate = 0.0496629\n",
      "DEBUG \t batch ate = 0.0564167\n",
      "DEBUG \t batch ate = 0.0477474\n",
      "DEBUG \t batch ate = 0.0579948\n",
      "DEBUG \t batch ate = 0.0570713\n",
      "DEBUG \t batch ate = 0.0493457\n",
      "DEBUG \t batch ate = 0.0561836\n",
      "DEBUG \t batch ate = 0.0410741\n",
      "DEBUG \t batch ate = 0.0447732\n",
      "DEBUG \t batch ate = 0.0617379\n",
      "DEBUG \t batch ate = 0.0654137\n",
      "DEBUG \t batch ate = 0.0512379\n",
      "DEBUG \t batch ate = 0.0530303\n",
      "DEBUG \t batch ate = 0.0450549\n",
      "DEBUG \t batch ate = 0.0577246\n",
      "DEBUG \t batch ate = 0.0501268\n",
      "DEBUG \t batch ate = 0.0472105\n",
      "DEBUG \t batch ate = 0.0526673\n",
      "DEBUG \t batch ate = 0.0561298\n",
      "DEBUG \t batch ate = 0.0541456\n",
      "DEBUG \t batch ate = 0.050819\n",
      "DEBUG \t batch ate = 0.0615028\n",
      "DEBUG \t batch ate = 0.0542476\n",
      "DEBUG \t batch ate = 0.0572598\n",
      "DEBUG \t batch ate = 0.0410495\n",
      "DEBUG \t batch ate = 0.0518758\n",
      "DEBUG \t batch ate = 0.0504331\n",
      "DEBUG \t batch ate = 0.0588503\n",
      "DEBUG \t batch ate = 0.063848\n",
      "DEBUG \t batch ate = 0.0587028\n",
      "DEBUG \t batch ate = 0.0597486\n",
      "DEBUG \t batch ate = 0.0575413\n",
      "DEBUG \t batch ate = 0.0603636\n",
      "DEBUG \t batch ate = 0.0527438\n",
      "DEBUG \t batch ate = 0.048032\n",
      "DEBUG \t batch ate = 0.0572063\n",
      "DEBUG \t batch ate = 0.0646127\n",
      "DEBUG \t batch ate = 0.0735098\n",
      "DEBUG \t batch ate = 0.0440048\n",
      "DEBUG \t batch ate = 0.0608491\n",
      "DEBUG \t batch ate = 0.0561344\n",
      "DEBUG \t batch ate = 0.0509322\n",
      "DEBUG \t batch ate = 0.0601517\n",
      "DEBUG \t batch ate = 0.0503972\n",
      "DEBUG \t batch ate = 0.0509595\n",
      "DEBUG \t batch ate = 0.0577711\n",
      "DEBUG \t batch ate = 0.0389246\n",
      "DEBUG \t batch ate = 0.0638308\n",
      "DEBUG \t batch ate = 0.0617998\n",
      "DEBUG \t batch ate = 0.0698487\n",
      "DEBUG \t batch ate = 0.0530186\n",
      "DEBUG \t batch ate = 0.0584437\n",
      "DEBUG \t batch ate = 0.0581378\n",
      "DEBUG \t batch ate = 0.0709758\n",
      "DEBUG \t batch ate = 0.0610389\n",
      "DEBUG \t batch ate = 0.0679302\n",
      "DEBUG \t batch ate = 0.062412\n",
      "DEBUG \t batch ate = 0.0477562\n",
      "DEBUG \t batch ate = 0.0566838\n",
      "DEBUG \t batch ate = 0.0563836\n",
      "DEBUG \t batch ate = 0.04188\n",
      "DEBUG \t batch ate = 0.0599236\n",
      "DEBUG \t batch ate = 0.0589894\n",
      "DEBUG \t batch ate = 0.0469253\n",
      "DEBUG \t batch ate = 0.0538142\n",
      "DEBUG \t batch ate = 0.070069\n",
      "DEBUG \t batch ate = 0.0567308\n",
      "DEBUG \t batch ate = 0.0578566\n",
      "DEBUG \t batch ate = 0.0576937\n",
      "DEBUG \t batch ate = 0.0625019\n",
      "DEBUG \t batch ate = 0.049199\n",
      "DEBUG \t batch ate = 0.0551919\n",
      "DEBUG \t batch ate = 0.0549369\n",
      "DEBUG \t batch ate = 0.0486774\n",
      "DEBUG \t batch ate = 0.0436085\n",
      "DEBUG \t batch ate = 0.0710034\n",
      "DEBUG \t batch ate = 0.0462388\n",
      "DEBUG \t batch ate = 0.0450731\n",
      "DEBUG \t batch ate = 0.0542855\n",
      "DEBUG \t batch ate = 0.0514444\n",
      "DEBUG \t batch ate = 0.0609809\n",
      "DEBUG \t batch ate = 0.0501181\n",
      "DEBUG \t batch ate = 0.0534814\n",
      "DEBUG \t batch ate = 0.0469768\n",
      "DEBUG \t batch ate = 0.0415657\n",
      "DEBUG \t batch ate = 0.0561451\n",
      "DEBUG \t batch ate = 0.0466675\n",
      "DEBUG \t batch ate = 0.0562038\n",
      "DEBUG \t batch ate = 0.0473745\n",
      "DEBUG \t batch ate = 0.0605701\n",
      "DEBUG \t batch ate = 0.0608997\n",
      "DEBUG \t batch ate = 0.0523475\n",
      "DEBUG \t batch ate = 0.0527185\n",
      "DEBUG \t batch ate = 0.0500763\n",
      "DEBUG \t batch ate = 0.0458774\n",
      "DEBUG \t batch ate = 0.0557623\n",
      "DEBUG \t batch ate = 0.0653113\n",
      "DEBUG \t batch ate = 0.0510033\n",
      "DEBUG \t batch ate = 0.0579511\n",
      "DEBUG \t batch ate = 0.0505511\n",
      "DEBUG \t batch ate = 0.0673177\n",
      "DEBUG \t batch ate = 0.050968\n",
      "DEBUG \t batch ate = 0.0557449\n",
      "DEBUG \t batch ate = 0.0542891\n",
      "DEBUG \t batch ate = 0.0543092\n",
      "DEBUG \t batch ate = 0.0524995\n",
      "DEBUG \t batch ate = 0.0794402\n",
      "DEBUG \t batch ate = 0.0518422\n",
      "DEBUG \t batch ate = 0.0599698\n",
      "DEBUG \t batch ate = 0.0652581\n",
      "DEBUG \t batch ate = 0.0631784\n",
      "DEBUG \t batch ate = 0.0495217\n",
      "DEBUG \t batch ate = 0.054902\n",
      "DEBUG \t batch ate = 0.0573814\n",
      "DEBUG \t batch ate = 0.0512057\n",
      "DEBUG \t batch ate = 0.0604385\n",
      "DEBUG \t batch ate = 0.0590666\n",
      "DEBUG \t batch ate = 0.0577438\n",
      "DEBUG \t batch ate = 0.0632384\n",
      "DEBUG \t batch ate = 0.0571669\n",
      "DEBUG \t batch ate = 0.057734\n",
      "DEBUG \t batch ate = 0.0608875\n",
      "DEBUG \t batch ate = 0.0587494\n",
      "DEBUG \t batch ate = 0.0598414\n",
      "DEBUG \t batch ate = 0.0458899\n",
      "DEBUG \t batch ate = 0.0513314\n",
      "DEBUG \t batch ate = 0.0642342\n",
      "DEBUG \t batch ate = 0.0600042\n",
      "DEBUG \t batch ate = 0.0597861\n",
      "DEBUG \t batch ate = 0.0477434\n",
      "DEBUG \t batch ate = 0.0396463\n",
      "DEBUG \t batch ate = 0.0599295\n",
      "DEBUG \t batch ate = 0.0495606\n",
      "DEBUG \t batch ate = 0.0540197\n",
      "DEBUG \t batch ate = 0.061542\n",
      "DEBUG \t batch ate = 0.055165\n",
      "DEBUG \t batch ate = 0.0575543\n",
      "DEBUG \t batch ate = 0.0575635\n",
      "DEBUG \t batch ate = 0.0606497\n",
      "DEBUG \t batch ate = 0.0545117\n",
      "DEBUG \t batch ate = 0.0582406\n",
      "DEBUG \t batch ate = 0.0437622\n",
      "DEBUG \t batch ate = 0.0508448\n",
      "DEBUG \t batch ate = 0.0538492\n",
      "DEBUG \t batch ate = 0.0541879\n",
      "DEBUG \t batch ate = 0.0550001\n",
      "DEBUG \t batch ate = 0.0708498\n",
      "DEBUG \t batch ate = 0.0580338\n",
      "DEBUG \t batch ate = 0.0595888\n",
      "DEBUG \t batch ate = 0.0613424\n",
      "DEBUG \t batch ate = 0.0550774\n",
      "DEBUG \t batch ate = 0.0562204\n",
      "DEBUG \t batch ate = 0.0590875\n",
      "DEBUG \t batch ate = 0.0394563\n",
      "DEBUG \t batch ate = 0.0605947\n",
      "DEBUG \t batch ate = 0.0459189\n",
      "DEBUG \t batch ate = 0.062801\n",
      "DEBUG \t batch ate = 0.047663\n",
      "DEBUG \t batch ate = 0.0555556\n",
      "DEBUG \t batch ate = 0.0625539\n",
      "DEBUG \t batch ate = 0.0373566\n",
      "DEBUG \t batch ate = 0.0302579\n",
      "DEBUG \t batch ate = 0.0514491\n",
      "DEBUG \t batch ate = 0.0604817\n",
      "DEBUG \t batch ate = 0.0593758\n",
      "DEBUG \t batch ate = 0.0525164\n",
      "DEBUG \t batch ate = 0.0693317\n",
      "DEBUG \t batch ate = 0.0588007\n",
      "DEBUG \t batch ate = 0.0459445\n",
      "DEBUG \t batch ate = 0.0622145\n",
      "DEBUG \t batch ate = 0.0536708\n",
      "DEBUG \t batch ate = 0.0607419\n",
      "DEBUG \t batch ate = 0.0688611\n",
      "DEBUG \t batch ate = 0.0460089\n",
      "DEBUG \t batch ate = 0.049105\n",
      "DEBUG \t batch ate = 0.0613591\n",
      "DEBUG \t batch ate = 0.0532492\n",
      "DEBUG \t batch ate = 0.0493136\n",
      "DEBUG \t batch ate = 0.0437968\n",
      "DEBUG \t batch ate = 0.0609682\n",
      "DEBUG \t batch ate = 0.0545342\n",
      "DEBUG \t batch ate = 0.0566463\n",
      "DEBUG \t batch ate = 0.0577901\n",
      "DEBUG \t batch ate = 0.0426288\n",
      "DEBUG \t batch ate = 0.0305983\n",
      "DEBUG \t batch ate = 0.0534588\n",
      "DEBUG \t batch ate = 0.0658817\n",
      "DEBUG \t batch ate = 0.0562871\n",
      "DEBUG \t batch ate = 0.0683084\n",
      "DEBUG \t batch ate = 0.0463744\n",
      "DEBUG \t batch ate = 0.0585405\n",
      "DEBUG \t batch ate = 0.0531698\n",
      "DEBUG \t batch ate = 0.0604222\n",
      "DEBUG \t batch ate = 0.0544659\n",
      "DEBUG \t batch ate = 0.0473546\n",
      "DEBUG \t batch ate = 0.0510754\n",
      "DEBUG \t batch ate = 0.0588085\n",
      "DEBUG \t batch ate = 0.0602161\n",
      "DEBUG \t batch ate = 0.0590216\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "ite_train = cevae.predict(X_train)\n",
    "ite_val = cevae.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc0df9-ab3c-439e-aa59-03d4a007e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_train = ite_train.mean()\n",
    "ate_val = ite_val.mean()\n",
    "print(ate_train, ate_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-causalml",
   "language": "python",
   "name": "nn-causalml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

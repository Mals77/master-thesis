{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2677f95e-9002-4778-b600-ade456820209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 16:57:13.657205: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-22 16:57:14.888481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-22 16:57:14.888517: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-22 16:57:14.889871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-22 16:57:15.240203: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 16:57:39.820508: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "\n",
    "from causalml.inference.meta import LRSRegressor\n",
    "from causalml.inference.meta import XGBTRegressor, MLPTRegressor\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor\n",
    "from causalml.inference.tf import DragonNet\n",
    "from causalml.match import NearestNeighborMatch, MatchOptimizer, create_table_one\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.metrics import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "from causalml.inference.nn import CEVAE\n",
    "\n",
    "import os, sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Paired')\n",
    "plt.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161a78d9-b745-4eac-9ff6-10270dd30782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['case:concept:name', 'NumberOfOffers', 'Action', 'org:resource',\n",
      "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
      "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
      "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
      "       'Selected', 'CreditScore', 'OfferedAmount', 'treatedCase',\n",
      "       'caseSuccesful', 'treatmentSuccess', 'offerNumber', 'offerSuccess',\n",
      "       'treatmentOffer', 'timeApplication', 'weekdayApplication'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>NumberOfOffers</th>\n",
       "      <th>Action</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>EventOrigin</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>case:LoanGoal</th>\n",
       "      <th>case:ApplicationType</th>\n",
       "      <th>...</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>OfferedAmount</th>\n",
       "      <th>treatedCase</th>\n",
       "      <th>caseSuccesful</th>\n",
       "      <th>treatmentSuccess</th>\n",
       "      <th>offerNumber</th>\n",
       "      <th>offerSuccess</th>\n",
       "      <th>treatmentOffer</th>\n",
       "      <th>timeApplication</th>\n",
       "      <th>weekdayApplication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>651433.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>651434.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>651435.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>651437.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.613</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>651438.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.620</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case:concept:name  NumberOfOffers  Action  org:resource  concept:name  \\\n",
       "0                0.0             1.0     0.0           0.0           4.0   \n",
       "1                0.0             1.0     4.0           0.0           8.0   \n",
       "2                0.0             1.0     0.0           0.0          22.0   \n",
       "3                0.0             1.0     1.0           0.0          22.0   \n",
       "4                0.0             1.0     0.0           0.0          21.0   \n",
       "\n",
       "   EventOrigin  lifecycle:transition  time:timestamp  case:LoanGoal  \\\n",
       "0          0.0                   1.0        651433.0           10.0   \n",
       "1          0.0                   1.0        651434.0           10.0   \n",
       "2          2.0                   3.0        651435.0           10.0   \n",
       "3          2.0                   6.0        651437.0           10.0   \n",
       "4          2.0                   3.0        651438.0           10.0   \n",
       "\n",
       "   case:ApplicationType  ...  CreditScore  OfferedAmount  treatedCase  \\\n",
       "0                   1.0  ...          0.0         5000.0          0.0   \n",
       "1                   1.0  ...          0.0         5000.0          0.0   \n",
       "2                   1.0  ...          0.0         5000.0          0.0   \n",
       "3                   1.0  ...          0.0         5000.0          0.0   \n",
       "4                   1.0  ...          0.0         5000.0          0.0   \n",
       "\n",
       "   caseSuccesful  treatmentSuccess  offerNumber  offerSuccess  treatmentOffer  \\\n",
       "0            0.0               0.0          1.0           0.0             0.0   \n",
       "1            0.0               0.0          1.0           0.0             0.0   \n",
       "2            0.0               0.0          1.0           0.0             0.0   \n",
       "3            0.0               0.0          1.0           0.0             0.0   \n",
       "4            0.0               0.0          1.0           0.0             0.0   \n",
       "\n",
       "   timeApplication  weekdayApplication  \n",
       "0            0.000                 2.0  \n",
       "1            0.061                 2.0  \n",
       "2            0.290                 2.0  \n",
       "3           66.613                 2.0  \n",
       "4           66.620                 2.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bpi2017_final.csv\")\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90aba3a4-965d-496f-babb-ef916ff93d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['NumberOfOffers', 'Action', 'org:resource',\n",
    "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
    "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
    "       'CreditScore', 'OfferedAmount', 'offerNumber','timeApplication', 'weekdayApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa096af-76bc-49f7-8412-5d65ea3feae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_outcome = df['offerSuccess'].values\n",
    "x_feature = df[feature_names].values\n",
    "t_treatment = np.array([np.array([value]) for value in df['treatmentOffer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f1150b-d1c2-4ec7-8c11-841b172c8f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14979/14979 [==============================] - 40s 3ms/step - loss: 6485313.0000 - regression_loss: 3242603.0000 - binary_classification_loss: 70.0225 - treatment_accuracy: 0.8418 - track_epsilon: 0.0102 - val_loss: 367.5421 - val_regression_loss: 149.4940 - val_binary_classification_loss: 64.3313 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0011 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 1485.4623 - regression_loss: 718.2080 - binary_classification_loss: 44.2135 - treatment_accuracy: 0.8513 - track_epsilon: 0.0105 - val_loss: 63.8605 - val_regression_loss: 15.9450 - val_binary_classification_loss: 27.7129 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0072 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 61.8982 - regression_loss: 16.1280 - binary_classification_loss: 27.0140 - treatment_accuracy: 0.8515 - track_epsilon: 9.2154e-04 - val_loss: 59.2252 - val_regression_loss: 15.8645 - val_binary_classification_loss: 26.5415 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 1.6129e-04 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 69725.3594 - regression_loss: 34822.7344 - binary_classification_loss: 27.0419 - treatment_accuracy: 0.8514 - track_epsilon: 0.0095 - val_loss: 62.2746 - val_regression_loss: 15.8756 - val_binary_classification_loss: 27.3184 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0011 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 60.1775 - regression_loss: 15.9504 - binary_classification_loss: 27.0078 - treatment_accuracy: 0.8515 - track_epsilon: 8.2330e-04 - val_loss: 58.6811 - val_regression_loss: 15.8314 - val_binary_classification_loss: 26.6036 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 3.9573e-04 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 59.2484 - regression_loss: 16.0296 - binary_classification_loss: 27.0183 - treatment_accuracy: 0.8515 - track_epsilon: 9.8952e-04 - val_loss: 58.8071 - val_regression_loss: 16.0862 - val_binary_classification_loss: 26.5408 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 2.7828e-04 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 59.1582 - regression_loss: 16.0251 - binary_classification_loss: 27.0089 - treatment_accuracy: 0.8515 - track_epsilon: 9.3579e-04 - val_loss: 58.3745 - val_regression_loss: 15.8286 - val_binary_classification_loss: 26.6340 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0012 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 59.1108 - regression_loss: 16.0078 - binary_classification_loss: 27.0158 - treatment_accuracy: 0.8515 - track_epsilon: 0.0011 - val_loss: 58.7025 - val_regression_loss: 16.0427 - val_binary_classification_loss: 26.5316 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0022 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 59.0514 - regression_loss: 15.9857 - binary_classification_loss: 27.0127 - treatment_accuracy: 0.8515 - track_epsilon: 0.0010 - val_loss: 58.6882 - val_regression_loss: 15.9806 - val_binary_classification_loss: 26.6253 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0036 - lr: 0.0010\n",
      "Epoch 1/100\n",
      "14979/14979 [==============================] - 38s 2ms/step - loss: 58.8821 - regression_loss: 15.9511 - binary_classification_loss: 26.9197 - treatment_accuracy: 0.8515 - track_epsilon: 0.0024 - val_loss: 58.3988 - val_regression_loss: 15.8759 - val_binary_classification_loss: 26.5958 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0039 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.8823 - regression_loss: 15.9523 - binary_classification_loss: 26.9202 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.2509 - val_regression_loss: 15.8257 - val_binary_classification_loss: 26.5598 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 2.6053e-05 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.8770 - regression_loss: 15.9497 - binary_classification_loss: 26.9233 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.3540 - val_regression_loss: 15.8887 - val_binary_classification_loss: 26.5314 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0037 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.8717 - regression_loss: 15.9496 - binary_classification_loss: 26.9206 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.3933 - val_regression_loss: 15.8839 - val_binary_classification_loss: 26.5592 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0031 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.8645 - regression_loss: 15.9470 - binary_classification_loss: 26.9215 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.4352 - val_regression_loss: 15.9082 - val_binary_classification_loss: 26.5915 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.8581 - regression_loss: 15.9451 - binary_classification_loss: 26.9212 - treatment_accuracy: 0.8515 - track_epsilon: 0.0026 - val_loss: 58.5921 - val_regression_loss: 15.9228 - val_binary_classification_loss: 26.7083 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0022 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.8661 - regression_loss: 15.9495 - binary_classification_loss: 26.9221 - treatment_accuracy: 0.8515 - track_epsilon: 0.0024 - val_loss: 58.5513 - val_regression_loss: 15.9920 - val_binary_classification_loss: 26.5380 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0038 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.8551 - regression_loss: 15.9449 - binary_classification_loss: 26.9228 - treatment_accuracy: 0.8515 - track_epsilon: 0.0026 - val_loss: 58.3976 - val_regression_loss: 15.8430 - val_binary_classification_loss: 26.6949 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0030 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.8503 - regression_loss: 15.9445 - binary_classification_loss: 26.9209 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.2549 - val_regression_loss: 15.8321 - val_binary_classification_loss: 26.5680 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 5.3707e-04 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.8503 - regression_loss: 15.9449 - binary_classification_loss: 26.9221 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.4392 - val_regression_loss: 15.8993 - val_binary_classification_loss: 26.5343 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0075 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.8420 - regression_loss: 15.9418 - binary_classification_loss: 26.9227 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.3995 - val_regression_loss: 15.8884 - val_binary_classification_loss: 26.5722 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0068 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.8235 - regression_loss: 15.9341 - binary_classification_loss: 26.9209 - treatment_accuracy: 0.8515 - track_epsilon: 0.0026 - val_loss: 58.5878 - val_regression_loss: 16.0238 - val_binary_classification_loss: 26.5321 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.8183 - regression_loss: 15.9322 - binary_classification_loss: 26.9213 - treatment_accuracy: 0.8515 - track_epsilon: 0.0026 - val_loss: 58.5698 - val_regression_loss: 16.0178 - val_binary_classification_loss: 26.5355 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.8068 - regression_loss: 15.9266 - binary_classification_loss: 26.9230 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.5653 - val_regression_loss: 16.0104 - val_binary_classification_loss: 26.5324 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0034 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 58.8111 - regression_loss: 15.9301 - binary_classification_loss: 26.9202 - treatment_accuracy: 0.8515 - track_epsilon: 0.0026 - val_loss: 58.4139 - val_regression_loss: 15.8945 - val_binary_classification_loss: 26.5947 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0045 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.8085 - regression_loss: 15.9275 - binary_classification_loss: 26.9235 - treatment_accuracy: 0.8515 - track_epsilon: 0.0026 - val_loss: 58.3117 - val_regression_loss: 15.8296 - val_binary_classification_loss: 26.6159 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0039 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.7968 - regression_loss: 15.9216 - binary_classification_loss: 26.9244 - treatment_accuracy: 0.8515 - track_epsilon: 0.0027 - val_loss: 58.2941 - val_regression_loss: 15.8497 - val_binary_classification_loss: 26.5392 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0052 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.7742 - regression_loss: 15.9131 - binary_classification_loss: 26.9211 - treatment_accuracy: 0.8515 - track_epsilon: 0.0025 - val_loss: 58.3414 - val_regression_loss: 15.8835 - val_binary_classification_loss: 26.5352 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0031 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.7669 - regression_loss: 15.9082 - binary_classification_loss: 26.9233 - treatment_accuracy: 0.8515 - track_epsilon: 0.0027 - val_loss: 58.4814 - val_regression_loss: 15.9638 - val_binary_classification_loss: 26.5349 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.7525 - regression_loss: 15.9017 - binary_classification_loss: 26.9220 - treatment_accuracy: 0.8515 - track_epsilon: 0.0026 - val_loss: 58.2486 - val_regression_loss: 15.8335 - val_binary_classification_loss: 26.5712 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0022 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.7359 - regression_loss: 15.8926 - binary_classification_loss: 26.9237 - treatment_accuracy: 0.8515 - track_epsilon: 0.0027 - val_loss: 58.3818 - val_regression_loss: 15.9040 - val_binary_classification_loss: 26.5561 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 9.2894e-04 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.7144 - regression_loss: 15.8824 - binary_classification_loss: 26.9215 - treatment_accuracy: 0.8515 - track_epsilon: 0.0027 - val_loss: 58.2339 - val_regression_loss: 15.8444 - val_binary_classification_loss: 26.5338 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 7.0598e-04 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.6927 - regression_loss: 15.8706 - binary_classification_loss: 26.9240 - treatment_accuracy: 0.8515 - track_epsilon: 0.0027 - val_loss: 58.2968 - val_regression_loss: 15.8436 - val_binary_classification_loss: 26.5973 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 8.3095e-04 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.6779 - regression_loss: 15.8641 - binary_classification_loss: 26.9215 - treatment_accuracy: 0.8515 - track_epsilon: 0.0028 - val_loss: 58.3977 - val_regression_loss: 15.8353 - val_binary_classification_loss: 26.6972 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0027 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 58.6739 - regression_loss: 15.8611 - binary_classification_loss: 26.9238 - treatment_accuracy: 0.8515 - track_epsilon: 0.0028 - val_loss: 58.2639 - val_regression_loss: 15.8398 - val_binary_classification_loss: 26.5530 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0038 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.6656 - regression_loss: 15.8577 - binary_classification_loss: 26.9231 - treatment_accuracy: 0.8515 - track_epsilon: 0.0029 - val_loss: 58.3086 - val_regression_loss: 15.8678 - val_binary_classification_loss: 26.5392 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0037 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.6568 - regression_loss: 15.8549 - binary_classification_loss: 26.9209 - treatment_accuracy: 0.8515 - track_epsilon: 0.0028 - val_loss: 58.2813 - val_regression_loss: 15.8515 - val_binary_classification_loss: 26.5562 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0047 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.6634 - regression_loss: 15.8577 - binary_classification_loss: 26.9214 - treatment_accuracy: 0.8515 - track_epsilon: 0.0028 - val_loss: 58.2773 - val_regression_loss: 15.8620 - val_binary_classification_loss: 26.5372 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.6551 - regression_loss: 15.8549 - binary_classification_loss: 26.9192 - treatment_accuracy: 0.8515 - track_epsilon: 0.0029 - val_loss: 58.4254 - val_regression_loss: 15.8312 - val_binary_classification_loss: 26.7518 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 1.5255e-05 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.6617 - regression_loss: 15.8560 - binary_classification_loss: 26.9224 - treatment_accuracy: 0.8515 - track_epsilon: 0.0031 - val_loss: 58.2653 - val_regression_loss: 15.8369 - val_binary_classification_loss: 26.5502 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.6582 - regression_loss: 15.8553 - binary_classification_loss: 26.9219 - treatment_accuracy: 0.8515 - track_epsilon: 0.0029 - val_loss: 58.2285 - val_regression_loss: 15.8394 - val_binary_classification_loss: 26.5340 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0017 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.6565 - regression_loss: 15.8545 - binary_classification_loss: 26.9214 - treatment_accuracy: 0.8515 - track_epsilon: 0.0030 - val_loss: 58.2479 - val_regression_loss: 15.8350 - val_binary_classification_loss: 26.5464 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0034 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.6579 - regression_loss: 15.8550 - binary_classification_loss: 26.9226 - treatment_accuracy: 0.8515 - track_epsilon: 0.0029 - val_loss: 58.3032 - val_regression_loss: 15.8426 - val_binary_classification_loss: 26.5318 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0077 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "14965/14979 [============================>.] - ETA: 0s - loss: 58.6552 - regression_loss: 15.8550 - binary_classification_loss: 26.9195 - treatment_accuracy: 0.8515 - track_epsilon: 0.0030\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.6556 - regression_loss: 15.8550 - binary_classification_loss: 26.9199 - treatment_accuracy: 0.8515 - track_epsilon: 0.0030 - val_loss: 58.3847 - val_regression_loss: 15.8679 - val_binary_classification_loss: 26.6372 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 2.9923e-04 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.6182 - regression_loss: 15.8478 - binary_classification_loss: 26.9055 - treatment_accuracy: 0.8515 - track_epsilon: 0.0019 - val_loss: 58.2244 - val_regression_loss: 15.8284 - val_binary_classification_loss: 26.5323 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0047 - lr: 5.0000e-06\n",
      "Epoch 36/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.6180 - regression_loss: 15.8474 - binary_classification_loss: 26.9051 - treatment_accuracy: 0.8515 - track_epsilon: 0.0021 - val_loss: 58.2161 - val_regression_loss: 15.8352 - val_binary_classification_loss: 26.5323 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0020 - lr: 5.0000e-06\n",
      "Epoch 37/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.6172 - regression_loss: 15.8471 - binary_classification_loss: 26.9049 - treatment_accuracy: 0.8515 - track_epsilon: 0.0022 - val_loss: 58.3205 - val_regression_loss: 15.8276 - val_binary_classification_loss: 26.6481 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0034 - lr: 5.0000e-06\n",
      "Epoch 38/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.6162 - regression_loss: 15.8466 - binary_classification_loss: 26.9056 - treatment_accuracy: 0.8515 - track_epsilon: 0.0021 - val_loss: 58.2004 - val_regression_loss: 15.8287 - val_binary_classification_loss: 26.5310 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0015 - lr: 5.0000e-06\n",
      "Epoch 39/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.6185 - regression_loss: 15.8482 - binary_classification_loss: 26.9037 - treatment_accuracy: 0.8515 - track_epsilon: 0.0021 - val_loss: 58.1919 - val_regression_loss: 15.8238 - val_binary_classification_loss: 26.5336 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 6.7757e-06 - lr: 5.0000e-06\n",
      "Epoch 40/100\n",
      "14979/14979 [==============================] - 37s 3ms/step - loss: 58.6179 - regression_loss: 15.8470 - binary_classification_loss: 26.9063 - treatment_accuracy: 0.8515 - track_epsilon: 0.0020 - val_loss: 58.2420 - val_regression_loss: 15.8291 - val_binary_classification_loss: 26.5561 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0037 - lr: 5.0000e-06\n",
      "Epoch 41/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.6215 - regression_loss: 15.8480 - binary_classification_loss: 26.9076 - treatment_accuracy: 0.8515 - track_epsilon: 0.0020 - val_loss: 58.2632 - val_regression_loss: 15.8266 - val_binary_classification_loss: 26.5712 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0058 - lr: 5.0000e-06\n",
      "Epoch 42/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.6196 - regression_loss: 15.8480 - binary_classification_loss: 26.9057 - treatment_accuracy: 0.8515 - track_epsilon: 0.0021 - val_loss: 58.1956 - val_regression_loss: 15.8247 - val_binary_classification_loss: 26.5321 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0028 - lr: 5.0000e-06\n",
      "Epoch 43/100\n",
      "14967/14979 [============================>.] - ETA: 0s - loss: 58.6149 - regression_loss: 15.8471 - binary_classification_loss: 26.9033 - treatment_accuracy: 0.8515 - track_epsilon: 0.0020\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "14979/14979 [==============================] - 39s 3ms/step - loss: 58.6165 - regression_loss: 15.8470 - binary_classification_loss: 26.9050 - treatment_accuracy: 0.8515 - track_epsilon: 0.0020 - val_loss: 58.2029 - val_regression_loss: 15.8291 - val_binary_classification_loss: 26.5331 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 44/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5952 - regression_loss: 15.8430 - binary_classification_loss: 26.8960 - treatment_accuracy: 0.8515 - track_epsilon: 0.0016 - val_loss: 58.2382 - val_regression_loss: 15.8372 - val_binary_classification_loss: 26.5354 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0042 - lr: 2.5000e-06\n",
      "Epoch 45/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5952 - regression_loss: 15.8430 - binary_classification_loss: 26.8959 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.2193 - val_regression_loss: 15.8239 - val_binary_classification_loss: 26.5618 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0014 - lr: 2.5000e-06\n",
      "Epoch 46/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5962 - regression_loss: 15.8433 - binary_classification_loss: 26.8963 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.1907 - val_regression_loss: 15.8239 - val_binary_classification_loss: 26.5333 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 3.3624e-04 - lr: 2.5000e-06\n",
      "Epoch 47/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5968 - regression_loss: 15.8432 - binary_classification_loss: 26.8969 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.1945 - val_regression_loss: 15.8241 - val_binary_classification_loss: 26.5352 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0016 - lr: 2.5000e-06\n",
      "Epoch 48/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5949 - regression_loss: 15.8429 - binary_classification_loss: 26.8958 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.2147 - val_regression_loss: 15.8362 - val_binary_classification_loss: 26.5324 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 7.8456e-04 - lr: 2.5000e-06\n",
      "Epoch 49/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5955 - regression_loss: 15.8436 - binary_classification_loss: 26.8950 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.1999 - val_regression_loss: 15.8238 - val_binary_classification_loss: 26.5361 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0030 - lr: 2.5000e-06\n",
      "Epoch 50/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5948 - regression_loss: 15.8426 - binary_classification_loss: 26.8963 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.1898 - val_regression_loss: 15.8236 - val_binary_classification_loss: 26.5317 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0010 - lr: 2.5000e-06\n",
      "Epoch 51/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5950 - regression_loss: 15.8429 - binary_classification_loss: 26.8961 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.2218 - val_regression_loss: 15.8328 - val_binary_classification_loss: 26.5367 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0027 - lr: 2.5000e-06\n",
      "Epoch 52/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5947 - regression_loss: 15.8435 - binary_classification_loss: 26.8945 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.1956 - val_regression_loss: 15.8252 - val_binary_classification_loss: 26.5352 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 1.3217e-05 - lr: 2.5000e-06\n",
      "Epoch 53/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5931 - regression_loss: 15.8423 - binary_classification_loss: 26.8954 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.1934 - val_regression_loss: 15.8248 - val_binary_classification_loss: 26.5318 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0014 - lr: 2.5000e-06\n",
      "Epoch 54/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5949 - regression_loss: 15.8430 - binary_classification_loss: 26.8959 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.1936 - val_regression_loss: 15.8255 - val_binary_classification_loss: 26.5309 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 5.9438e-04 - lr: 2.5000e-06\n",
      "Epoch 55/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5938 - regression_loss: 15.8432 - binary_classification_loss: 26.8948 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.1989 - val_regression_loss: 15.8281 - val_binary_classification_loss: 26.5319 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 6.1265e-04 - lr: 2.5000e-06\n",
      "Epoch 56/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5933 - regression_loss: 15.8429 - binary_classification_loss: 26.8946 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.1990 - val_regression_loss: 15.8259 - val_binary_classification_loss: 26.5379 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 3.3994e-04 - lr: 2.5000e-06\n",
      "Epoch 57/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5966 - regression_loss: 15.8435 - binary_classification_loss: 26.8966 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.2304 - val_regression_loss: 15.8310 - val_binary_classification_loss: 26.5577 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0011 - lr: 2.5000e-06\n",
      "Epoch 58/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5925 - regression_loss: 15.8425 - binary_classification_loss: 26.8948 - treatment_accuracy: 0.8515 - track_epsilon: 0.0016 - val_loss: 58.1904 - val_regression_loss: 15.8242 - val_binary_classification_loss: 26.5318 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0015 - lr: 2.5000e-06\n",
      "Epoch 59/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5946 - regression_loss: 15.8432 - binary_classification_loss: 26.8953 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.1940 - val_regression_loss: 15.8265 - val_binary_classification_loss: 26.5310 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 3.3498e-04 - lr: 2.5000e-06\n",
      "Epoch 60/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5959 - regression_loss: 15.8434 - binary_classification_loss: 26.8961 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.2119 - val_regression_loss: 15.8340 - val_binary_classification_loss: 26.5337 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 1.2399e-04 - lr: 2.5000e-06\n",
      "Epoch 61/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5972 - regression_loss: 15.8439 - binary_classification_loss: 26.8963 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.2300 - val_regression_loss: 15.8250 - val_binary_classification_loss: 26.5698 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0013 - lr: 2.5000e-06\n",
      "Epoch 62/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5949 - regression_loss: 15.8431 - binary_classification_loss: 26.8958 - treatment_accuracy: 0.8515 - track_epsilon: 0.0014 - val_loss: 58.2042 - val_regression_loss: 15.8275 - val_binary_classification_loss: 26.5359 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0017 - lr: 2.5000e-06\n",
      "Epoch 63/100\n",
      "14964/14979 [============================>.] - ETA: 0s - loss: 58.5949 - regression_loss: 15.8432 - binary_classification_loss: 26.8957 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5946 - regression_loss: 15.8432 - binary_classification_loss: 26.8955 - treatment_accuracy: 0.8515 - track_epsilon: 0.0015 - val_loss: 58.2243 - val_regression_loss: 15.8244 - val_binary_classification_loss: 26.5611 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0023 - lr: 2.5000e-06\n",
      "Epoch 64/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5834 - regression_loss: 15.8413 - binary_classification_loss: 26.8900 - treatment_accuracy: 0.8515 - track_epsilon: 0.0010 - val_loss: 58.1991 - val_regression_loss: 15.8269 - val_binary_classification_loss: 26.5324 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0013 - lr: 1.2500e-06\n",
      "Epoch 65/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5849 - regression_loss: 15.8414 - binary_classification_loss: 26.8912 - treatment_accuracy: 0.8515 - track_epsilon: 9.0355e-04 - val_loss: 58.2162 - val_regression_loss: 15.8284 - val_binary_classification_loss: 26.5431 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0019 - lr: 1.2500e-06\n",
      "Epoch 66/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5828 - regression_loss: 15.8405 - binary_classification_loss: 26.8914 - treatment_accuracy: 0.8515 - track_epsilon: 0.0011 - val_loss: 58.1914 - val_regression_loss: 15.8254 - val_binary_classification_loss: 26.5312 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 4.2451e-04 - lr: 1.2500e-06\n",
      "Epoch 67/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5841 - regression_loss: 15.8414 - binary_classification_loss: 26.8908 - treatment_accuracy: 0.8515 - track_epsilon: 0.0010 - val_loss: 58.1956 - val_regression_loss: 15.8250 - val_binary_classification_loss: 26.5354 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 8.8603e-04 - lr: 1.2500e-06\n",
      "Epoch 68/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5833 - regression_loss: 15.8411 - binary_classification_loss: 26.8904 - treatment_accuracy: 0.8515 - track_epsilon: 9.8458e-04 - val_loss: 58.2044 - val_regression_loss: 15.8296 - val_binary_classification_loss: 26.5353 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 6.8363e-04 - lr: 1.2500e-06\n",
      "Epoch 69/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5834 - regression_loss: 15.8410 - binary_classification_loss: 26.8909 - treatment_accuracy: 0.8515 - track_epsilon: 0.0010 - val_loss: 58.2191 - val_regression_loss: 15.8314 - val_binary_classification_loss: 26.5399 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0015 - lr: 1.2500e-06\n",
      "Epoch 70/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5836 - regression_loss: 15.8410 - binary_classification_loss: 26.8910 - treatment_accuracy: 0.8515 - track_epsilon: 0.0011 - val_loss: 58.1938 - val_regression_loss: 15.8239 - val_binary_classification_loss: 26.5365 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 5.0673e-04 - lr: 1.2500e-06\n",
      "Epoch 71/100\n",
      "14979/14979 [==============================] - ETA: 0s - loss: 58.5836 - regression_loss: 15.8411 - binary_classification_loss: 26.8906 - treatment_accuracy: 0.8515 - track_epsilon: 0.0011\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5836 - regression_loss: 15.8411 - binary_classification_loss: 26.8906 - treatment_accuracy: 0.8515 - track_epsilon: 0.0011 - val_loss: 58.2215 - val_regression_loss: 15.8336 - val_binary_classification_loss: 26.5447 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 6.1020e-04 - lr: 1.2500e-06\n",
      "Epoch 72/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5784 - regression_loss: 15.8404 - binary_classification_loss: 26.8880 - treatment_accuracy: 0.8515 - track_epsilon: 6.5712e-04 - val_loss: 58.1960 - val_regression_loss: 15.8274 - val_binary_classification_loss: 26.5315 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 6.8338e-04 - lr: 6.2500e-07\n",
      "Epoch 73/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5781 - regression_loss: 15.8402 - binary_classification_loss: 26.8882 - treatment_accuracy: 0.8515 - track_epsilon: 7.2622e-04 - val_loss: 58.2136 - val_regression_loss: 15.8292 - val_binary_classification_loss: 26.5454 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 9.2346e-04 - lr: 6.2500e-07\n",
      "Epoch 74/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 58.5787 - regression_loss: 15.8404 - binary_classification_loss: 26.8882 - treatment_accuracy: 0.8515 - track_epsilon: 7.0653e-04 - val_loss: 58.2055 - val_regression_loss: 15.8256 - val_binary_classification_loss: 26.5446 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 1.4457e-04 - lr: 6.2500e-07\n",
      "Epoch 75/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 58.5778 - regression_loss: 15.8400 - binary_classification_loss: 26.8882 - treatment_accuracy: 0.8515 - track_epsilon: 7.4493e-04 - val_loss: 58.1988 - val_regression_loss: 15.8247 - val_binary_classification_loss: 26.5386 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0011 - lr: 6.2500e-07\n",
      "Epoch 76/100\n",
      "14979/14979 [==============================] - 35s 2ms/step - loss: 58.5775 - regression_loss: 15.8402 - binary_classification_loss: 26.8874 - treatment_accuracy: 0.8515 - track_epsilon: 7.2609e-04 - val_loss: 58.1961 - val_regression_loss: 15.8258 - val_binary_classification_loss: 26.5336 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 9.9802e-04 - lr: 6.2500e-07\n",
      "Epoch 77/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5789 - regression_loss: 15.8406 - binary_classification_loss: 26.8881 - treatment_accuracy: 0.8515 - track_epsilon: 6.7802e-04 - val_loss: 58.2093 - val_regression_loss: 15.8270 - val_binary_classification_loss: 26.5425 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 0.0018 - lr: 6.2500e-07\n",
      "Epoch 78/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5782 - regression_loss: 15.8404 - binary_classification_loss: 26.8878 - treatment_accuracy: 0.8515 - track_epsilon: 6.8508e-04 - val_loss: 58.2025 - val_regression_loss: 15.8258 - val_binary_classification_loss: 26.5425 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 1.7287e-04 - lr: 6.2500e-07\n",
      "Epoch 79/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5786 - regression_loss: 15.8403 - binary_classification_loss: 26.8885 - treatment_accuracy: 0.8515 - track_epsilon: 6.9934e-04 - val_loss: 58.2015 - val_regression_loss: 15.8252 - val_binary_classification_loss: 26.5419 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 2.0832e-04 - lr: 6.2500e-07\n",
      "Epoch 80/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5775 - regression_loss: 15.8402 - binary_classification_loss: 26.8879 - treatment_accuracy: 0.8515 - track_epsilon: 6.6489e-04 - val_loss: 58.2100 - val_regression_loss: 15.8278 - val_binary_classification_loss: 26.5446 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 2.1359e-05 - lr: 6.2500e-07\n",
      "Epoch 81/100\n",
      "14976/14979 [============================>.] - ETA: 0s - loss: 58.5791 - regression_loss: 15.8402 - binary_classification_loss: 26.8892 - treatment_accuracy: 0.8515 - track_epsilon: 7.1659e-04\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5781 - regression_loss: 15.8402 - binary_classification_loss: 26.8882 - treatment_accuracy: 0.8515 - track_epsilon: 7.1652e-04 - val_loss: 58.1940 - val_regression_loss: 15.8242 - val_binary_classification_loss: 26.5360 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 2.5887e-04 - lr: 6.2500e-07\n",
      "Epoch 82/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5753 - regression_loss: 15.8399 - binary_classification_loss: 26.8865 - treatment_accuracy: 0.8515 - track_epsilon: 4.4489e-04 - val_loss: 58.1906 - val_regression_loss: 15.8242 - val_binary_classification_loss: 26.5323 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 5.2690e-04 - lr: 3.1250e-07\n",
      "Epoch 83/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5751 - regression_loss: 15.8397 - binary_classification_loss: 26.8867 - treatment_accuracy: 0.8515 - track_epsilon: 5.2975e-04 - val_loss: 58.1978 - val_regression_loss: 15.8258 - val_binary_classification_loss: 26.5364 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 8.4430e-04 - lr: 3.1250e-07\n",
      "Epoch 84/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5754 - regression_loss: 15.8396 - binary_classification_loss: 26.8868 - treatment_accuracy: 0.8515 - track_epsilon: 5.2045e-04 - val_loss: 58.2042 - val_regression_loss: 15.8282 - val_binary_classification_loss: 26.5396 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 1.0065e-04 - lr: 3.1250e-07\n",
      "Epoch 85/100\n",
      "14979/14979 [==============================] - 38s 3ms/step - loss: 58.5753 - regression_loss: 15.8398 - binary_classification_loss: 26.8870 - treatment_accuracy: 0.8515 - track_epsilon: 5.1758e-04 - val_loss: 58.1990 - val_regression_loss: 15.8245 - val_binary_classification_loss: 26.5397 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 2.7172e-04 - lr: 3.1250e-07\n",
      "Epoch 86/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5749 - regression_loss: 15.8397 - binary_classification_loss: 26.8865 - treatment_accuracy: 0.8515 - track_epsilon: 4.6138e-04 - val_loss: 58.2024 - val_regression_loss: 15.8238 - val_binary_classification_loss: 26.5455 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 4.6527e-04 - lr: 3.1250e-07\n",
      "Epoch 87/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5750 - regression_loss: 15.8397 - binary_classification_loss: 26.8869 - treatment_accuracy: 0.8515 - track_epsilon: 5.1934e-04 - val_loss: 58.1951 - val_regression_loss: 15.8243 - val_binary_classification_loss: 26.5368 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 8.4684e-04 - lr: 3.1250e-07\n",
      "Epoch 88/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5754 - regression_loss: 15.8398 - binary_classification_loss: 26.8865 - treatment_accuracy: 0.8515 - track_epsilon: 5.1724e-04 - val_loss: 58.1928 - val_regression_loss: 15.8246 - val_binary_classification_loss: 26.5335 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 9.2028e-04 - lr: 3.1250e-07\n",
      "Epoch 89/100\n",
      "14979/14979 [==============================] - 36s 2ms/step - loss: 58.5754 - regression_loss: 15.8397 - binary_classification_loss: 26.8866 - treatment_accuracy: 0.8515 - track_epsilon: 4.8319e-04 - val_loss: 58.1993 - val_regression_loss: 15.8253 - val_binary_classification_loss: 26.5400 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 1.0926e-04 - lr: 3.1250e-07\n",
      "Epoch 90/100\n",
      "14979/14979 [==============================] - 37s 2ms/step - loss: 58.5753 - regression_loss: 15.8398 - binary_classification_loss: 26.8866 - treatment_accuracy: 0.8515 - track_epsilon: 4.8609e-04 - val_loss: 58.1963 - val_regression_loss: 15.8267 - val_binary_classification_loss: 26.5326 - val_treatment_accuracy: 0.8546 - val_track_epsilon: 7.5295e-04 - lr: 3.1250e-07\n",
      "37448/37448 [==============================] - 30s 810us/step\n"
     ]
    }
   ],
   "source": [
    "dragon = DragonNet(neurons_per_layer=200, targeted_reg=True)\n",
    "dragon_ite = dragon.fit_predict(x_feature, t_treatment, y_outcome, return_components=False)\n",
    "dragon_ate = dragon_ite.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27fa51d-911d-4f10-ba37-aa96b2c9f6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16267785]\n",
      " [0.16267785]\n",
      " [0.16267785]\n",
      " ...\n",
      " [0.16267785]\n",
      " [0.16267785]\n",
      " [0.16267785]]\n",
      "0.16267778\n"
     ]
    }
   ],
   "source": [
    "print(dragon_ite)\n",
    "print(dragon_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae4d26b-0438-4239-979e-97e02af18b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 0.12840658\n",
      "First Quartile: 0.1284065842628479\n",
      "Median: 0.12840658\n",
      "Third Quartile: 0.1284065842628479\n",
      "Maximum: 0.12840658\n",
      "Interquartile Range: 0.0\n",
      "Upper Bound (Outliers): 0.1284065842628479\n",
      "Lower Bound (Outliers): 0.1284065842628479\n",
      "Outliers: []\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "\n",
    "\n",
    "# Calculate statistics\n",
    "data = np.reshape(dragon_ite, -1)\n",
    "minimum = np.min(data)\n",
    "first_quartile = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "third_quartile = np.percentile(data, 75)\n",
    "maximum = np.max(data)\n",
    "\n",
    "# Interquartile range (IQR)\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Define upper and lower bounds for outliers\n",
    "upper_bound = third_quartile + 1.5 * iqr\n",
    "lower_bound = first_quartile - 1.5 * iqr\n",
    "\n",
    "# Detect outliers\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Minimum:\", minimum)\n",
    "print(\"First Quartile:\", first_quartile)\n",
    "print(\"Median:\", median)\n",
    "print(\"Third Quartile:\", third_quartile)\n",
    "print(\"Maximum:\", maximum)\n",
    "print(\"Interquartile Range:\", iqr)\n",
    "print(\"Upper Bound (Outliers):\", upper_bound)\n",
    "print(\"Lower Bound (Outliers):\", lower_bound)\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "ite_dragon = [minimum, first_quartile, median, third_quartile, maximum, iqr, upper_bound, lower_bound]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe749ca3-5c12-4a5c-b433-70b16c694f2b",
   "metadata": {},
   "source": [
    "## CEVAE Model\n",
    "\n",
    "This module implements the Causal Effect Variational Autoencoder [1]\n",
    "\n",
    "[1] C. Louizos, U. Shalit, J. Mooij, D. Sontag, R. Zemel, M. Welling (2017).\r\n",
    "Causal Effect Inference with Deep Latent-Variable Models.\r\n",
    "http://papers.nips.cc/paper/7223-causal-effect-inference-with-deep-latent-variable-models.pdf\r\n",
    "https://github.com/AMLab-Amsterdam/CEVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c2c74d-e827-452d-9082-1fbd95cff7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=11101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1698d98-3ebb-41f6-a52c-0c1737bb2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[feature_names].values\n",
    "X_test = df_test[feature_names].values\n",
    "treatment_train = df_train['treatmentOffer'].values\n",
    "treatment_test = df_test['treatmentOffer'].values\n",
    "y_train = df_train['offerSuccess'].values\n",
    "y_test = df_test['offerSuccess'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f19ae3-646e-45b8-a322-88f151020120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cevae model settings\n",
    "outcome_dist = \"normal\"\n",
    "latent_dim = 20\n",
    "hidden_dim = 200\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.01\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603cf6e6-93ae-4697-8233-33d3260bd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "cevae = CEVAE(outcome_dist=outcome_dist,\n",
    "              latent_dim=latent_dim,\n",
    "              hidden_dim=hidden_dim,\n",
    "              num_epochs=num_epochs,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              learning_rate_decay=learning_rate_decay,\n",
    "              num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c51f5e8-705f-4fdc-89ca-41d608cf464a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Training with 959 minibatches per epoch\n",
      "DEBUG \t step     0 loss = 39.432\n",
      "DEBUG \t step   100 loss = 21.5587\n",
      "DEBUG \t step   200 loss = 17.7834\n",
      "DEBUG \t step   300 loss = 16.1056\n",
      "DEBUG \t step   400 loss = 14.7949\n",
      "DEBUG \t step   500 loss = 13.9928\n",
      "DEBUG \t step   600 loss = 12.7464\n",
      "DEBUG \t step   700 loss = 11.3728\n",
      "DEBUG \t step   800 loss = 11.5583\n",
      "DEBUG \t step   900 loss = 11.1287\n",
      "DEBUG \t step  1000 loss = 10.064\n",
      "DEBUG \t step  1100 loss = 10.4556\n",
      "DEBUG \t step  1200 loss = 9.80264\n",
      "DEBUG \t step  1300 loss = 9.04074\n",
      "DEBUG \t step  1400 loss = 8.62775\n",
      "DEBUG \t step  1500 loss = 8.42838\n",
      "DEBUG \t step  1600 loss = 7.67158\n",
      "DEBUG \t step  1700 loss = 8.49774\n",
      "DEBUG \t step  1800 loss = 8.13176\n",
      "DEBUG \t step  1900 loss = 7.54507\n",
      "DEBUG \t step  2000 loss = 7.20558\n",
      "DEBUG \t step  2100 loss = 7.25838\n",
      "DEBUG \t step  2200 loss = 7.25943\n",
      "DEBUG \t step  2300 loss = 7.16487\n",
      "DEBUG \t step  2400 loss = 6.76904\n",
      "DEBUG \t step  2500 loss = 7.35741\n",
      "DEBUG \t step  2600 loss = 7.30728\n",
      "DEBUG \t step  2700 loss = 6.72472\n",
      "DEBUG \t step  2800 loss = 5.91676\n",
      "DEBUG \t step  2900 loss = 7.01281\n",
      "DEBUG \t step  3000 loss = 6.46313\n",
      "DEBUG \t step  3100 loss = 6.01823\n",
      "DEBUG \t step  3200 loss = 6.77085\n",
      "DEBUG \t step  3300 loss = 6.09749\n",
      "DEBUG \t step  3400 loss = 6.0199\n",
      "DEBUG \t step  3500 loss = 6.10678\n",
      "DEBUG \t step  3600 loss = 6.03084\n",
      "DEBUG \t step  3700 loss = 5.77299\n",
      "DEBUG \t step  3800 loss = 6.07771\n",
      "DEBUG \t step  3900 loss = 5.44751\n",
      "DEBUG \t step  4000 loss = 6.00574\n",
      "DEBUG \t step  4100 loss = 5.40907\n",
      "DEBUG \t step  4200 loss = 5.17617\n",
      "DEBUG \t step  4300 loss = 5.88157\n",
      "DEBUG \t step  4400 loss = 5.46786\n",
      "DEBUG \t step  4500 loss = 5.53999\n",
      "DEBUG \t step  4600 loss = 5.44812\n",
      "DEBUG \t step  4700 loss = 5.25068\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "losses = cevae.fit(X=torch.tensor(X_train, dtype=torch.float),\n",
    "                   treatment=torch.tensor(treatment_train, dtype=torch.float),\n",
    "                   y=torch.tensor(y_train, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be0af1e-41d0-4926-9bec-a221d5bff8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO \t Evaluating 240 minibatches\n",
      "DEBUG \t batch ate = -0.0518396\n",
      "DEBUG \t batch ate = -0.0578103\n",
      "DEBUG \t batch ate = 0.0179996\n",
      "DEBUG \t batch ate = -0.0672696\n",
      "DEBUG \t batch ate = -0.0212976\n",
      "DEBUG \t batch ate = -0.0394065\n",
      "DEBUG \t batch ate = -0.071669\n",
      "DEBUG \t batch ate = -0.0345337\n",
      "DEBUG \t batch ate = -0.0644459\n",
      "DEBUG \t batch ate = -0.0492067\n",
      "DEBUG \t batch ate = -0.0511732\n",
      "DEBUG \t batch ate = -0.0738739\n",
      "DEBUG \t batch ate = -0.0653151\n",
      "DEBUG \t batch ate = -0.0608201\n",
      "DEBUG \t batch ate = -0.0718545\n",
      "DEBUG \t batch ate = -0.0776044\n",
      "DEBUG \t batch ate = -0.0560759\n",
      "DEBUG \t batch ate = -0.089948\n",
      "DEBUG \t batch ate = -0.0721967\n",
      "DEBUG \t batch ate = -0.0310648\n",
      "DEBUG \t batch ate = -0.0875605\n",
      "DEBUG \t batch ate = -0.056983\n",
      "DEBUG \t batch ate = -0.0885981\n",
      "DEBUG \t batch ate = -0.0835431\n",
      "DEBUG \t batch ate = -0.0559851\n",
      "DEBUG \t batch ate = -0.096993\n",
      "DEBUG \t batch ate = -0.04468\n",
      "DEBUG \t batch ate = -0.0763461\n",
      "DEBUG \t batch ate = -0.0586184\n",
      "DEBUG \t batch ate = -0.0412979\n",
      "DEBUG \t batch ate = -0.0367664\n",
      "DEBUG \t batch ate = -0.0927234\n",
      "DEBUG \t batch ate = -0.108862\n",
      "DEBUG \t batch ate = -0.0730253\n",
      "DEBUG \t batch ate = -0.0390559\n",
      "DEBUG \t batch ate = -0.0739159\n",
      "DEBUG \t batch ate = -0.0598399\n",
      "DEBUG \t batch ate = -0.051695\n",
      "DEBUG \t batch ate = -0.0707358\n",
      "DEBUG \t batch ate = -0.0775618\n",
      "DEBUG \t batch ate = -0.0488791\n",
      "DEBUG \t batch ate = -0.0600749\n",
      "DEBUG \t batch ate = -0.0845111\n",
      "DEBUG \t batch ate = -0.100801\n",
      "DEBUG \t batch ate = -0.0790013\n",
      "DEBUG \t batch ate = -0.016994\n",
      "DEBUG \t batch ate = -0.0754732\n",
      "DEBUG \t batch ate = -0.0868959\n",
      "DEBUG \t batch ate = -0.0673093\n",
      "DEBUG \t batch ate = -0.0949192\n",
      "DEBUG \t batch ate = -0.0824036\n",
      "DEBUG \t batch ate = -0.00960939\n",
      "DEBUG \t batch ate = -0.063209\n",
      "DEBUG \t batch ate = -0.0953682\n",
      "DEBUG \t batch ate = -0.0456414\n",
      "DEBUG \t batch ate = -0.031986\n",
      "DEBUG \t batch ate = -0.0799159\n",
      "DEBUG \t batch ate = -0.0779614\n",
      "DEBUG \t batch ate = -0.0588869\n",
      "DEBUG \t batch ate = -0.0799067\n",
      "DEBUG \t batch ate = -0.0389658\n",
      "DEBUG \t batch ate = -0.0560717\n",
      "DEBUG \t batch ate = -0.0605139\n",
      "DEBUG \t batch ate = -0.0511218\n",
      "DEBUG \t batch ate = -0.0641014\n",
      "DEBUG \t batch ate = -0.0939969\n",
      "DEBUG \t batch ate = -0.103768\n",
      "DEBUG \t batch ate = -0.0983576\n",
      "DEBUG \t batch ate = -0.0532777\n",
      "DEBUG \t batch ate = -0.0568877\n",
      "DEBUG \t batch ate = -0.0776604\n",
      "DEBUG \t batch ate = -0.0498199\n",
      "DEBUG \t batch ate = -0.118822\n",
      "DEBUG \t batch ate = -0.0482421\n",
      "DEBUG \t batch ate = -0.0743496\n",
      "DEBUG \t batch ate = -0.0867228\n",
      "DEBUG \t batch ate = -0.0442112\n",
      "DEBUG \t batch ate = -0.0591625\n",
      "DEBUG \t batch ate = -0.067195\n",
      "DEBUG \t batch ate = -0.0478237\n",
      "DEBUG \t batch ate = -0.0707295\n",
      "DEBUG \t batch ate = -0.0953564\n",
      "DEBUG \t batch ate = -0.0814185\n",
      "DEBUG \t batch ate = -0.0932294\n",
      "DEBUG \t batch ate = -0.0276468\n",
      "DEBUG \t batch ate = -0.0656049\n",
      "DEBUG \t batch ate = 0.00514644\n",
      "DEBUG \t batch ate = -0.0558441\n",
      "DEBUG \t batch ate = -0.114718\n",
      "DEBUG \t batch ate = -0.0804223\n",
      "DEBUG \t batch ate = -0.0506658\n",
      "DEBUG \t batch ate = -0.0336759\n",
      "DEBUG \t batch ate = -0.0941283\n",
      "DEBUG \t batch ate = -0.0909179\n",
      "DEBUG \t batch ate = -0.0233474\n",
      "DEBUG \t batch ate = -0.0839163\n",
      "DEBUG \t batch ate = -0.0616572\n",
      "DEBUG \t batch ate = -0.0693285\n",
      "DEBUG \t batch ate = -0.0697896\n",
      "DEBUG \t batch ate = -0.0572622\n",
      "DEBUG \t batch ate = -0.051247\n",
      "DEBUG \t batch ate = -0.0891693\n",
      "DEBUG \t batch ate = -0.0652369\n",
      "DEBUG \t batch ate = -0.0671666\n",
      "DEBUG \t batch ate = -0.0428197\n",
      "DEBUG \t batch ate = -0.0448927\n",
      "DEBUG \t batch ate = -0.0807265\n",
      "DEBUG \t batch ate = -0.0717007\n",
      "DEBUG \t batch ate = -0.0675121\n",
      "DEBUG \t batch ate = -0.0240663\n",
      "DEBUG \t batch ate = -0.072596\n",
      "DEBUG \t batch ate = -0.0435182\n",
      "DEBUG \t batch ate = -0.0505328\n",
      "DEBUG \t batch ate = -0.0499899\n",
      "DEBUG \t batch ate = -0.0654331\n",
      "DEBUG \t batch ate = -0.0816748\n",
      "DEBUG \t batch ate = -0.0791226\n",
      "DEBUG \t batch ate = -0.0618122\n",
      "DEBUG \t batch ate = -0.067927\n",
      "DEBUG \t batch ate = -0.0418753\n",
      "DEBUG \t batch ate = -0.0924758\n",
      "DEBUG \t batch ate = -0.0743561\n",
      "DEBUG \t batch ate = -0.0744041\n",
      "DEBUG \t batch ate = -0.0369726\n",
      "DEBUG \t batch ate = -0.0752771\n",
      "DEBUG \t batch ate = -0.046929\n",
      "DEBUG \t batch ate = -0.0716558\n",
      "DEBUG \t batch ate = -0.0593658\n",
      "DEBUG \t batch ate = -0.0666576\n",
      "DEBUG \t batch ate = -0.0291832\n",
      "DEBUG \t batch ate = -0.104986\n",
      "DEBUG \t batch ate = -0.0668255\n",
      "DEBUG \t batch ate = -0.103578\n",
      "DEBUG \t batch ate = -0.0292368\n",
      "DEBUG \t batch ate = -0.0674471\n",
      "DEBUG \t batch ate = -0.0670198\n",
      "DEBUG \t batch ate = -0.0632421\n",
      "DEBUG \t batch ate = -0.0497514\n",
      "DEBUG \t batch ate = -0.0583849\n",
      "DEBUG \t batch ate = -0.100242\n",
      "DEBUG \t batch ate = -0.0735576\n",
      "DEBUG \t batch ate = -0.0626039\n",
      "DEBUG \t batch ate = -0.025532\n",
      "DEBUG \t batch ate = -0.0941244\n",
      "DEBUG \t batch ate = -0.0585512\n",
      "DEBUG \t batch ate = -0.128337\n",
      "DEBUG \t batch ate = -0.04384\n",
      "DEBUG \t batch ate = -0.0300757\n",
      "DEBUG \t batch ate = -0.094013\n",
      "DEBUG \t batch ate = -0.0415032\n",
      "DEBUG \t batch ate = -0.0149274\n",
      "DEBUG \t batch ate = -0.0541562\n",
      "DEBUG \t batch ate = -0.0611741\n",
      "DEBUG \t batch ate = -0.0299421\n",
      "DEBUG \t batch ate = -0.0544316\n",
      "DEBUG \t batch ate = -0.0580529\n",
      "DEBUG \t batch ate = -0.0602158\n",
      "DEBUG \t batch ate = -0.0936916\n",
      "DEBUG \t batch ate = -0.0319495\n",
      "DEBUG \t batch ate = -0.0410073\n",
      "DEBUG \t batch ate = -0.0328568\n",
      "DEBUG \t batch ate = -0.0376928\n",
      "DEBUG \t batch ate = -0.06352\n",
      "DEBUG \t batch ate = -0.0686171\n",
      "DEBUG \t batch ate = -0.0827995\n",
      "DEBUG \t batch ate = -0.121243\n",
      "DEBUG \t batch ate = -0.059349\n",
      "DEBUG \t batch ate = -0.0395915\n",
      "DEBUG \t batch ate = -0.0977358\n",
      "DEBUG \t batch ate = -0.0331063\n",
      "DEBUG \t batch ate = -0.0937992\n",
      "DEBUG \t batch ate = -0.0682878\n",
      "DEBUG \t batch ate = -0.0453255\n",
      "DEBUG \t batch ate = -0.038154\n",
      "DEBUG \t batch ate = -0.0979606\n",
      "DEBUG \t batch ate = -0.140811\n",
      "DEBUG \t batch ate = -0.0583728\n",
      "DEBUG \t batch ate = -0.0446266\n",
      "DEBUG \t batch ate = -0.0734503\n",
      "DEBUG \t batch ate = -0.0814751\n",
      "DEBUG \t batch ate = -0.0265666\n",
      "DEBUG \t batch ate = -0.0983509\n",
      "DEBUG \t batch ate = -0.0315252\n",
      "DEBUG \t batch ate = -0.0617645\n",
      "DEBUG \t batch ate = -0.0662485\n",
      "DEBUG \t batch ate = -0.0527539\n",
      "DEBUG \t batch ate = -0.0596034\n",
      "DEBUG \t batch ate = -0.0519054\n",
      "DEBUG \t batch ate = -0.0900351\n",
      "DEBUG \t batch ate = -0.0615231\n",
      "DEBUG \t batch ate = -0.0822812\n",
      "DEBUG \t batch ate = -0.0472789\n",
      "DEBUG \t batch ate = -0.019942\n",
      "DEBUG \t batch ate = -0.0878153\n",
      "DEBUG \t batch ate = -0.0455233\n",
      "DEBUG \t batch ate = -0.0709021\n",
      "DEBUG \t batch ate = -0.137001\n",
      "DEBUG \t batch ate = -0.0577906\n",
      "DEBUG \t batch ate = -0.0750455\n",
      "DEBUG \t batch ate = -0.0449473\n",
      "DEBUG \t batch ate = -0.110148\n",
      "DEBUG \t batch ate = -0.108896\n",
      "DEBUG \t batch ate = -0.0594219\n",
      "DEBUG \t batch ate = -0.0845948\n",
      "DEBUG \t batch ate = -0.0511399\n",
      "DEBUG \t batch ate = -0.0301736\n",
      "DEBUG \t batch ate = -0.0846234\n",
      "DEBUG \t batch ate = -0.12821\n",
      "DEBUG \t batch ate = -0.122765\n",
      "DEBUG \t batch ate = -0.0742341\n",
      "DEBUG \t batch ate = -0.0799123\n",
      "DEBUG \t batch ate = -0.0446661\n",
      "DEBUG \t batch ate = -0.0315228\n",
      "DEBUG \t batch ate = -0.0432041\n",
      "DEBUG \t batch ate = -0.0371497\n",
      "DEBUG \t batch ate = -0.0466465\n",
      "DEBUG \t batch ate = -0.0616004\n",
      "DEBUG \t batch ate = -0.0536708\n",
      "DEBUG \t batch ate = -0.0720947\n",
      "DEBUG \t batch ate = -0.0719956\n",
      "DEBUG \t batch ate = -0.0457895\n",
      "DEBUG \t batch ate = -0.084357\n",
      "DEBUG \t batch ate = -0.00208379\n",
      "DEBUG \t batch ate = -0.0469171\n",
      "DEBUG \t batch ate = -0.0985466\n",
      "DEBUG \t batch ate = -0.0722363\n",
      "DEBUG \t batch ate = -0.0700491\n",
      "DEBUG \t batch ate = -0.0743302\n",
      "DEBUG \t batch ate = -0.0396094\n",
      "DEBUG \t batch ate = -0.0946563\n",
      "DEBUG \t batch ate = -0.058871\n",
      "DEBUG \t batch ate = -0.0722969\n",
      "DEBUG \t batch ate = -0.0846419\n",
      "DEBUG \t batch ate = -0.0424237\n",
      "DEBUG \t batch ate = -0.054277\n",
      "DEBUG \t batch ate = -0.0830123\n",
      "DEBUG \t batch ate = -0.108943\n",
      "DEBUG \t batch ate = -0.096532\n",
      "DEBUG \t batch ate = -0.12012\n",
      "DEBUG \t batch ate = -0.0686008\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "ate_val = cevae.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2eb0c-a9eb-45fe-87e9-79cf1a6743cd",
   "metadata": {},
   "source": [
    "## Input results in result df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724fc993-9d6b-4e7a-8943-d01d733a471d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m df_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m      7\u001b[0m     df_results\u001b[38;5;241m.\u001b[39mloc[df_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dragon_ate\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mdf_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mITE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m ite_dragon\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     df_results \u001b[38;5;241m=\u001b[39m df_results\u001b[38;5;241m.\u001b[39m_append({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mATE\u001b[39m\u001b[38;5;124m'\u001b[39m: dragon_ate, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITE\u001b[39m\u001b[38;5;124m'\u001b[39m: ite_dragon, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLibrary\u001b[39m\u001b[38;5;124m'\u001b[39m: lib}, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/base/2023-10-10/lib/python3.9/site-packages/pandas/core/indexing.py:885\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    884\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 885\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/base/2023-10-10/lib/python3.9/site-packages/pandas/core/indexing.py:1893\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1893\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/base/2023-10-10/lib/python3.9/site-packages/pandas/core/indexing.py:1949\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(info_axis):\n\u001b[1;32m   1945\u001b[0m         \u001b[38;5;66;03m# This is a case like df.iloc[:3, [1]] = [0]\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m         \u001b[38;5;66;03m#  where we treat as df.iloc[:3, 1] = 0\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer((pi, info_axis[\u001b[38;5;241m0\u001b[39m]), value[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have equal len keys and value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1951\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen setting with an iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1952\u001b[0m     )\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lplane_indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;66;03m# We get here in one case via .loc with a all-False mask\u001b[39;00m\n\u001b[1;32m   1956\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "#for Dragonnet\n",
    "%store -r df_results\n",
    "lib = \"CausalML\"\n",
    "method = \"Dragonnet\"\n",
    "\n",
    "if method in df_results['method'].values:\n",
    "    df_results.loc[df_results['method'] == method, 'ATE'] = dragon_ate\n",
    "    df_results.loc[df_results['method'] == method, 'ITE'] = ite_dragon\n",
    "\n",
    "else:\n",
    "    df_results = df_results._append({'method': method, 'ATE': dragon_ate, 'ITE': ite_dragon, 'Library': lib}, ignore_index=True)\n",
    "\n",
    "%store df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1e115d-9d58-4f04-b758-1ae7ffa057be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_results' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# for CEVAE\n",
    "%store -r df_results\n",
    "ate = ate_val.mean()\n",
    "lib = \"CausalML\"\n",
    "method = \"CEVAE\"\n",
    "\n",
    "if method in df_results['method'].values:\n",
    "    df_results.loc[df_results['method'] == method, 'ATE'] = ate\n",
    "\n",
    "else:\n",
    "    df_results = df_results._append({'method': method, 'ATE': ate, 'Library': lib}, ignore_index=True)\n",
    "\n",
    "%store df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-causalml",
   "language": "python",
   "name": "nn-causalml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667579c2-ecbc-4b09-a8ce-bf2e3b669623",
   "metadata": {},
   "source": [
    "Accelerated Bayesian Causal Forest\n",
    "https://johaupt.github.io/blog/xbcf.html\n",
    "\n",
    "Accelerated Bayesian Causal Forest (XBCF) to estimate the conditional average treatment effect (or uplift) using a specialized version of Bayesian Additive Regression Trees (BART). Itâ€™s better described as Bayesian boosted trees for non-parametric causal inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745ad78f-b583-44bc-a860-0fa4a13262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xbcausalforest import XBCF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff8872-9d01-4752-95ad-713ab4b8caaa",
   "metadata": {},
   "source": [
    "## BPIC 2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a723eb1e-39ae-45da-8114-a558c2222b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bpi2017_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68aac10-373e-4efe-8528-43776b46739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['NumberOfOffers', 'Action', 'org:resource',\n",
    "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
    "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
    "       'CreditScore', 'OfferedAmount', 'offerNumber','timeApplication', 'weekdayApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde3a0fd-7f22-4066-9e88-7424475acd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=11101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b17ec1-7208-4b25-8ab0-c392190d31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment=df_train['treatmentOffer']\n",
    "X = df_train[feature_names]\n",
    "y=df_train['offerSuccess']\n",
    "\n",
    "treatment_test=df_test['treatmentOffer']\n",
    "X_test = df_test[feature_names]\n",
    "y_test=df_test['offerSuccess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3e664c-127f-4aa0-9353-0dff5b0f6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TREES_PR  = 200\n",
    "NUM_TREES_TRT = 100\n",
    "\n",
    "cf = XBCF(\n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    num_sweeps=50, \n",
    "    burnin=15,\n",
    "    max_depth=250,\n",
    "    num_trees_pr=NUM_TREES_PR,\n",
    "    num_trees_trt=NUM_TREES_TRT,\n",
    "    num_cutpoints=100,\n",
    "    Nmin=1,\n",
    "    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'\n",
    "    #mtry_trt=X.shape[1], \n",
    "    tau_pr = 0.6 * np.var(y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,\n",
    "    tau_trt = 0.1 * np.var(y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,\n",
    "    alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "    beta_pr= 2, # shrinkage (tree depth)\n",
    "    alpha_trt= 0.95, # shrinkage for treatment part\n",
    "    beta_trt= 2,\n",
    "    p_categorical_pr = 0,\n",
    "    p_categorical_trt = 0,\n",
    "    standardize_target=True, # standardize y and unstandardize for prediction\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551935a8-4688-4d43-9ab3-f0ecc942c287",
   "metadata": {},
   "source": [
    "Since we specify the model as a sum of two BARTs, we can pass different sets of covariates to the outcome model and the treatment model, denoted by x and x_t. z is the treatment indicator coded 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2b870-fe15-40ef-864b-171ad62776de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## should this be included in the second X, as the propensity score\n",
    "p_model = ElasticNetPropensityModel()\n",
    "p = p_model.fit_predict(X, treatment)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcf4ccc-5c18-49f8-88ed-5bbf6e13106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10h 56min 59s, sys: 10min 8s, total: 11h 7min 7s\n",
      "Wall time: 1h 21min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XBCF(num_sweeps = 50, burnin = 15, max_depth = 250, Nmin = 1, num_cutpoints = 100, no_split_penality = 4.605170185988092, mtry_pr = 19, mtry_trt = 19, p_categorical_pr = 0, p_categorical_trt = 0, num_trees_pr = 200, alpha_pr = 0.95, beta_pr = 2.0, tau_pr = 0.0007494867168461104, kap_pr = 16.0, s_pr = 4.0, pr_scale = False, num_trees_trt = 100, alpha_trt = 0.95, beta_trt = 2.0, tau_trt = 0.00024982890561537014, kap_trt = 16.0, s_trt = 4.0, trt_scale = False, verbose = False, parallel = True, set_random_seed = False, random_seed = 0, sample_weights_flag = True, a_scaling = True, b_scaling = True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cf.fit(\n",
    "    x_t=X, # Covariates treatment effect\n",
    "    x=X, # Covariates outcome (including propensity score)\n",
    "    y=y,  # Outcome\n",
    "    z=treatment, # Treatment group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562b3bcc-04c8-4e51-b9c0-9db9cb2319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 186 ms, total: 29.7 s\n",
      "Wall time: 29.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03308522, -0.0446359 , -0.05176309, ..., -0.03810597,\n",
       "        0.0370041 , -0.03629823])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tau_xbcf = cf.predict(X_test, return_mean=True)\n",
    "tau_xbcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6c29f1-eb73-4a9d-9c00-433e00ea8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: -0.9658506333554799\n",
      "First Quartile: -0.04886845276852485\n",
      "Median: -0.0368942777357658\n",
      "Third Quartile: 0.03376068109580269\n",
      "Maximum: 2.5479356161410225\n",
      "Interquartile Range: 0.08262913386432755\n",
      "Upper Bound (Outliers): 0.15770438189229402\n",
      "Lower Bound (Outliers): -0.17281215356501617\n",
      "Outliers: [-0.7247538  -0.22329774  0.77603805 ...  0.17415126  2.38680562\n",
      " -0.40932287]\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "data = np.reshape(tau_xbcf, -1)\n",
    "minimum = np.min(data)\n",
    "first_quartile = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "third_quartile = np.percentile(data, 75)\n",
    "maximum = np.max(data)\n",
    "\n",
    "# Interquartile range (IQR)\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Define upper and lower bounds for outliers\n",
    "upper_bound = third_quartile + 1.5 * iqr\n",
    "lower_bound = first_quartile - 1.5 * iqr\n",
    "\n",
    "# Detect outliers\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Minimum:\", minimum)\n",
    "print(\"First Quartile:\", first_quartile)\n",
    "print(\"Median:\", median)\n",
    "print(\"Third Quartile:\", third_quartile)\n",
    "print(\"Maximum:\", maximum)\n",
    "print(\"Interquartile Range:\", iqr)\n",
    "print(\"Upper Bound (Outliers):\", upper_bound)\n",
    "print(\"Lower Bound (Outliers):\", lower_bound)\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "ite_bart = [minimum, first_quartile, median, third_quartile, maximum, iqr, upper_bound, lower_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246df696-f942-4081-b82d-ae8f4e769914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           method       ATE  \\\n",
      "0                               Linear Regression  0.449046   \n",
      "1                                       Double ML  0.471019   \n",
      "2                                             IPW  0.311352   \n",
      "3                                       IPW Hajek  0.311352   \n",
      "4                                  IPW Stabalized  0.311352   \n",
      "5                       Propensity Score Matching -0.179289   \n",
      "6                               Distance Matching  0.630322   \n",
      "7                                          IPW LR  0.149171   \n",
      "8                                       CausalEGM  0.124771   \n",
      "9                                     Causal Tree  0.034357   \n",
      "10                                    cforest_mse  0.087602   \n",
      "11                                   cforest_cmse  0.301697   \n",
      "12                             cforest_cmse_p=0.5  0.032615   \n",
      "13                        cforest_cmse_p=0.5_md=3  0.042756   \n",
      "14                                  cforest_ttest  0.130985   \n",
      "15             Accelerated Bayesian Causal Forest -0.012411   \n",
      "16                                   S-Learner LR  0.064775   \n",
      "17                                  XGBTRegressor  0.182946   \n",
      "18                             BaseTRegressor XGB  0.182946   \n",
      "19                              BaseTRegressor LR  0.094099   \n",
      "20                             BaseXRegressor XGB  0.512767   \n",
      "21                              BaseXRegressor LR  0.079609   \n",
      "22  BaseXRegressor XGB (without propensity score)  0.539519   \n",
      "23   BaseXRegressor LR (without propensity score)  0.084548   \n",
      "24                                           TMLE -0.000583   \n",
      "25                              TMLE Standardized -0.000868   \n",
      "26                            TMLE (with weights)  0.031268   \n",
      "27                             BaseRRegressor XGB  0.001693   \n",
      "28                              BaseRRegressor LR  0.031397   \n",
      "29        BaseRRegressor XGB (with random weight)  0.002552   \n",
      "30  BaseRRegressor XGB (without propensity score)  0.000052   \n",
      "31                           Neural Network (MLP)  0.137118   \n",
      "32                                         BCAUSS  0.015102   \n",
      "33                                      Dragonnet  0.128407   \n",
      "34                                          CEVAE -0.065165   \n",
      "\n",
      "                                                  ITE         Library  \\\n",
      "0                                                 NaN           DoWhy   \n",
      "1   [0.0, 0.0, 0.0, 0.9730893270729309, 1.24351258...           DoWhy   \n",
      "2                                                 NaN           DoWhy   \n",
      "3                                                 NaN           DoWhy   \n",
      "4                                                 NaN           DoWhy   \n",
      "5                                                 NaN           DoWhy   \n",
      "6                                                 NaN           DoWhy   \n",
      "7                                                 NaN       Causallib   \n",
      "8                                                 NaN       CausalEGM   \n",
      "9   [-0.7879656160458453, -0.043494462980468274, 0...        CausalML   \n",
      "10  862609     0.009281\\n1158138    0.155046\\n6033...        CausalML   \n",
      "11  862609     0.534503\\n1158138    0.267160\\n6033...        CausalML   \n",
      "12  862609    -0.032847\\n1158138   -0.032847\\n6033...        CausalML   \n",
      "13  862609    -0.072982\\n1158138   -0.072982\\n6033...        CausalML   \n",
      "14  862609     0.020529\\n1158138    0.090639\\n6033...        CausalML   \n",
      "15  [-0.9658506333554799, -0.04886845276852485, -0...  xbcausalforest   \n",
      "16  [0.06477473769604769, 0.06477473769604813, 0.0...        CausalML   \n",
      "17  [-1.247403621673584, -0.007445007562637329, 0....        CausalML   \n",
      "18  [-1.247403621673584, -0.007445007562637329, 0....        CausalML   \n",
      "19  [-0.7549372243351995, 0.017519640303828188, 0....        CausalML   \n",
      "20  [-0.5197197980772417, 0.14606450637986346, 0.5...        CausalML   \n",
      "21  [-0.7549372243359451, 0.016062774379361974, 0....        CausalML   \n",
      "22  [-0.5114930704305567, 0.1463220536527003, 0.68...        CausalML   \n",
      "23  [-0.7549372243358587, 0.017081473269835855, 0....        CausalML   \n",
      "24                                                NaN       Causallib   \n",
      "25                                                NaN       Causallib   \n",
      "26                                                NaN       Causallib   \n",
      "27  [-2.841172695159912, -0.022177141159772873, -0...        CausalML   \n",
      "28  [-0.6539629778797732, -0.014001676778572877, 0...        CausalML   \n",
      "29  [-3.097541332244873, -0.020540252327919006, -0...        CausalML   \n",
      "30  [-5.4697980880737305, -0.026043339632451534, -...        CausalML   \n",
      "31                                                NaN        CausalML   \n",
      "32                                                NaN          BCAUSS   \n",
      "33  [0.12840658, 0.1284065842628479, 0.12840658, 0...        CausalML   \n",
      "34                                                NaN        CausalML   \n",
      "\n",
      "          Method Type  \n",
      "0   Linear Regression  \n",
      "1                DML?  \n",
      "2                 IPW  \n",
      "3                 IPW  \n",
      "4                 IPW  \n",
      "5            Matching  \n",
      "6            Matching  \n",
      "7                 IPW  \n",
      "8       Deep Learning  \n",
      "9              Uplift  \n",
      "10             Uplift  \n",
      "11             Uplift  \n",
      "12             Uplift  \n",
      "13             Uplift  \n",
      "14             Uplift  \n",
      "15             Uplift  \n",
      "16       Meta-Learner  \n",
      "17       Meta-Learner  \n",
      "18       Meta-Learner  \n",
      "19       Meta-Learner  \n",
      "20       Meta-Learner  \n",
      "21       Meta-Learner  \n",
      "22       Meta-Learner  \n",
      "23       Meta-Learner  \n",
      "24               TMLE  \n",
      "25               TMLE  \n",
      "26               TMLE  \n",
      "27       Meta-Learner  \n",
      "28       Meta-Learner  \n",
      "29       Meta-Learner  \n",
      "30       Meta-Learner  \n",
      "31      Deep Learning  \n",
      "32      Deep Learning  \n",
      "33      Deep Learning  \n",
      "34      Deep Learning  \n",
      "Stored 'df_results' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store -r df_results\n",
    "lib = \"xbcausalforest\"\n",
    "method = \"Accelerated Bayesian Causal Forest\"\n",
    "ite = ite_bart\n",
    "ate = tau_xbcf.mean()\n",
    "\n",
    "if method in df_results['method'].values:\n",
    "     # If the method is already in the DataFrame, update the ATE and ITE columns\n",
    "    df_results.loc[df_results['method'] == method, 'ATE'] = ate\n",
    "    index = df_results[df_results['method'] == method].index[0]\n",
    "    df_results.at[index, 'ITE'] = ite\n",
    "else:\n",
    "    # If the method is not in the DataFrame, add a new row\n",
    "    df_results = df_results._append({'method': method, 'ATE': ate, 'ITE': ite, 'Library': lib}, ignore_index=True)\n",
    "\n",
    "print(df_results)\n",
    "%store df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00dabd-40be-4563-a92a-17f50cfa39a4",
   "metadata": {},
   "source": [
    "## Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08bb7220-f4d6-43fc-9abc-d355013ff316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synth = pd.read_csv(\"synthetic_dataset.csv\")\n",
    "df_synth.head()\n",
    "synthetic_features = ['NumberOfOffers', 'concept:name',\n",
    "       'lifecycle:transition', 'time:timestamp', 'elementId', 'resourceId',\n",
    "       'weekdayApplication', 'timeApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148f5c77-777b-464d-97c8-25f6caf9bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_synth_train, df_synth_test = train_test_split(df_synth, test_size=0.2, random_state=11101)\n",
    "\n",
    "synth_treatment=df_synth_train['treatment']\n",
    "synth_X = df_synth_train[synthetic_features]\n",
    "synth_y=df_synth_train['treatmentSuccess']\n",
    "\n",
    "synth_treatment_test=df_synth_test['treatment']\n",
    "synth_X_test = df_synth_test[synthetic_features]\n",
    "synth_y_test=df_synth_test['treatmentSuccess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11c0a6d-c23c-4703-9e13-e390e578ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TREES_PR  = 200\n",
    "NUM_TREES_TRT = 100\n",
    "\n",
    "cf = XBCF(\n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    num_sweeps=50, \n",
    "    burnin=15,\n",
    "    max_depth=250,\n",
    "    num_trees_pr=NUM_TREES_PR,\n",
    "    num_trees_trt=NUM_TREES_TRT,\n",
    "    num_cutpoints=100,\n",
    "    Nmin=1,\n",
    "    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'\n",
    "    #mtry_trt=X.shape[1], \n",
    "    tau_pr = 0.6 * np.var(y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,\n",
    "    tau_trt = 0.1 * np.var(y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,\n",
    "    alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "    beta_pr= 2, # shrinkage (tree depth)\n",
    "    alpha_trt= 0.95, # shrinkage for treatment part\n",
    "    beta_trt= 2,\n",
    "    p_categorical_pr = 0,\n",
    "    p_categorical_trt = 0,\n",
    "    standardize_target=True, # standardize y and unstandardize for prediction\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6331bf-aca9-4c00-9dc1-c12c22f9db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cf.fit(\n",
    "    x_t=synth_X, # Covariates treatment effect\n",
    "    x=synth_X, # Covariates outcome (including propensity score)\n",
    "    y=synth_y,  # Outcome\n",
    "    z=synth_treatment, # Treatment group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe422b6-821f-4d63-b08c-6de0250525c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "synth_tau_xbcf = cf.predict(X_test, return_mean=True)\n",
    "synth_tau_xbcf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart",
   "language": "python",
   "name": "bart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667579c2-ecbc-4b09-a8ce-bf2e3b669623",
   "metadata": {},
   "source": [
    "Accelerated Bayesian Causal Forest\n",
    "https://johaupt.github.io/blog/xbcf.html\n",
    "\n",
    "Accelerated Bayesian Causal Forest (XBCF) to estimate the conditional average treatment effect (or uplift) using a specialized version of Bayesian Additive Regression Trees (BART). Itâ€™s better described as Bayesian boosted trees for non-parametric causal inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745ad78f-b583-44bc-a860-0fa4a13262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xbcausalforest import XBCF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff8872-9d01-4752-95ad-713ab4b8caaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BPIC 2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a723eb1e-39ae-45da-8114-a558c2222b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bpi2017_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68aac10-373e-4efe-8528-43776b46739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['NumberOfOffers', 'Action', 'org:resource',\n",
    "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
    "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
    "       'CreditScore', 'OfferedAmount', 'offerNumber','timeApplication', 'weekdayApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde3a0fd-7f22-4066-9e88-7424475acd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=11101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b17ec1-7208-4b25-8ab0-c392190d31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment=df_train['treatmentOffer']\n",
    "X = df_train[feature_names]\n",
    "y=df_train['offerSuccess']\n",
    "\n",
    "treatment_test=df_test['treatmentOffer']\n",
    "X_test = df_test[feature_names]\n",
    "y_test=df_test['offerSuccess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e54eac-03a7-4394-846f-ca5841ef3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_df = \n",
    "\n",
    "# df1 = df[df['treatment'] == 1]\n",
    "# df2 = df[df['treatment'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3e664c-127f-4aa0-9353-0dff5b0f6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TREES_PR  = 200\n",
    "NUM_TREES_TRT = 100\n",
    "\n",
    "cf = XBCF(\n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    num_sweeps=50, \n",
    "    burnin=15,\n",
    "    max_depth=250,\n",
    "    num_trees_pr=NUM_TREES_PR,\n",
    "    num_trees_trt=NUM_TREES_TRT,\n",
    "    num_cutpoints=100,\n",
    "    Nmin=1,\n",
    "    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'\n",
    "    #mtry_trt=X.shape[1], \n",
    "    tau_pr = 0.6 * np.var(y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,\n",
    "    tau_trt = 0.1 * np.var(y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,\n",
    "    alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "    beta_pr= 2, # shrinkage (tree depth)\n",
    "    alpha_trt= 0.95, # shrinkage for treatment part\n",
    "    beta_trt= 2,\n",
    "    p_categorical_pr = 0,\n",
    "    p_categorical_trt = 0,\n",
    "    standardize_target=True, # standardize y and unstandardize for prediction\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551935a8-4688-4d43-9ab3-f0ecc942c287",
   "metadata": {},
   "source": [
    "Since we specify the model as a sum of two BARTs, we can pass different sets of covariates to the outcome model and the treatment model, denoted by x and x_t. z is the treatment indicator coded 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2b870-fe15-40ef-864b-171ad62776de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## should this be included in the second X, as the propensity score\n",
    "p_model = ElasticNetPropensityModel()\n",
    "p = p_model.fit_predict(X, treatment)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcf4ccc-5c18-49f8-88ed-5bbf6e13106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10h 56min 59s, sys: 10min 8s, total: 11h 7min 7s\n",
      "Wall time: 1h 21min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XBCF(num_sweeps = 50, burnin = 15, max_depth = 250, Nmin = 1, num_cutpoints = 100, no_split_penality = 4.605170185988092, mtry_pr = 19, mtry_trt = 19, p_categorical_pr = 0, p_categorical_trt = 0, num_trees_pr = 200, alpha_pr = 0.95, beta_pr = 2.0, tau_pr = 0.0007494867168461104, kap_pr = 16.0, s_pr = 4.0, pr_scale = False, num_trees_trt = 100, alpha_trt = 0.95, beta_trt = 2.0, tau_trt = 0.00024982890561537014, kap_trt = 16.0, s_trt = 4.0, trt_scale = False, verbose = False, parallel = True, set_random_seed = False, random_seed = 0, sample_weights_flag = True, a_scaling = True, b_scaling = True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cf.fit(\n",
    "    x_t=X, # Covariates treatment effect\n",
    "    x=X, # Covariates outcome (including propensity score)\n",
    "    y=y,  # Outcome\n",
    "    z=treatment, # Treatment group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562b3bcc-04c8-4e51-b9c0-9db9cb2319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 186 ms, total: 29.7 s\n",
      "Wall time: 29.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03308522, -0.0446359 , -0.05176309, ..., -0.03810597,\n",
       "        0.0370041 , -0.03629823])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tau_xbcf = cf.predict(X_test, return_mean=True)\n",
    "tau_xbcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6c29f1-eb73-4a9d-9c00-433e00ea8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: -0.9658506333554799\n",
      "First Quartile: -0.04886845276852485\n",
      "Median: -0.0368942777357658\n",
      "Third Quartile: 0.03376068109580269\n",
      "Maximum: 2.5479356161410225\n",
      "Interquartile Range: 0.08262913386432755\n",
      "Upper Bound (Outliers): 0.15770438189229402\n",
      "Lower Bound (Outliers): -0.17281215356501617\n",
      "Outliers: [-0.7247538  -0.22329774  0.77603805 ...  0.17415126  2.38680562\n",
      " -0.40932287]\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "data = np.reshape(tau_xbcf, -1)\n",
    "minimum = np.min(data)\n",
    "first_quartile = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "third_quartile = np.percentile(data, 75)\n",
    "maximum = np.max(data)\n",
    "\n",
    "# Interquartile range (IQR)\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Define upper and lower bounds for outliers\n",
    "upper_bound = third_quartile + 1.5 * iqr\n",
    "lower_bound = first_quartile - 1.5 * iqr\n",
    "\n",
    "# Detect outliers\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Minimum:\", minimum)\n",
    "print(\"First Quartile:\", first_quartile)\n",
    "print(\"Median:\", median)\n",
    "print(\"Third Quartile:\", third_quartile)\n",
    "print(\"Maximum:\", maximum)\n",
    "print(\"Interquartile Range:\", iqr)\n",
    "print(\"Upper Bound (Outliers):\", upper_bound)\n",
    "print(\"Lower Bound (Outliers):\", lower_bound)\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "ite_bart = [minimum, first_quartile, median, third_quartile, maximum, iqr, upper_bound, lower_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246df696-f942-4081-b82d-ae8f4e769914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r df_results\n",
    "lib = \"xbcausalforest\"\n",
    "method = \"Accelerated Bayesian Causal Forest\"\n",
    "ite = ite_bart\n",
    "ate = tau_xbcf.mean()\n",
    "\n",
    "if method in df_results['method'].values:\n",
    "     # If the method is already in the DataFrame, update the ATE and ITE columns\n",
    "    df_results.loc[df_results['method'] == method, 'ATE'] = ate\n",
    "    index = df_results[df_results['method'] == method].index[0]\n",
    "    df_results.at[index, 'ITE'] = ite\n",
    "else:\n",
    "    # If the method is not in the DataFrame, add a new row\n",
    "    df_results = df_results._append({'method': method, 'ATE': ate, 'ITE': ite, 'Library': lib}, ignore_index=True)\n",
    "\n",
    "print(df_results)\n",
    "%store df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00dabd-40be-4563-a92a-17f50cfa39a4",
   "metadata": {},
   "source": [
    "## Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bb7220-f4d6-43fc-9abc-d355013ff316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synth = pd.read_csv(\"synthetic_dataset.csv\")\n",
    "df_synth.head()\n",
    "synthetic_features = ['NumberOfOffers', 'concept:name',\n",
    "       'lifecycle:transition', 'time:timestamp', 'elementId', 'resourceId',\n",
    "       'weekdayApplication', 'timeApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148f5c77-777b-464d-97c8-25f6caf9bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_synth_train, df_synth_test = train_test_split(df_synth, test_size=0.2, random_state=11101)\n",
    "\n",
    "synth_treatment=df_synth_train['treatment']\n",
    "synth_X = df_synth_train[synthetic_features]\n",
    "synth_y=df_synth_train['treatmentSuccess']\n",
    "\n",
    "synth_treatment_test=df_synth_test['treatment']\n",
    "synth_X_test = df_synth_test[synthetic_features]\n",
    "synth_y_test=df_synth_test['treatmentSuccess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11c0a6d-c23c-4703-9e13-e390e578ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TREES_PR  = 200\n",
    "NUM_TREES_TRT = 100\n",
    "\n",
    "cf = XBCF(\n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    num_sweeps=50, \n",
    "    burnin=15,\n",
    "    max_depth=250,\n",
    "    num_trees_pr=NUM_TREES_PR,\n",
    "    num_trees_trt=NUM_TREES_TRT,\n",
    "    num_cutpoints=100,\n",
    "    Nmin=1,\n",
    "    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'\n",
    "    #mtry_trt=X.shape[1], \n",
    "    tau_pr = 0.6 * np.var(synth_y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,\n",
    "    tau_trt = 0.1 * np.var(synth_y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,\n",
    "    alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "    beta_pr= 2, # shrinkage (tree depth)\n",
    "    alpha_trt= 0.95, # shrinkage for treatment part\n",
    "    beta_trt= 2,\n",
    "    p_categorical_pr = 0,\n",
    "    p_categorical_trt = 0,\n",
    "    standardize_target=True, # standardize y and unstandardize for prediction\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6331bf-aca9-4c00-9dc1-c12c22f9db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 50s, sys: 11.6 s, total: 22min 2s\n",
      "Wall time: 6min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XBCF(num_sweeps = 50, burnin = 15, max_depth = 250, Nmin = 1, num_cutpoints = 100, no_split_penality = 4.605170185988092, mtry_pr = 8, mtry_trt = 8, p_categorical_pr = 0, p_categorical_trt = 0, num_trees_pr = 200, alpha_pr = 0.95, beta_pr = 2.0, tau_pr = 0.002735641295552384, kap_pr = 16.0, s_pr = 4.0, pr_scale = False, num_trees_trt = 100, alpha_trt = 0.95, beta_trt = 2.0, tau_trt = 0.0009118804318507947, kap_trt = 16.0, s_trt = 4.0, trt_scale = False, verbose = False, parallel = True, set_random_seed = False, random_seed = 0, sample_weights_flag = True, a_scaling = True, b_scaling = True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cf.fit(\n",
    "    x_t=synth_X, # Covariates treatment effect\n",
    "    x=synth_X, # Covariates outcome (including propensity score)\n",
    "    y=synth_y,  # Outcome\n",
    "    z=synth_treatment, # Treatment group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe422b6-821f-4d63-b08c-6de0250525c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 s, sys: 12 ms, total: 2.58 s\n",
      "Wall time: 2.56 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.00000441, -0.01259462,  2.00000071, ...,  1.99999129,\n",
       "        2.00000158,  2.00000441])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "synth_tau_xbcf = cf.predict(synth_X_test, return_mean=True)\n",
    "synth_tau_xbcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca31865e-45b5-40e0-b488-047e4e6142dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04994711611560761, 1.999994124582845, 2.0000020060979793, 2.0000037880084416, 2.0000082955627065, 9.6634255966066e-06, 2.0000182831468365, 1.99997962944445] [0.5152341866786688, 0.2577259784442367, 0.7177981517659884, 0.2577223132045925, 0.3588990758829942]\n"
     ]
    }
   ],
   "source": [
    "import evaluation_metrics\n",
    "true_ate = 2\n",
    "boxplot = evaluation_metrics.boxplot_ite(synth_tau_xbcf)\n",
    "metrics = evaluation_metrics.evaluation_metrics(true_ate, synth_tau_xbcf)\n",
    "print(boxplot, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091cbe2b-cd98-4ee5-9974-9f31b5b7221b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           method  \\\n",
      "0                                    S-Learner LR   \n",
      "1                                   XGBTRegressor   \n",
      "2                              BaseTRegressor XGB   \n",
      "3                               BaseTRegressor LR   \n",
      "4                              BaseXRegressor XGB   \n",
      "5                               BaseXRegressor LR   \n",
      "6   BaseXRegressor XGB (without propensity score)   \n",
      "7    BaseXRegressor LR (without propensity score)   \n",
      "8                              BaseRRegressor XGB   \n",
      "9                               BaseRRegressor LR   \n",
      "10        BaseRRegressor XGB (with random weight)   \n",
      "11  BaseRRegressor XGB (without propensity score)   \n",
      "12                           Neural Network (MLP)   \n",
      "13                                         BCAUSS   \n",
      "14                                          CEVAE   \n",
      "15             Accelerated Bayesian Causal Forest   \n",
      "\n",
      "                                                  ITE       ATE  \\\n",
      "0   [1.999999999999998, 1.9999999999999987, 1.9999...  2.000000   \n",
      "1            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "2            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "3            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "4            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "5            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "6            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "7            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  2.000000   \n",
      "8   [3.6223648294253508e-06, 3.6223648294253508e-0...  0.000004   \n",
      "9   [-2.9272906491754958, 0.7035801392352368, 1.20...  0.963643   \n",
      "10  [3.6222759263182525e-06, 3.6222759263182525e-0...  0.000004   \n",
      "11  [3.6812089092563838e-06, 3.6812089092563838e-0...  0.000004   \n",
      "12                                                     2.000000   \n",
      "13  [0.0, 4.016592899560254e-23, 5.3845826e-16, 2....  0.003403   \n",
      "14  [1.8834785, 1.950274258852005, 1.9614615, 1.99...  1.968778   \n",
      "15  [-0.04994711611560761, 1.999994124582845, 2.00...  1.742278   \n",
      "\n",
      "                                              metrics  \n",
      "0   [1.698796199394374e-30, 1.2862177740158606e-15...  \n",
      "1                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "2                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "3                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "4                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "5                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "6                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "7                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "8   [3.9999855105538034, 1.9999963776351706, 1.999...  \n",
      "9   [1.415439487411538, 1.036357273412645, 1.18972...  \n",
      "10  [3.9999855109094153, 1.9999963777240737, 1.999...  \n",
      "11  [3.9999852751779157, 1.9999963187910907, 1.999...  \n",
      "12                                                     \n",
      "13  [3.9870214, 1.9965966, 1.9967527, 1.9965966, 0...  \n",
      "14  [0.0014018206, 0.031313766, 0.037440896, 0.031...  \n",
      "15  [0.5152341866786688, 0.2577259784442367, 0.717...  \n",
      "Stored 'df_synthetic_results_metric' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store -r df_synthetic_results_metric\n",
    "\n",
    "method = \"Accelerated Bayesian Causal Forest\"\n",
    "ate = synth_tau_xbcf.mean()\n",
    "ite = boxplot\n",
    "metric = metrics\n",
    "\n",
    "df_synthetic_results_metric = df_synthetic_results_metric._append({'method': method, 'ATE': ate, 'ITE': ite, 'metrics': metric}, ignore_index=True)\n",
    "\n",
    "print(df_synthetic_results_metric)\n",
    "%store df_synthetic_results_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart",
   "language": "python",
   "name": "bart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

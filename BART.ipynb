{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667579c2-ecbc-4b09-a8ce-bf2e3b669623",
   "metadata": {},
   "source": [
    "Accelerated Bayesian Causal Forest\n",
    "https://johaupt.github.io/blog/xbcf.html\n",
    "\n",
    "Accelerated Bayesian Causal Forest (XBCF) to estimate the conditional average treatment effect (or uplift) using a specialized version of Bayesian Additive Regression Trees (BART). Itâ€™s better described as Bayesian boosted trees for non-parametric causal inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745ad78f-b583-44bc-a860-0fa4a13262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xbcausalforest import XBCF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff8872-9d01-4752-95ad-713ab4b8caaa",
   "metadata": {},
   "source": [
    "## BPIC 2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a723eb1e-39ae-45da-8114-a558c2222b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bpi2017_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68aac10-373e-4efe-8528-43776b46739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['NumberOfOffers', 'Action', 'org:resource',\n",
    "       'concept:name', 'EventOrigin', 'lifecycle:transition', 'time:timestamp',\n",
    "       'case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost',\n",
    "       'CreditScore', 'OfferedAmount', 'offerNumber','timeApplication', 'weekdayApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde3a0fd-7f22-4066-9e88-7424475acd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=11101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b17ec1-7208-4b25-8ab0-c392190d31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment=df_train['treatmentOffer']\n",
    "X = df_train[feature_names]\n",
    "y=df_train['offerSuccess']\n",
    "\n",
    "treatment_test=df_test['treatmentOffer']\n",
    "X_test = df_test[feature_names]\n",
    "y_test=df_test['offerSuccess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e54eac-03a7-4394-846f-ca5841ef3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_df = \n",
    "\n",
    "# df1 = df[df['treatment'] == 1]\n",
    "# df2 = df[df['treatment'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3e664c-127f-4aa0-9353-0dff5b0f6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TREES_PR  = 200\n",
    "NUM_TREES_TRT = 100\n",
    "\n",
    "cf = XBCF(\n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    num_sweeps=50, \n",
    "    burnin=15,\n",
    "    max_depth=250,\n",
    "    num_trees_pr=NUM_TREES_PR,\n",
    "    num_trees_trt=NUM_TREES_TRT,\n",
    "    num_cutpoints=100,\n",
    "    Nmin=1,\n",
    "    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'\n",
    "    #mtry_trt=X.shape[1], \n",
    "    tau_pr = 0.6 * np.var(y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,\n",
    "    tau_trt = 0.1 * np.var(y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,\n",
    "    alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "    beta_pr= 2, # shrinkage (tree depth)\n",
    "    alpha_trt= 0.95, # shrinkage for treatment part\n",
    "    beta_trt= 2,\n",
    "    p_categorical_pr = 0,\n",
    "    p_categorical_trt = 0,\n",
    "    standardize_target=True, # standardize y and unstandardize for prediction\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551935a8-4688-4d43-9ab3-f0ecc942c287",
   "metadata": {},
   "source": [
    "Since we specify the model as a sum of two BARTs, we can pass different sets of covariates to the outcome model and the treatment model, denoted by x and x_t. z is the treatment indicator coded 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2b870-fe15-40ef-864b-171ad62776de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## should this be included in the second X, as the propensity score\n",
    "p_model = ElasticNetPropensityModel()\n",
    "p = p_model.fit_predict(X, treatment)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcf4ccc-5c18-49f8-88ed-5bbf6e13106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10h 56min 59s, sys: 10min 8s, total: 11h 7min 7s\n",
      "Wall time: 1h 21min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XBCF(num_sweeps = 50, burnin = 15, max_depth = 250, Nmin = 1, num_cutpoints = 100, no_split_penality = 4.605170185988092, mtry_pr = 19, mtry_trt = 19, p_categorical_pr = 0, p_categorical_trt = 0, num_trees_pr = 200, alpha_pr = 0.95, beta_pr = 2.0, tau_pr = 0.0007494867168461104, kap_pr = 16.0, s_pr = 4.0, pr_scale = False, num_trees_trt = 100, alpha_trt = 0.95, beta_trt = 2.0, tau_trt = 0.00024982890561537014, kap_trt = 16.0, s_trt = 4.0, trt_scale = False, verbose = False, parallel = True, set_random_seed = False, random_seed = 0, sample_weights_flag = True, a_scaling = True, b_scaling = True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cf.fit(\n",
    "    x_t=X, # Covariates treatment effect\n",
    "    x=X, # Covariates outcome (including propensity score)\n",
    "    y=y,  # Outcome\n",
    "    z=treatment, # Treatment group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562b3bcc-04c8-4e51-b9c0-9db9cb2319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 186 ms, total: 29.7 s\n",
      "Wall time: 29.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03308522, -0.0446359 , -0.05176309, ..., -0.03810597,\n",
       "        0.0370041 , -0.03629823])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tau_xbcf = cf.predict(X_test, return_mean=True)\n",
    "tau_xbcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6c29f1-eb73-4a9d-9c00-433e00ea8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: -0.9658506333554799\n",
      "First Quartile: -0.04886845276852485\n",
      "Median: -0.0368942777357658\n",
      "Third Quartile: 0.03376068109580269\n",
      "Maximum: 2.5479356161410225\n",
      "Interquartile Range: 0.08262913386432755\n",
      "Upper Bound (Outliers): 0.15770438189229402\n",
      "Lower Bound (Outliers): -0.17281215356501617\n",
      "Outliers: [-0.7247538  -0.22329774  0.77603805 ...  0.17415126  2.38680562\n",
      " -0.40932287]\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "data = np.reshape(tau_xbcf, -1)\n",
    "minimum = np.min(data)\n",
    "first_quartile = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "third_quartile = np.percentile(data, 75)\n",
    "maximum = np.max(data)\n",
    "\n",
    "# Interquartile range (IQR)\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Define upper and lower bounds for outliers\n",
    "upper_bound = third_quartile + 1.5 * iqr\n",
    "lower_bound = first_quartile - 1.5 * iqr\n",
    "\n",
    "# Detect outliers\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Minimum:\", minimum)\n",
    "print(\"First Quartile:\", first_quartile)\n",
    "print(\"Median:\", median)\n",
    "print(\"Third Quartile:\", third_quartile)\n",
    "print(\"Maximum:\", maximum)\n",
    "print(\"Interquartile Range:\", iqr)\n",
    "print(\"Upper Bound (Outliers):\", upper_bound)\n",
    "print(\"Lower Bound (Outliers):\", lower_bound)\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "ite_bart = [minimum, first_quartile, median, third_quartile, maximum, iqr, upper_bound, lower_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246df696-f942-4081-b82d-ae8f4e769914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r df_results\n",
    "lib = \"xbcausalforest\"\n",
    "method = \"Accelerated Bayesian Causal Forest\"\n",
    "ite = ite_bart\n",
    "ate = tau_xbcf.mean()\n",
    "\n",
    "if method in df_results['method'].values:\n",
    "     # If the method is already in the DataFrame, update the ATE and ITE columns\n",
    "    df_results.loc[df_results['method'] == method, 'ATE'] = ate\n",
    "    index = df_results[df_results['method'] == method].index[0]\n",
    "    df_results.at[index, 'ITE'] = ite\n",
    "else:\n",
    "    # If the method is not in the DataFrame, add a new row\n",
    "    df_results = df_results._append({'method': method, 'ATE': ate, 'ITE': ite, 'Library': lib}, ignore_index=True)\n",
    "\n",
    "print(df_results)\n",
    "%store df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00dabd-40be-4563-a92a-17f50cfa39a4",
   "metadata": {},
   "source": [
    "## Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bb7220-f4d6-43fc-9abc-d355013ff316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synth = pd.read_csv(\"synthetic_dataset.csv\")\n",
    "df_synth.head()\n",
    "synthetic_features = ['NumberOfOffers', 'concept:name',\n",
    "       'lifecycle:transition', 'time:timestamp', 'elementId', 'resourceId',\n",
    "       'weekdayApplication', 'timeApplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148f5c77-777b-464d-97c8-25f6caf9bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and testing samples for model validation (next section)\n",
    "df_synth_train, df_synth_test = train_test_split(df_synth, test_size=0.2, random_state=11101)\n",
    "\n",
    "synth_treatment=df_synth_train['treatment']\n",
    "synth_X = df_synth_train[synthetic_features]\n",
    "synth_y=df_synth_train['treatmentSuccess']\n",
    "\n",
    "synth_treatment_test=df_synth_test['treatment']\n",
    "synth_X_test = df_synth_test[synthetic_features]\n",
    "synth_y_test=df_synth_test['treatmentSuccess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d11c0a6d-c23c-4703-9e13-e390e578ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TREES_PR  = 200\n",
    "NUM_TREES_TRT = 100\n",
    "\n",
    "cf = XBCF(\n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    num_sweeps=50, \n",
    "    burnin=15,\n",
    "    max_depth=250,\n",
    "    num_trees_pr=NUM_TREES_PR,\n",
    "    num_trees_trt=NUM_TREES_TRT,\n",
    "    num_cutpoints=100,\n",
    "    Nmin=1,\n",
    "    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'\n",
    "    #mtry_trt=X.shape[1], \n",
    "    tau_pr = 0.6 * np.var(synth_y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,\n",
    "    tau_trt = 0.1 * np.var(synth_y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,\n",
    "    alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "    beta_pr= 2, # shrinkage (tree depth)\n",
    "    alpha_trt= 0.95, # shrinkage for treatment part\n",
    "    beta_trt= 2,\n",
    "    p_categorical_pr = 0,\n",
    "    p_categorical_trt = 0,\n",
    "    standardize_target=True, # standardize y and unstandardize for prediction\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6331bf-aca9-4c00-9dc1-c12c22f9db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 5s, sys: 11.1 s, total: 17min 16s\n",
      "Wall time: 5min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XBCF(num_sweeps = 50, burnin = 15, max_depth = 250, Nmin = 1, num_cutpoints = 100, no_split_penality = 4.605170185988092, mtry_pr = 8, mtry_trt = 8, p_categorical_pr = 0, p_categorical_trt = 0, num_trees_pr = 200, alpha_pr = 0.95, beta_pr = 2.0, tau_pr = 0.002735641295552384, kap_pr = 16.0, s_pr = 4.0, pr_scale = False, num_trees_trt = 100, alpha_trt = 0.95, beta_trt = 2.0, tau_trt = 0.0009118804318507947, kap_trt = 16.0, s_trt = 4.0, trt_scale = False, verbose = False, parallel = True, set_random_seed = False, random_seed = 0, sample_weights_flag = True, a_scaling = True, b_scaling = True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cf.fit(\n",
    "    x_t=synth_X, # Covariates treatment effect\n",
    "    x=synth_X, # Covariates outcome (including propensity score)\n",
    "    y=synth_y,  # Outcome\n",
    "    z=synth_treatment, # Treatment group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe422b6-821f-4d63-b08c-6de0250525c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 s, sys: 19.8 ms, total: 2.61 s\n",
      "Wall time: 2.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.99999285, 0.07704664, 1.99999596, ..., 1.99999138, 1.99999185,\n",
       "       1.99999248])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "synth_tau_xbcf = cf.predict(synth_X_test, return_mean=True)\n",
    "synth_tau_xbcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca31865e-45b5-40e0-b488-047e4e6142dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "data = np.reshape(synth_tau_xbcf, -1)\n",
    "minimum = np.min(data)\n",
    "first_quartile = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "third_quartile = np.percentile(data, 75)\n",
    "maximum = np.max(data)\n",
    "\n",
    "# Interquartile range (IQR)\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Define upper and lower bounds for outliers\n",
    "upper_bound = third_quartile + 1.5 * iqr\n",
    "lower_bound = first_quartile - 1.5 * iqr\n",
    "\n",
    "# Detect outliers\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "synth_ite_xbcf = [minimum, first_quartile, median, third_quartile, maximum, iqr, upper_bound, lower_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "091cbe2b-cd98-4ee5-9974-9f31b5b7221b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           method       ATE  \\\n",
      "0                                     Causal Tree  2.000000   \n",
      "1                                     cforest_mse       NaN   \n",
      "2                                    cforest_cmse  2.000000   \n",
      "3                              cforest_cmse_p=0.5  2.000000   \n",
      "4                         cforest_cmse_p=0.5_md=3  2.000000   \n",
      "5                                   cforest_ttest  2.000000   \n",
      "6                                          IPW LR  2.000000   \n",
      "7                                       Double ML  1.994487   \n",
      "8                               Linear Regression  2.000000   \n",
      "9                                    S-Learner LR  2.000000   \n",
      "10                                  XGBTRegressor  2.000000   \n",
      "11                             BaseTRegressor XGB  2.000000   \n",
      "12                              BaseTRegressor LR  2.000000   \n",
      "13                             BaseXRegressor XGB  2.000000   \n",
      "14                              BaseXRegressor LR  2.000000   \n",
      "15  BaseXRegressor XGB (without propensity score)  2.000000   \n",
      "16   BaseXRegressor LR (without propensity score)  2.000000   \n",
      "17                             BaseRRegressor XGB  0.000004   \n",
      "18                              BaseRRegressor LR  0.963636   \n",
      "19        BaseRRegressor XGB (with random weight)  0.000004   \n",
      "20  BaseRRegressor XGB (without propensity score)  0.000004   \n",
      "21   BaseRRegressor LR (without propensity score)  1.037307   \n",
      "22                           Neural Network (MLP)  2.000000   \n",
      "23                                          CEVAE  1.970946   \n",
      "24             Accelerated Bayesian Causal Forest  1.753875   \n",
      "\n",
      "                                                  ITE  \n",
      "0            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "1                                                 NaN  \n",
      "2            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "3            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "4            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "5            [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "6                                                 NaN  \n",
      "7   [1.992640849756571, 1.992640849756571, 1.99264...  \n",
      "8                                                 NaN  \n",
      "9   [1.999999999999998, 1.9999999999999987, 1.9999...  \n",
      "10           [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "11           [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "12           [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "13           [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "14           [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "15           [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "16           [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0]  \n",
      "17  [3.6223518691258505e-06, 3.6223518691258505e-0...  \n",
      "18  [-2.927061628951367, 0.70359919027986, 1.20964...  \n",
      "19  [3.6225269468559418e-06, 3.6225269468559418e-0...  \n",
      "20  [3.681200723804068e-06, 3.681200723804068e-06,...  \n",
      "21  [-3.029083798917791, 0.7664939624838119, 1.380...  \n",
      "22                                                NaN  \n",
      "23  [1.9071604, 1.9520107507705688, 1.9689205, 1.9...  \n",
      "24  [0.0668822689315117, 1.9999913796466386, 1.999...  \n",
      "Stored 'df_synthetic_results' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store -r df_synthetic_results\n",
    "lib = \"xbcausalforest\"\n",
    "method = \"Accelerated Bayesian Causal Forest\"\n",
    "ite = synth_ite_xbcf\n",
    "ate = synth_tau_xbcf.mean()\n",
    "\n",
    "if method in df_synthetic_results['method'].values:\n",
    "     # If the method is already in the DataFrame, update the ATE and ITE columns\n",
    "    df_synthetic_results.loc[df_synthetic_results['method'] == method, 'ATE'] = ate\n",
    "    index = df_synthetic_results[df_synthetic_results['method'] == method].index[0]\n",
    "    df_synthetic_results.at[index, 'ITE'] = ite\n",
    "else:\n",
    "    # If the method is not in the DataFrame, add a new row\n",
    "    df_synthetic_results = df_synthetic_results._append({'method': method, 'ATE': ate, 'ITE': ite}, ignore_index=True)\n",
    "\n",
    "print(df_synthetic_results)\n",
    "%store df_synthetic_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart",
   "language": "python",
   "name": "bart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
